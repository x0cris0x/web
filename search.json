[
  {
    "objectID": "r/r_spam.html",
    "href": "r/r_spam.html",
    "title": "Dataset Spam",
    "section": "",
    "text": "El dataset de spam es una recopilado en los laboratorios de Hewlett-Packard, clasifica 4601 correos electrónicos como spam o no spam. Además de esta etiqueta de clase, existen 57 variables que indican la frecuencia de ciertas palabras y caracteres en el correo electrónico.\nEl dataset contiene 2788 correos “no spam” y 1813 como “spam”. Mas información\n\n\n\nlibrary(caret)\nlibrary(kernlab)\n\nCargar el dataset dentro de la variable datos\n\ndata(spam)\ndatos  = spam\n\nInformación del dataset\n\ndim(datos) \n\n[1] 4601   58\n\n\nEl dataset tiene 4601 observaciones y 58 variables\n\nstr(datos) \n\n'data.frame':   4601 obs. of  58 variables:\n $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...\n $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...\n $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...\n $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ our              : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...\n $ over             : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...\n $ remove           : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...\n $ internet         : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...\n $ order            : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...\n $ mail             : num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...\n $ receive          : num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...\n $ will             : num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...\n $ people           : num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...\n $ report           : num  0 0.21 0 0 0 0 0 0 0 0 ...\n $ addresses        : num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...\n $ free             : num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...\n $ business         : num  0 0.07 0.06 0 0 0 0 0 0 0 ...\n $ email            : num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...\n $ you              : num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...\n $ credit           : num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...\n $ your             : num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...\n $ font             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num000           : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...\n $ money            : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...\n $ hp               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ hpl              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ george           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num650           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ lab              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ labs             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ telnet           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num857           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ data             : num  0 0 0 0 0 0 0 0 0.15 0 ...\n $ num415           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num85            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ technology       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num1999          : num  0 0.07 0 0 0 0 0 0 0 0 ...\n $ parts            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ pm               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ direct           : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ cs               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ meeting          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ original         : num  0 0 0.12 0 0 0 0 0 0.3 0 ...\n $ project          : num  0 0 0 0 0 0 0 0 0 0.06 ...\n $ re               : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ edu              : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ table            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ conference       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ charSemicolon    : num  0 0 0.01 0 0 0 0 0 0 0.04 ...\n $ charRoundbracket : num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...\n $ charSquarebracket: num  0 0 0 0 0 0 0 0 0 0 ...\n $ charExclamation  : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...\n $ charDollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...\n $ charHash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...\n $ capitalAve       : num  3.76 5.11 9.82 3.54 3.54 ...\n $ capitalLong      : num  61 101 485 40 40 15 4 11 445 43 ...\n $ capitalTotal     : num  278 1028 2259 191 191 ...\n $ type             : Factor w/ 2 levels \"nonspam\",\"spam\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nSe observa que todos las variables son numéricas, excepto type que es categórica, es la variable a analizar\n\ncolSums(is.na(datos)) \n\n             make           address               all             num3d \n                0                 0                 0                 0 \n              our              over            remove          internet \n                0                 0                 0                 0 \n            order              mail           receive              will \n                0                 0                 0                 0 \n           people            report         addresses              free \n                0                 0                 0                 0 \n         business             email               you            credit \n                0                 0                 0                 0 \n             your              font            num000             money \n                0                 0                 0                 0 \n               hp               hpl            george            num650 \n                0                 0                 0                 0 \n              lab              labs            telnet            num857 \n                0                 0                 0                 0 \n             data            num415             num85        technology \n                0                 0                 0                 0 \n          num1999             parts                pm            direct \n                0                 0                 0                 0 \n               cs           meeting          original           project \n                0                 0                 0                 0 \n               re               edu             table        conference \n                0                 0                 0                 0 \n    charSemicolon  charRoundbracket charSquarebracket   charExclamation \n                0                 0                 0                 0 \n       charDollar          charHash        capitalAve       capitalLong \n                0                 0                 0                 0 \n     capitalTotal              type \n                0                 0 \n\n\nEl dataset no contiene valores faltantes\n\ntable(datos$type) \n\n\nnonspam    spam \n   2788    1813 \n\n\nExiste 2788 correos “no spam” y 1813 con spam\n\n\n\n\nset.seed(123) \ntrain &lt;- createDataPartition(datos$type, p = 0.8, list = FALSE) \ntrainData &lt;- datos[train, ] \ntestData  &lt;- datos[-train, ] \n\nSe divide el dataset en 80% train y 20% test\n\npreProcValues &lt;- preProcess(trainData[,1:57], method = c(\"center\", \"scale\")) \ntrainTransformed &lt;- predict(preProcValues, trainData[, 1:57]) \ntestTransformed  &lt;- predict(preProcValues, testData[, 1:57]) \n\nSe realiza un escalado y centrado de las variables\n\ntrainTransformed$type &lt;- trainData$type \ntestTransformed$type  &lt;- testData$type \n\nReconstruimos el datasets con la variable objetivo\n\nset.seed(123) \nctrl &lt;- trainControl(method = \"cv\", number = 5) \n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)  \nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\") \n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbour y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) {\n  train(type ~ .,\n        data = trainTransformed,\n        method = m,\n        trControl = ctrl, \n        tuneLength = 5)\n  }) \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2790            -nan     0.1000    0.0294\n     2        1.2238            -nan     0.1000    0.0274\n     3        1.1765            -nan     0.1000    0.0222\n     4        1.1342            -nan     0.1000    0.0206\n     5        1.0970            -nan     0.1000    0.0174\n     6        1.0630            -nan     0.1000    0.0169\n     7        1.0297            -nan     0.1000    0.0163\n     8        1.0014            -nan     0.1000    0.0140\n     9        0.9745            -nan     0.1000    0.0119\n    10        0.9489            -nan     0.1000    0.0114\n    20        0.7761            -nan     0.1000    0.0071\n    40        0.5933            -nan     0.1000    0.0025\n    60        0.5080            -nan     0.1000    0.0009\n    80        0.4534            -nan     0.1000    0.0004\n   100        0.4184            -nan     0.1000    0.0011\n   120        0.3923            -nan     0.1000    0.0000\n   140        0.3734            -nan     0.1000    0.0003\n   160        0.3601            -nan     0.1000    0.0000\n   180        0.3491            -nan     0.1000   -0.0001\n   200        0.3377            -nan     0.1000    0.0000\n   220        0.3298            -nan     0.1000    0.0001\n   240        0.3215            -nan     0.1000   -0.0002\n   250        0.3196            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2619            -nan     0.1000    0.0373\n     2        1.1822            -nan     0.1000    0.0387\n     3        1.1239            -nan     0.1000    0.0289\n     4        1.0637            -nan     0.1000    0.0296\n     5        1.0167            -nan     0.1000    0.0226\n     6        0.9745            -nan     0.1000    0.0204\n     7        0.9326            -nan     0.1000    0.0202\n     8        0.8934            -nan     0.1000    0.0181\n     9        0.8631            -nan     0.1000    0.0140\n    10        0.8368            -nan     0.1000    0.0123\n    20        0.6315            -nan     0.1000    0.0064\n    40        0.4537            -nan     0.1000    0.0024\n    60        0.3860            -nan     0.1000   -0.0001\n    80        0.3491            -nan     0.1000    0.0004\n   100        0.3251            -nan     0.1000   -0.0002\n   120        0.3075            -nan     0.1000   -0.0003\n   140        0.2925            -nan     0.1000   -0.0000\n   160        0.2792            -nan     0.1000   -0.0002\n   180        0.2669            -nan     0.1000   -0.0003\n   200        0.2587            -nan     0.1000   -0.0001\n   220        0.2521            -nan     0.1000   -0.0003\n   240        0.2445            -nan     0.1000    0.0000\n   250        0.2414            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2380            -nan     0.1000    0.0512\n     2        1.1593            -nan     0.1000    0.0381\n     3        1.0842            -nan     0.1000    0.0359\n     4        1.0260            -nan     0.1000    0.0277\n     5        0.9706            -nan     0.1000    0.0280\n     6        0.9253            -nan     0.1000    0.0212\n     7        0.8840            -nan     0.1000    0.0191\n     8        0.8463            -nan     0.1000    0.0187\n     9        0.8133            -nan     0.1000    0.0156\n    10        0.7818            -nan     0.1000    0.0155\n    20        0.5737            -nan     0.1000    0.0071\n    40        0.4007            -nan     0.1000    0.0016\n    60        0.3349            -nan     0.1000    0.0005\n    80        0.3019            -nan     0.1000    0.0000\n   100        0.2827            -nan     0.1000   -0.0003\n   120        0.2660            -nan     0.1000   -0.0005\n   140        0.2504            -nan     0.1000   -0.0001\n   160        0.2385            -nan     0.1000   -0.0001\n   180        0.2277            -nan     0.1000    0.0002\n   200        0.2200            -nan     0.1000   -0.0004\n   220        0.2102            -nan     0.1000   -0.0004\n   240        0.2024            -nan     0.1000   -0.0001\n   250        0.1985            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2384            -nan     0.1000    0.0501\n     2        1.1431            -nan     0.1000    0.0459\n     3        1.0677            -nan     0.1000    0.0382\n     4        1.0049            -nan     0.1000    0.0312\n     5        0.9507            -nan     0.1000    0.0258\n     6        0.8981            -nan     0.1000    0.0265\n     7        0.8506            -nan     0.1000    0.0222\n     8        0.8133            -nan     0.1000    0.0176\n     9        0.7768            -nan     0.1000    0.0175\n    10        0.7474            -nan     0.1000    0.0132\n    20        0.5313            -nan     0.1000    0.0094\n    40        0.3736            -nan     0.1000    0.0010\n    60        0.3126            -nan     0.1000    0.0003\n    80        0.2768            -nan     0.1000    0.0005\n   100        0.2535            -nan     0.1000   -0.0002\n   120        0.2382            -nan     0.1000   -0.0002\n   140        0.2240            -nan     0.1000   -0.0003\n   160        0.2094            -nan     0.1000   -0.0003\n   180        0.1960            -nan     0.1000   -0.0001\n   200        0.1868            -nan     0.1000   -0.0002\n   220        0.1774            -nan     0.1000   -0.0001\n   240        0.1665            -nan     0.1000   -0.0002\n   250        0.1614            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2328            -nan     0.1000    0.0514\n     2        1.1327            -nan     0.1000    0.0501\n     3        1.0519            -nan     0.1000    0.0388\n     4        0.9835            -nan     0.1000    0.0317\n     5        0.9245            -nan     0.1000    0.0275\n     6        0.8708            -nan     0.1000    0.0251\n     7        0.8270            -nan     0.1000    0.0209\n     8        0.7870            -nan     0.1000    0.0194\n     9        0.7499            -nan     0.1000    0.0180\n    10        0.7175            -nan     0.1000    0.0143\n    20        0.5060            -nan     0.1000    0.0093\n    40        0.3435            -nan     0.1000    0.0012\n    60        0.2822            -nan     0.1000    0.0001\n    80        0.2542            -nan     0.1000    0.0000\n   100        0.2328            -nan     0.1000   -0.0001\n   120        0.2132            -nan     0.1000   -0.0004\n   140        0.1963            -nan     0.1000    0.0001\n   160        0.1818            -nan     0.1000   -0.0001\n   180        0.1714            -nan     0.1000   -0.0003\n   200        0.1590            -nan     0.1000   -0.0003\n   220        0.1496            -nan     0.1000   -0.0003\n   240        0.1415            -nan     0.1000   -0.0002\n   250        0.1375            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2797            -nan     0.1000    0.0295\n     2        1.2225            -nan     0.1000    0.0289\n     3        1.1766            -nan     0.1000    0.0211\n     4        1.1304            -nan     0.1000    0.0214\n     5        1.0944            -nan     0.1000    0.0174\n     6        1.0598            -nan     0.1000    0.0167\n     7        1.0287            -nan     0.1000    0.0155\n     8        1.0014            -nan     0.1000    0.0133\n     9        0.9742            -nan     0.1000    0.0135\n    10        0.9494            -nan     0.1000    0.0119\n    20        0.7735            -nan     0.1000    0.0069\n    40        0.5909            -nan     0.1000    0.0024\n    60        0.4997            -nan     0.1000    0.0010\n    80        0.4440            -nan     0.1000    0.0006\n   100        0.4043            -nan     0.1000    0.0004\n   120        0.3767            -nan     0.1000   -0.0002\n   140        0.3564            -nan     0.1000    0.0005\n   160        0.3423            -nan     0.1000   -0.0004\n   180        0.3290            -nan     0.1000    0.0005\n   200        0.3209            -nan     0.1000   -0.0004\n   220        0.3129            -nan     0.1000   -0.0000\n   240        0.3046            -nan     0.1000    0.0000\n   250        0.3015            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2501            -nan     0.1000    0.0452\n     2        1.1788            -nan     0.1000    0.0338\n     3        1.1200            -nan     0.1000    0.0281\n     4        1.0632            -nan     0.1000    0.0285\n     5        1.0169            -nan     0.1000    0.0210\n     6        0.9738            -nan     0.1000    0.0209\n     7        0.9321            -nan     0.1000    0.0204\n     8        0.8985            -nan     0.1000    0.0167\n     9        0.8665            -nan     0.1000    0.0151\n    10        0.8363            -nan     0.1000    0.0138\n    20        0.6189            -nan     0.1000    0.0088\n    40        0.4503            -nan     0.1000    0.0026\n    60        0.3736            -nan     0.1000    0.0009\n    80        0.3315            -nan     0.1000    0.0003\n   100        0.3062            -nan     0.1000    0.0001\n   120        0.2894            -nan     0.1000   -0.0003\n   140        0.2771            -nan     0.1000   -0.0002\n   160        0.2624            -nan     0.1000   -0.0001\n   180        0.2539            -nan     0.1000   -0.0002\n   200        0.2454            -nan     0.1000    0.0000\n   220        0.2388            -nan     0.1000   -0.0001\n   240        0.2302            -nan     0.1000   -0.0002\n   250        0.2266            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2464            -nan     0.1000    0.0489\n     2        1.1576            -nan     0.1000    0.0435\n     3        1.0883            -nan     0.1000    0.0338\n     4        1.0233            -nan     0.1000    0.0325\n     5        0.9700            -nan     0.1000    0.0258\n     6        0.9215            -nan     0.1000    0.0235\n     7        0.8812            -nan     0.1000    0.0190\n     8        0.8430            -nan     0.1000    0.0169\n     9        0.8074            -nan     0.1000    0.0174\n    10        0.7761            -nan     0.1000    0.0155\n    20        0.5647            -nan     0.1000    0.0106\n    40        0.3889            -nan     0.1000    0.0014\n    60        0.3281            -nan     0.1000    0.0002\n    80        0.2924            -nan     0.1000    0.0002\n   100        0.2679            -nan     0.1000   -0.0004\n   120        0.2501            -nan     0.1000   -0.0000\n   140        0.2368            -nan     0.1000   -0.0002\n   160        0.2256            -nan     0.1000   -0.0002\n   180        0.2155            -nan     0.1000   -0.0003\n   200        0.2055            -nan     0.1000   -0.0001\n   220        0.1963            -nan     0.1000   -0.0001\n   240        0.1883            -nan     0.1000    0.0001\n   250        0.1851            -nan     0.1000   -0.0004\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2365            -nan     0.1000    0.0500\n     2        1.1373            -nan     0.1000    0.0471\n     3        1.0613            -nan     0.1000    0.0357\n     4        0.9900            -nan     0.1000    0.0352\n     5        0.9319            -nan     0.1000    0.0283\n     6        0.8845            -nan     0.1000    0.0227\n     7        0.8417            -nan     0.1000    0.0204\n     8        0.8016            -nan     0.1000    0.0197\n     9        0.7677            -nan     0.1000    0.0159\n    10        0.7330            -nan     0.1000    0.0170\n    20        0.5216            -nan     0.1000    0.0079\n    40        0.3543            -nan     0.1000    0.0007\n    60        0.2898            -nan     0.1000    0.0005\n    80        0.2578            -nan     0.1000   -0.0001\n   100        0.2383            -nan     0.1000   -0.0001\n   120        0.2225            -nan     0.1000   -0.0004\n   140        0.2073            -nan     0.1000   -0.0003\n   160        0.1933            -nan     0.1000   -0.0004\n   180        0.1830            -nan     0.1000   -0.0004\n   200        0.1726            -nan     0.1000   -0.0001\n   220        0.1632            -nan     0.1000   -0.0002\n   240        0.1527            -nan     0.1000   -0.0001\n   250        0.1482            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2223            -nan     0.1000    0.0585\n     2        1.1319            -nan     0.1000    0.0435\n     3        1.0483            -nan     0.1000    0.0398\n     4        0.9765            -nan     0.1000    0.0343\n     5        0.9196            -nan     0.1000    0.0265\n     6        0.8667            -nan     0.1000    0.0253\n     7        0.8171            -nan     0.1000    0.0220\n     8        0.7766            -nan     0.1000    0.0190\n     9        0.7436            -nan     0.1000    0.0158\n    10        0.7119            -nan     0.1000    0.0143\n    20        0.4941            -nan     0.1000    0.0069\n    40        0.3272            -nan     0.1000    0.0027\n    60        0.2708            -nan     0.1000   -0.0003\n    80        0.2377            -nan     0.1000   -0.0002\n   100        0.2164            -nan     0.1000   -0.0003\n   120        0.1983            -nan     0.1000   -0.0002\n   140        0.1847            -nan     0.1000   -0.0003\n   160        0.1713            -nan     0.1000   -0.0003\n   180        0.1589            -nan     0.1000   -0.0005\n   200        0.1473            -nan     0.1000   -0.0002\n   220        0.1380            -nan     0.1000   -0.0001\n   240        0.1303            -nan     0.1000   -0.0004\n   250        0.1267            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2823            -nan     0.1000    0.0293\n     2        1.2254            -nan     0.1000    0.0285\n     3        1.1785            -nan     0.1000    0.0229\n     4        1.1353            -nan     0.1000    0.0201\n     5        1.0995            -nan     0.1000    0.0162\n     6        1.0641            -nan     0.1000    0.0175\n     7        1.0315            -nan     0.1000    0.0151\n     8        1.0030            -nan     0.1000    0.0134\n     9        0.9767            -nan     0.1000    0.0122\n    10        0.9535            -nan     0.1000    0.0104\n    20        0.7759            -nan     0.1000    0.0061\n    40        0.5977            -nan     0.1000    0.0026\n    60        0.5040            -nan     0.1000    0.0022\n    80        0.4512            -nan     0.1000    0.0004\n   100        0.4146            -nan     0.1000    0.0002\n   120        0.3874            -nan     0.1000    0.0003\n   140        0.3655            -nan     0.1000   -0.0001\n   160        0.3498            -nan     0.1000    0.0006\n   180        0.3364            -nan     0.1000    0.0001\n   200        0.3254            -nan     0.1000    0.0000\n   220        0.3153            -nan     0.1000   -0.0001\n   240        0.3084            -nan     0.1000    0.0001\n   250        0.3037            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2640            -nan     0.1000    0.0374\n     2        1.1837            -nan     0.1000    0.0396\n     3        1.1173            -nan     0.1000    0.0322\n     4        1.0651            -nan     0.1000    0.0261\n     5        1.0202            -nan     0.1000    0.0215\n     6        0.9759            -nan     0.1000    0.0211\n     7        0.9385            -nan     0.1000    0.0170\n     8        0.9014            -nan     0.1000    0.0184\n     9        0.8709            -nan     0.1000    0.0151\n    10        0.8397            -nan     0.1000    0.0148\n    20        0.6309            -nan     0.1000    0.0063\n    40        0.4512            -nan     0.1000    0.0018\n    60        0.3824            -nan     0.1000    0.0003\n    80        0.3426            -nan     0.1000    0.0003\n   100        0.3169            -nan     0.1000    0.0004\n   120        0.2964            -nan     0.1000    0.0000\n   140        0.2801            -nan     0.1000   -0.0005\n   160        0.2706            -nan     0.1000   -0.0001\n   180        0.2588            -nan     0.1000   -0.0001\n   200        0.2502            -nan     0.1000   -0.0000\n   220        0.2441            -nan     0.1000   -0.0001\n   240        0.2370            -nan     0.1000   -0.0002\n   250        0.2340            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2365            -nan     0.1000    0.0505\n     2        1.1621            -nan     0.1000    0.0365\n     3        1.0878            -nan     0.1000    0.0370\n     4        1.0272            -nan     0.1000    0.0288\n     5        0.9764            -nan     0.1000    0.0241\n     6        0.9267            -nan     0.1000    0.0246\n     7        0.8868            -nan     0.1000    0.0197\n     8        0.8455            -nan     0.1000    0.0196\n     9        0.8094            -nan     0.1000    0.0168\n    10        0.7806            -nan     0.1000    0.0135\n    20        0.5647            -nan     0.1000    0.0061\n    40        0.3954            -nan     0.1000    0.0022\n    60        0.3348            -nan     0.1000    0.0000\n    80        0.3003            -nan     0.1000    0.0000\n   100        0.2761            -nan     0.1000   -0.0001\n   120        0.2599            -nan     0.1000   -0.0003\n   140        0.2439            -nan     0.1000   -0.0004\n   160        0.2309            -nan     0.1000   -0.0001\n   180        0.2208            -nan     0.1000   -0.0001\n   200        0.2105            -nan     0.1000   -0.0000\n   220        0.2012            -nan     0.1000   -0.0002\n   240        0.1949            -nan     0.1000   -0.0002\n   250        0.1914            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2327            -nan     0.1000    0.0541\n     2        1.1402            -nan     0.1000    0.0469\n     3        1.0660            -nan     0.1000    0.0358\n     4        1.0016            -nan     0.1000    0.0304\n     5        0.9455            -nan     0.1000    0.0271\n     6        0.8908            -nan     0.1000    0.0261\n     7        0.8468            -nan     0.1000    0.0204\n     8        0.8073            -nan     0.1000    0.0183\n     9        0.7731            -nan     0.1000    0.0169\n    10        0.7402            -nan     0.1000    0.0147\n    20        0.5251            -nan     0.1000    0.0061\n    40        0.3630            -nan     0.1000    0.0021\n    60        0.3027            -nan     0.1000    0.0003\n    80        0.2688            -nan     0.1000   -0.0001\n   100        0.2484            -nan     0.1000   -0.0001\n   120        0.2310            -nan     0.1000    0.0001\n   140        0.2153            -nan     0.1000   -0.0003\n   160        0.2031            -nan     0.1000   -0.0002\n   180        0.1923            -nan     0.1000   -0.0002\n   200        0.1822            -nan     0.1000   -0.0003\n   220        0.1719            -nan     0.1000   -0.0003\n   240        0.1630            -nan     0.1000   -0.0001\n   250        0.1595            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2219            -nan     0.1000    0.0590\n     2        1.1333            -nan     0.1000    0.0436\n     3        1.0553            -nan     0.1000    0.0376\n     4        0.9928            -nan     0.1000    0.0306\n     5        0.9299            -nan     0.1000    0.0305\n     6        0.8791            -nan     0.1000    0.0232\n     7        0.8308            -nan     0.1000    0.0231\n     8        0.7891            -nan     0.1000    0.0193\n     9        0.7509            -nan     0.1000    0.0180\n    10        0.7183            -nan     0.1000    0.0155\n    20        0.4999            -nan     0.1000    0.0099\n    40        0.3382            -nan     0.1000    0.0014\n    60        0.2798            -nan     0.1000    0.0002\n    80        0.2470            -nan     0.1000    0.0001\n   100        0.2241            -nan     0.1000   -0.0002\n   120        0.2087            -nan     0.1000   -0.0001\n   140        0.1931            -nan     0.1000   -0.0003\n   160        0.1781            -nan     0.1000   -0.0004\n   180        0.1675            -nan     0.1000   -0.0005\n   200        0.1585            -nan     0.1000   -0.0003\n   220        0.1491            -nan     0.1000   -0.0003\n   240        0.1405            -nan     0.1000   -0.0004\n   250        0.1357            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2724            -nan     0.1000    0.0344\n     2        1.2136            -nan     0.1000    0.0276\n     3        1.1671            -nan     0.1000    0.0233\n     4        1.1266            -nan     0.1000    0.0194\n     5        1.0861            -nan     0.1000    0.0199\n     6        1.0521            -nan     0.1000    0.0170\n     7        1.0206            -nan     0.1000    0.0154\n     8        0.9915            -nan     0.1000    0.0143\n     9        0.9646            -nan     0.1000    0.0133\n    10        0.9399            -nan     0.1000    0.0108\n    20        0.7629            -nan     0.1000    0.0067\n    40        0.5878            -nan     0.1000    0.0023\n    60        0.4985            -nan     0.1000    0.0009\n    80        0.4421            -nan     0.1000    0.0008\n   100        0.4063            -nan     0.1000    0.0002\n   120        0.3809            -nan     0.1000   -0.0000\n   140        0.3617            -nan     0.1000    0.0001\n   160        0.3467            -nan     0.1000    0.0001\n   180        0.3348            -nan     0.1000   -0.0001\n   200        0.3247            -nan     0.1000   -0.0001\n   220        0.3156            -nan     0.1000    0.0001\n   240        0.3056            -nan     0.1000    0.0001\n   250        0.3033            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2565            -nan     0.1000    0.0390\n     2        1.1715            -nan     0.1000    0.0413\n     3        1.1032            -nan     0.1000    0.0329\n     4        1.0484            -nan     0.1000    0.0262\n     5        0.9997            -nan     0.1000    0.0235\n     6        0.9585            -nan     0.1000    0.0195\n     7        0.9175            -nan     0.1000    0.0197\n     8        0.8821            -nan     0.1000    0.0170\n     9        0.8484            -nan     0.1000    0.0164\n    10        0.8175            -nan     0.1000    0.0140\n    20        0.6167            -nan     0.1000    0.0064\n    40        0.4457            -nan     0.1000    0.0019\n    60        0.3755            -nan     0.1000    0.0008\n    80        0.3369            -nan     0.1000    0.0005\n   100        0.3141            -nan     0.1000    0.0000\n   120        0.2976            -nan     0.1000    0.0003\n   140        0.2825            -nan     0.1000    0.0001\n   160        0.2708            -nan     0.1000   -0.0001\n   180        0.2624            -nan     0.1000   -0.0001\n   200        0.2536            -nan     0.1000   -0.0001\n   220        0.2449            -nan     0.1000   -0.0002\n   240        0.2387            -nan     0.1000   -0.0003\n   250        0.2359            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2287            -nan     0.1000    0.0556\n     2        1.1472            -nan     0.1000    0.0410\n     3        1.0719            -nan     0.1000    0.0371\n     4        1.0104            -nan     0.1000    0.0316\n     5        0.9589            -nan     0.1000    0.0252\n     6        0.9111            -nan     0.1000    0.0230\n     7        0.8693            -nan     0.1000    0.0208\n     8        0.8348            -nan     0.1000    0.0169\n     9        0.8005            -nan     0.1000    0.0168\n    10        0.7682            -nan     0.1000    0.0148\n    20        0.5600            -nan     0.1000    0.0102\n    40        0.3880            -nan     0.1000    0.0012\n    60        0.3237            -nan     0.1000    0.0009\n    80        0.2911            -nan     0.1000   -0.0001\n   100        0.2675            -nan     0.1000    0.0002\n   120        0.2517            -nan     0.1000   -0.0009\n   140        0.2388            -nan     0.1000   -0.0002\n   160        0.2281            -nan     0.1000    0.0003\n   180        0.2187            -nan     0.1000   -0.0002\n   200        0.2101            -nan     0.1000   -0.0003\n   220        0.1992            -nan     0.1000   -0.0002\n   240        0.1920            -nan     0.1000   -0.0001\n   250        0.1881            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2345            -nan     0.1000    0.0514\n     2        1.1311            -nan     0.1000    0.0499\n     3        1.0566            -nan     0.1000    0.0356\n     4        0.9894            -nan     0.1000    0.0335\n     5        0.9333            -nan     0.1000    0.0275\n     6        0.8789            -nan     0.1000    0.0261\n     7        0.8358            -nan     0.1000    0.0206\n     8        0.7925            -nan     0.1000    0.0207\n     9        0.7578            -nan     0.1000    0.0159\n    10        0.7277            -nan     0.1000    0.0136\n    20        0.5102            -nan     0.1000    0.0096\n    40        0.3532            -nan     0.1000    0.0013\n    60        0.2899            -nan     0.1000   -0.0002\n    80        0.2584            -nan     0.1000    0.0006\n   100        0.2379            -nan     0.1000   -0.0006\n   120        0.2202            -nan     0.1000   -0.0000\n   140        0.2068            -nan     0.1000   -0.0003\n   160        0.1950            -nan     0.1000   -0.0002\n   180        0.1839            -nan     0.1000   -0.0002\n   200        0.1733            -nan     0.1000   -0.0003\n   220        0.1654            -nan     0.1000   -0.0001\n   240        0.1572            -nan     0.1000   -0.0001\n   250        0.1535            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2252            -nan     0.1000    0.0575\n     2        1.1299            -nan     0.1000    0.0462\n     3        1.0419            -nan     0.1000    0.0422\n     4        0.9741            -nan     0.1000    0.0320\n     5        0.9112            -nan     0.1000    0.0299\n     6        0.8619            -nan     0.1000    0.0233\n     7        0.8139            -nan     0.1000    0.0229\n     8        0.7713            -nan     0.1000    0.0193\n     9        0.7379            -nan     0.1000    0.0148\n    10        0.7062            -nan     0.1000    0.0146\n    20        0.4935            -nan     0.1000    0.0061\n    40        0.3363            -nan     0.1000    0.0015\n    60        0.2782            -nan     0.1000    0.0010\n    80        0.2439            -nan     0.1000   -0.0002\n   100        0.2212            -nan     0.1000   -0.0009\n   120        0.2043            -nan     0.1000   -0.0005\n   140        0.1860            -nan     0.1000   -0.0003\n   160        0.1746            -nan     0.1000   -0.0002\n   180        0.1636            -nan     0.1000   -0.0001\n   200        0.1517            -nan     0.1000   -0.0001\n   220        0.1405            -nan     0.1000   -0.0003\n   240        0.1317            -nan     0.1000   -0.0001\n   250        0.1267            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2794            -nan     0.1000    0.0303\n     2        1.2246            -nan     0.1000    0.0277\n     3        1.1786            -nan     0.1000    0.0222\n     4        1.1387            -nan     0.1000    0.0198\n     5        1.0986            -nan     0.1000    0.0198\n     6        1.0623            -nan     0.1000    0.0157\n     7        1.0311            -nan     0.1000    0.0148\n     8        1.0021            -nan     0.1000    0.0129\n     9        0.9761            -nan     0.1000    0.0128\n    10        0.9495            -nan     0.1000    0.0126\n    20        0.7779            -nan     0.1000    0.0048\n    40        0.5967            -nan     0.1000    0.0022\n    60        0.5053            -nan     0.1000    0.0016\n    80        0.4498            -nan     0.1000    0.0010\n   100        0.4117            -nan     0.1000    0.0006\n   120        0.3864            -nan     0.1000    0.0009\n   140        0.3642            -nan     0.1000    0.0002\n   160        0.3490            -nan     0.1000    0.0004\n   180        0.3341            -nan     0.1000   -0.0001\n   200        0.3228            -nan     0.1000    0.0000\n   220        0.3145            -nan     0.1000   -0.0000\n   240        0.3061            -nan     0.1000    0.0001\n   250        0.3027            -nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2483            -nan     0.1000    0.0444\n     2        1.1782            -nan     0.1000    0.0347\n     3        1.1140            -nan     0.1000    0.0309\n     4        1.0550            -nan     0.1000    0.0275\n     5        1.0051            -nan     0.1000    0.0235\n     6        0.9636            -nan     0.1000    0.0198\n     7        0.9258            -nan     0.1000    0.0182\n     8        0.8934            -nan     0.1000    0.0150\n     9        0.8612            -nan     0.1000    0.0150\n    10        0.8360            -nan     0.1000    0.0124\n    20        0.6347            -nan     0.1000    0.0065\n    40        0.4511            -nan     0.1000    0.0018\n    60        0.3748            -nan     0.1000    0.0007\n    80        0.3372            -nan     0.1000    0.0006\n   100        0.3109            -nan     0.1000    0.0001\n   120        0.2925            -nan     0.1000    0.0002\n   140        0.2785            -nan     0.1000   -0.0000\n   160        0.2641            -nan     0.1000   -0.0001\n   180        0.2536            -nan     0.1000   -0.0002\n   200        0.2429            -nan     0.1000   -0.0001\n   220        0.2360            -nan     0.1000    0.0003\n   240        0.2291            -nan     0.1000   -0.0003\n   250        0.2262            -nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2440            -nan     0.1000    0.0458\n     2        1.1517            -nan     0.1000    0.0443\n     3        1.0845            -nan     0.1000    0.0319\n     4        1.0185            -nan     0.1000    0.0309\n     5        0.9661            -nan     0.1000    0.0255\n     6        0.9196            -nan     0.1000    0.0219\n     7        0.8787            -nan     0.1000    0.0198\n     8        0.8408            -nan     0.1000    0.0182\n     9        0.8049            -nan     0.1000    0.0172\n    10        0.7749            -nan     0.1000    0.0138\n    20        0.5722            -nan     0.1000    0.0069\n    40        0.3905            -nan     0.1000    0.0019\n    60        0.3230            -nan     0.1000    0.0008\n    80        0.2885            -nan     0.1000    0.0004\n   100        0.2666            -nan     0.1000   -0.0001\n   120        0.2524            -nan     0.1000   -0.0003\n   140        0.2368            -nan     0.1000   -0.0003\n   160        0.2259            -nan     0.1000   -0.0003\n   180        0.2156            -nan     0.1000   -0.0003\n   200        0.2042            -nan     0.1000    0.0001\n   220        0.1955            -nan     0.1000   -0.0001\n   240        0.1890            -nan     0.1000    0.0001\n   250        0.1853            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2278            -nan     0.1000    0.0550\n     2        1.1409            -nan     0.1000    0.0411\n     3        1.0695            -nan     0.1000    0.0342\n     4        0.9953            -nan     0.1000    0.0348\n     5        0.9360            -nan     0.1000    0.0281\n     6        0.8845            -nan     0.1000    0.0249\n     7        0.8413            -nan     0.1000    0.0201\n     8        0.8019            -nan     0.1000    0.0190\n     9        0.7671            -nan     0.1000    0.0160\n    10        0.7350            -nan     0.1000    0.0151\n    20        0.5253            -nan     0.1000    0.0111\n    40        0.3603            -nan     0.1000    0.0010\n    60        0.2945            -nan     0.1000   -0.0003\n    80        0.2632            -nan     0.1000   -0.0002\n   100        0.2399            -nan     0.1000   -0.0001\n   120        0.2223            -nan     0.1000   -0.0002\n   140        0.2053            -nan     0.1000   -0.0001\n   160        0.1931            -nan     0.1000   -0.0004\n   180        0.1822            -nan     0.1000   -0.0004\n   200        0.1741            -nan     0.1000   -0.0005\n   220        0.1651            -nan     0.1000   -0.0002\n   240        0.1579            -nan     0.1000   -0.0001\n   250        0.1539            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2303            -nan     0.1000    0.0536\n     2        1.1335            -nan     0.1000    0.0472\n     3        1.0480            -nan     0.1000    0.0411\n     4        0.9790            -nan     0.1000    0.0338\n     5        0.9200            -nan     0.1000    0.0269\n     6        0.8683            -nan     0.1000    0.0246\n     7        0.8241            -nan     0.1000    0.0206\n     8        0.7813            -nan     0.1000    0.0202\n     9        0.7480            -nan     0.1000    0.0161\n    10        0.7178            -nan     0.1000    0.0142\n    20        0.5055            -nan     0.1000    0.0067\n    40        0.3404            -nan     0.1000    0.0015\n    60        0.2760            -nan     0.1000    0.0005\n    80        0.2444            -nan     0.1000   -0.0005\n   100        0.2194            -nan     0.1000   -0.0002\n   120        0.2005            -nan     0.1000   -0.0002\n   140        0.1843            -nan     0.1000   -0.0006\n   160        0.1722            -nan     0.1000   -0.0003\n   180        0.1592            -nan     0.1000   -0.0004\n   200        0.1477            -nan     0.1000   -0.0000\n   220        0.1377            -nan     0.1000   -0.0002\n   240        0.1297            -nan     0.1000   -0.0001\n   250        0.1265            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2285            -nan     0.1000    0.0570\n     2        1.1402            -nan     0.1000    0.0417\n     3        1.0619            -nan     0.1000    0.0390\n     4        0.9958            -nan     0.1000    0.0329\n     5        0.9347            -nan     0.1000    0.0293\n     6        0.8878            -nan     0.1000    0.0230\n     7        0.8489            -nan     0.1000    0.0192\n     8        0.8086            -nan     0.1000    0.0190\n     9        0.7718            -nan     0.1000    0.0171\n    10        0.7396            -nan     0.1000    0.0151\n    20        0.5292            -nan     0.1000    0.0090\n    40        0.3602            -nan     0.1000    0.0017\n    60        0.3024            -nan     0.1000    0.0003\n    80        0.2720            -nan     0.1000    0.0001\n   100        0.2531            -nan     0.1000   -0.0007\n   120        0.2375            -nan     0.1000   -0.0004\n   140        0.2229            -nan     0.1000    0.0004\n   160        0.2104            -nan     0.1000   -0.0002\n   180        0.1998            -nan     0.1000   -0.0001\n   200        0.1893            -nan     0.1000    0.0000\n   220        0.1819            -nan     0.1000   -0.0002\n   240        0.1742            -nan     0.1000   -0.0002\n   250        0.1693            -nan     0.1000   -0.0002\n\n\nSe entreno el modelo, con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods \nresults &lt;- resamples(models) \nsummary(results) \n\n\nCall:\nsummary.resamples(object = results)\n\nModels: rf, svmRadial, knn, gbm \nNumber of resamples: 5 \n\nAccuracy \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.9375000 0.9443691 0.9497965 0.9486701 0.9524457 0.9592391    0\nsvmRadial 0.9144022 0.9293478 0.9294437 0.9291133 0.9335142 0.9388587    0\nknn       0.8968792 0.8994565 0.9009498 0.9044028 0.9103261 0.9144022    0\ngbm       0.9402174 0.9442935 0.9511533 0.9489385 0.9538043 0.9552239    0\n\nKappa \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.8680057 0.8828194 0.8945004 0.8919135 0.9001163 0.9141257    0\nsvmRadial 0.8199916 0.8503670 0.8509704 0.8503147 0.8594480 0.8707966    0\nknn       0.7844764 0.7884335 0.7893354 0.7983825 0.8094581 0.8202094    0\ngbm       0.8734367 0.8827096 0.8975409 0.8926848 0.9032627 0.9064739    0\n\n\nResumen estadístico del Accuracy y Kappa de cada modelo.\n\ndotplot(results) \n\n\n\n\n\n\n\n\nComo se observa en la gráfica, tanto el rf como el gbm tienes excelentes resultados.\n\nbest_model &lt;- models[[\"rf\"]] \npredictions &lt;- predict(best_model, newdata = testTransformed) \ncm = confusionMatrix(predictions, testTransformed$type) \ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction nonspam spam\n   nonspam     541   29\n   spam         16  333\n                                         \n               Accuracy : 0.951          \n                 95% CI : (0.935, 0.9641)\n    No Information Rate : 0.6061         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8968         \n                                         \n Mcnemar's Test P-Value : 0.07364        \n                                         \n            Sensitivity : 0.9713         \n            Specificity : 0.9199         \n         Pos Pred Value : 0.9491         \n         Neg Pred Value : 0.9542         \n             Prevalence : 0.6061         \n         Detection Rate : 0.5887         \n   Detection Prevalence : 0.6202         \n      Balanced Accuracy : 0.9456         \n                                         \n       'Positive' Class : nonspam        \n                                         \n\n\nEn los datos de prueba, se obtuvo excelentes resultados, a la hora de clasificar entre spam y no spam. Con un 95% de Accuracy\n\nggplot( as.data.frame(cm$table), aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), vjust = 1.5, color = \"black\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"lightyellow\") +\n  labs(title = \"Matriz de Confusión en test\", x = \"Clase Real\", y = \"Clase Predicha\")\n\n\n\n\n\n\n\n\nGráfico de la matriz de confusión en el test y se observa que el modelo tiene buen rendimiento al clasificar el correo spam\n\nimportance &lt;- varImp(best_model, scale = TRUE) \nplot(importance, top = 5)  \n\n\n\n\n\n\n\n\nEl gráfico muestra el top 5 de las variables mas importantes dentro de las 57 que existen."
  },
  {
    "objectID": "r/r_spam.html#cargar-las-librerias-y-el-dataset",
    "href": "r/r_spam.html#cargar-las-librerias-y-el-dataset",
    "title": "Dataset Spam",
    "section": "",
    "text": "library(caret)\nlibrary(kernlab)\n\nCargar el dataset dentro de la variable datos\n\ndata(spam)\ndatos  = spam\n\nInformación del dataset\n\ndim(datos) \n\n[1] 4601   58\n\n\nEl dataset tiene 4601 observaciones y 58 variables\n\nstr(datos) \n\n'data.frame':   4601 obs. of  58 variables:\n $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...\n $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...\n $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...\n $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ our              : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...\n $ over             : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...\n $ remove           : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...\n $ internet         : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...\n $ order            : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...\n $ mail             : num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...\n $ receive          : num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...\n $ will             : num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...\n $ people           : num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...\n $ report           : num  0 0.21 0 0 0 0 0 0 0 0 ...\n $ addresses        : num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...\n $ free             : num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...\n $ business         : num  0 0.07 0.06 0 0 0 0 0 0 0 ...\n $ email            : num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...\n $ you              : num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...\n $ credit           : num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...\n $ your             : num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...\n $ font             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num000           : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...\n $ money            : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...\n $ hp               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ hpl              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ george           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num650           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ lab              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ labs             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ telnet           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num857           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ data             : num  0 0 0 0 0 0 0 0 0.15 0 ...\n $ num415           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num85            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ technology       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ num1999          : num  0 0.07 0 0 0 0 0 0 0 0 ...\n $ parts            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ pm               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ direct           : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ cs               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ meeting          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ original         : num  0 0 0.12 0 0 0 0 0 0.3 0 ...\n $ project          : num  0 0 0 0 0 0 0 0 0 0.06 ...\n $ re               : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ edu              : num  0 0 0.06 0 0 0 0 0 0 0 ...\n $ table            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ conference       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ charSemicolon    : num  0 0 0.01 0 0 0 0 0 0 0.04 ...\n $ charRoundbracket : num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...\n $ charSquarebracket: num  0 0 0 0 0 0 0 0 0 0 ...\n $ charExclamation  : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...\n $ charDollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...\n $ charHash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...\n $ capitalAve       : num  3.76 5.11 9.82 3.54 3.54 ...\n $ capitalLong      : num  61 101 485 40 40 15 4 11 445 43 ...\n $ capitalTotal     : num  278 1028 2259 191 191 ...\n $ type             : Factor w/ 2 levels \"nonspam\",\"spam\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nSe observa que todos las variables son numéricas, excepto type que es categórica, es la variable a analizar\n\ncolSums(is.na(datos)) \n\n             make           address               all             num3d \n                0                 0                 0                 0 \n              our              over            remove          internet \n                0                 0                 0                 0 \n            order              mail           receive              will \n                0                 0                 0                 0 \n           people            report         addresses              free \n                0                 0                 0                 0 \n         business             email               you            credit \n                0                 0                 0                 0 \n             your              font            num000             money \n                0                 0                 0                 0 \n               hp               hpl            george            num650 \n                0                 0                 0                 0 \n              lab              labs            telnet            num857 \n                0                 0                 0                 0 \n             data            num415             num85        technology \n                0                 0                 0                 0 \n          num1999             parts                pm            direct \n                0                 0                 0                 0 \n               cs           meeting          original           project \n                0                 0                 0                 0 \n               re               edu             table        conference \n                0                 0                 0                 0 \n    charSemicolon  charRoundbracket charSquarebracket   charExclamation \n                0                 0                 0                 0 \n       charDollar          charHash        capitalAve       capitalLong \n                0                 0                 0                 0 \n     capitalTotal              type \n                0                 0 \n\n\nEl dataset no contiene valores faltantes\n\ntable(datos$type) \n\n\nnonspam    spam \n   2788    1813 \n\n\nExiste 2788 correos “no spam” y 1813 con spam"
  },
  {
    "objectID": "r/r_spam.html#modelado",
    "href": "r/r_spam.html#modelado",
    "title": "Dataset Spam",
    "section": "",
    "text": "set.seed(123) \ntrain &lt;- createDataPartition(datos$type, p = 0.8, list = FALSE) \ntrainData &lt;- datos[train, ] \ntestData  &lt;- datos[-train, ] \n\nSe divide el dataset en 80% train y 20% test\n\npreProcValues &lt;- preProcess(trainData[,1:57], method = c(\"center\", \"scale\")) \ntrainTransformed &lt;- predict(preProcValues, trainData[, 1:57]) \ntestTransformed  &lt;- predict(preProcValues, testData[, 1:57]) \n\nSe realiza un escalado y centrado de las variables\n\ntrainTransformed$type &lt;- trainData$type \ntestTransformed$type  &lt;- testData$type \n\nReconstruimos el datasets con la variable objetivo\n\nset.seed(123) \nctrl &lt;- trainControl(method = \"cv\", number = 5) \n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)  \nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\") \n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbour y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) {\n  train(type ~ .,\n        data = trainTransformed,\n        method = m,\n        trControl = ctrl, \n        tuneLength = 5)\n  }) \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2790            -nan     0.1000    0.0294\n     2        1.2238            -nan     0.1000    0.0274\n     3        1.1765            -nan     0.1000    0.0222\n     4        1.1342            -nan     0.1000    0.0206\n     5        1.0970            -nan     0.1000    0.0174\n     6        1.0630            -nan     0.1000    0.0169\n     7        1.0297            -nan     0.1000    0.0163\n     8        1.0014            -nan     0.1000    0.0140\n     9        0.9745            -nan     0.1000    0.0119\n    10        0.9489            -nan     0.1000    0.0114\n    20        0.7761            -nan     0.1000    0.0071\n    40        0.5933            -nan     0.1000    0.0025\n    60        0.5080            -nan     0.1000    0.0009\n    80        0.4534            -nan     0.1000    0.0004\n   100        0.4184            -nan     0.1000    0.0011\n   120        0.3923            -nan     0.1000    0.0000\n   140        0.3734            -nan     0.1000    0.0003\n   160        0.3601            -nan     0.1000    0.0000\n   180        0.3491            -nan     0.1000   -0.0001\n   200        0.3377            -nan     0.1000    0.0000\n   220        0.3298            -nan     0.1000    0.0001\n   240        0.3215            -nan     0.1000   -0.0002\n   250        0.3196            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2619            -nan     0.1000    0.0373\n     2        1.1822            -nan     0.1000    0.0387\n     3        1.1239            -nan     0.1000    0.0289\n     4        1.0637            -nan     0.1000    0.0296\n     5        1.0167            -nan     0.1000    0.0226\n     6        0.9745            -nan     0.1000    0.0204\n     7        0.9326            -nan     0.1000    0.0202\n     8        0.8934            -nan     0.1000    0.0181\n     9        0.8631            -nan     0.1000    0.0140\n    10        0.8368            -nan     0.1000    0.0123\n    20        0.6315            -nan     0.1000    0.0064\n    40        0.4537            -nan     0.1000    0.0024\n    60        0.3860            -nan     0.1000   -0.0001\n    80        0.3491            -nan     0.1000    0.0004\n   100        0.3251            -nan     0.1000   -0.0002\n   120        0.3075            -nan     0.1000   -0.0003\n   140        0.2925            -nan     0.1000   -0.0000\n   160        0.2792            -nan     0.1000   -0.0002\n   180        0.2669            -nan     0.1000   -0.0003\n   200        0.2587            -nan     0.1000   -0.0001\n   220        0.2521            -nan     0.1000   -0.0003\n   240        0.2445            -nan     0.1000    0.0000\n   250        0.2414            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2380            -nan     0.1000    0.0512\n     2        1.1593            -nan     0.1000    0.0381\n     3        1.0842            -nan     0.1000    0.0359\n     4        1.0260            -nan     0.1000    0.0277\n     5        0.9706            -nan     0.1000    0.0280\n     6        0.9253            -nan     0.1000    0.0212\n     7        0.8840            -nan     0.1000    0.0191\n     8        0.8463            -nan     0.1000    0.0187\n     9        0.8133            -nan     0.1000    0.0156\n    10        0.7818            -nan     0.1000    0.0155\n    20        0.5737            -nan     0.1000    0.0071\n    40        0.4007            -nan     0.1000    0.0016\n    60        0.3349            -nan     0.1000    0.0005\n    80        0.3019            -nan     0.1000    0.0000\n   100        0.2827            -nan     0.1000   -0.0003\n   120        0.2660            -nan     0.1000   -0.0005\n   140        0.2504            -nan     0.1000   -0.0001\n   160        0.2385            -nan     0.1000   -0.0001\n   180        0.2277            -nan     0.1000    0.0002\n   200        0.2200            -nan     0.1000   -0.0004\n   220        0.2102            -nan     0.1000   -0.0004\n   240        0.2024            -nan     0.1000   -0.0001\n   250        0.1985            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2384            -nan     0.1000    0.0501\n     2        1.1431            -nan     0.1000    0.0459\n     3        1.0677            -nan     0.1000    0.0382\n     4        1.0049            -nan     0.1000    0.0312\n     5        0.9507            -nan     0.1000    0.0258\n     6        0.8981            -nan     0.1000    0.0265\n     7        0.8506            -nan     0.1000    0.0222\n     8        0.8133            -nan     0.1000    0.0176\n     9        0.7768            -nan     0.1000    0.0175\n    10        0.7474            -nan     0.1000    0.0132\n    20        0.5313            -nan     0.1000    0.0094\n    40        0.3736            -nan     0.1000    0.0010\n    60        0.3126            -nan     0.1000    0.0003\n    80        0.2768            -nan     0.1000    0.0005\n   100        0.2535            -nan     0.1000   -0.0002\n   120        0.2382            -nan     0.1000   -0.0002\n   140        0.2240            -nan     0.1000   -0.0003\n   160        0.2094            -nan     0.1000   -0.0003\n   180        0.1960            -nan     0.1000   -0.0001\n   200        0.1868            -nan     0.1000   -0.0002\n   220        0.1774            -nan     0.1000   -0.0001\n   240        0.1665            -nan     0.1000   -0.0002\n   250        0.1614            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2328            -nan     0.1000    0.0514\n     2        1.1327            -nan     0.1000    0.0501\n     3        1.0519            -nan     0.1000    0.0388\n     4        0.9835            -nan     0.1000    0.0317\n     5        0.9245            -nan     0.1000    0.0275\n     6        0.8708            -nan     0.1000    0.0251\n     7        0.8270            -nan     0.1000    0.0209\n     8        0.7870            -nan     0.1000    0.0194\n     9        0.7499            -nan     0.1000    0.0180\n    10        0.7175            -nan     0.1000    0.0143\n    20        0.5060            -nan     0.1000    0.0093\n    40        0.3435            -nan     0.1000    0.0012\n    60        0.2822            -nan     0.1000    0.0001\n    80        0.2542            -nan     0.1000    0.0000\n   100        0.2328            -nan     0.1000   -0.0001\n   120        0.2132            -nan     0.1000   -0.0004\n   140        0.1963            -nan     0.1000    0.0001\n   160        0.1818            -nan     0.1000   -0.0001\n   180        0.1714            -nan     0.1000   -0.0003\n   200        0.1590            -nan     0.1000   -0.0003\n   220        0.1496            -nan     0.1000   -0.0003\n   240        0.1415            -nan     0.1000   -0.0002\n   250        0.1375            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2797            -nan     0.1000    0.0295\n     2        1.2225            -nan     0.1000    0.0289\n     3        1.1766            -nan     0.1000    0.0211\n     4        1.1304            -nan     0.1000    0.0214\n     5        1.0944            -nan     0.1000    0.0174\n     6        1.0598            -nan     0.1000    0.0167\n     7        1.0287            -nan     0.1000    0.0155\n     8        1.0014            -nan     0.1000    0.0133\n     9        0.9742            -nan     0.1000    0.0135\n    10        0.9494            -nan     0.1000    0.0119\n    20        0.7735            -nan     0.1000    0.0069\n    40        0.5909            -nan     0.1000    0.0024\n    60        0.4997            -nan     0.1000    0.0010\n    80        0.4440            -nan     0.1000    0.0006\n   100        0.4043            -nan     0.1000    0.0004\n   120        0.3767            -nan     0.1000   -0.0002\n   140        0.3564            -nan     0.1000    0.0005\n   160        0.3423            -nan     0.1000   -0.0004\n   180        0.3290            -nan     0.1000    0.0005\n   200        0.3209            -nan     0.1000   -0.0004\n   220        0.3129            -nan     0.1000   -0.0000\n   240        0.3046            -nan     0.1000    0.0000\n   250        0.3015            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2501            -nan     0.1000    0.0452\n     2        1.1788            -nan     0.1000    0.0338\n     3        1.1200            -nan     0.1000    0.0281\n     4        1.0632            -nan     0.1000    0.0285\n     5        1.0169            -nan     0.1000    0.0210\n     6        0.9738            -nan     0.1000    0.0209\n     7        0.9321            -nan     0.1000    0.0204\n     8        0.8985            -nan     0.1000    0.0167\n     9        0.8665            -nan     0.1000    0.0151\n    10        0.8363            -nan     0.1000    0.0138\n    20        0.6189            -nan     0.1000    0.0088\n    40        0.4503            -nan     0.1000    0.0026\n    60        0.3736            -nan     0.1000    0.0009\n    80        0.3315            -nan     0.1000    0.0003\n   100        0.3062            -nan     0.1000    0.0001\n   120        0.2894            -nan     0.1000   -0.0003\n   140        0.2771            -nan     0.1000   -0.0002\n   160        0.2624            -nan     0.1000   -0.0001\n   180        0.2539            -nan     0.1000   -0.0002\n   200        0.2454            -nan     0.1000    0.0000\n   220        0.2388            -nan     0.1000   -0.0001\n   240        0.2302            -nan     0.1000   -0.0002\n   250        0.2266            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2464            -nan     0.1000    0.0489\n     2        1.1576            -nan     0.1000    0.0435\n     3        1.0883            -nan     0.1000    0.0338\n     4        1.0233            -nan     0.1000    0.0325\n     5        0.9700            -nan     0.1000    0.0258\n     6        0.9215            -nan     0.1000    0.0235\n     7        0.8812            -nan     0.1000    0.0190\n     8        0.8430            -nan     0.1000    0.0169\n     9        0.8074            -nan     0.1000    0.0174\n    10        0.7761            -nan     0.1000    0.0155\n    20        0.5647            -nan     0.1000    0.0106\n    40        0.3889            -nan     0.1000    0.0014\n    60        0.3281            -nan     0.1000    0.0002\n    80        0.2924            -nan     0.1000    0.0002\n   100        0.2679            -nan     0.1000   -0.0004\n   120        0.2501            -nan     0.1000   -0.0000\n   140        0.2368            -nan     0.1000   -0.0002\n   160        0.2256            -nan     0.1000   -0.0002\n   180        0.2155            -nan     0.1000   -0.0003\n   200        0.2055            -nan     0.1000   -0.0001\n   220        0.1963            -nan     0.1000   -0.0001\n   240        0.1883            -nan     0.1000    0.0001\n   250        0.1851            -nan     0.1000   -0.0004\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2365            -nan     0.1000    0.0500\n     2        1.1373            -nan     0.1000    0.0471\n     3        1.0613            -nan     0.1000    0.0357\n     4        0.9900            -nan     0.1000    0.0352\n     5        0.9319            -nan     0.1000    0.0283\n     6        0.8845            -nan     0.1000    0.0227\n     7        0.8417            -nan     0.1000    0.0204\n     8        0.8016            -nan     0.1000    0.0197\n     9        0.7677            -nan     0.1000    0.0159\n    10        0.7330            -nan     0.1000    0.0170\n    20        0.5216            -nan     0.1000    0.0079\n    40        0.3543            -nan     0.1000    0.0007\n    60        0.2898            -nan     0.1000    0.0005\n    80        0.2578            -nan     0.1000   -0.0001\n   100        0.2383            -nan     0.1000   -0.0001\n   120        0.2225            -nan     0.1000   -0.0004\n   140        0.2073            -nan     0.1000   -0.0003\n   160        0.1933            -nan     0.1000   -0.0004\n   180        0.1830            -nan     0.1000   -0.0004\n   200        0.1726            -nan     0.1000   -0.0001\n   220        0.1632            -nan     0.1000   -0.0002\n   240        0.1527            -nan     0.1000   -0.0001\n   250        0.1482            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2223            -nan     0.1000    0.0585\n     2        1.1319            -nan     0.1000    0.0435\n     3        1.0483            -nan     0.1000    0.0398\n     4        0.9765            -nan     0.1000    0.0343\n     5        0.9196            -nan     0.1000    0.0265\n     6        0.8667            -nan     0.1000    0.0253\n     7        0.8171            -nan     0.1000    0.0220\n     8        0.7766            -nan     0.1000    0.0190\n     9        0.7436            -nan     0.1000    0.0158\n    10        0.7119            -nan     0.1000    0.0143\n    20        0.4941            -nan     0.1000    0.0069\n    40        0.3272            -nan     0.1000    0.0027\n    60        0.2708            -nan     0.1000   -0.0003\n    80        0.2377            -nan     0.1000   -0.0002\n   100        0.2164            -nan     0.1000   -0.0003\n   120        0.1983            -nan     0.1000   -0.0002\n   140        0.1847            -nan     0.1000   -0.0003\n   160        0.1713            -nan     0.1000   -0.0003\n   180        0.1589            -nan     0.1000   -0.0005\n   200        0.1473            -nan     0.1000   -0.0002\n   220        0.1380            -nan     0.1000   -0.0001\n   240        0.1303            -nan     0.1000   -0.0004\n   250        0.1267            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2823            -nan     0.1000    0.0293\n     2        1.2254            -nan     0.1000    0.0285\n     3        1.1785            -nan     0.1000    0.0229\n     4        1.1353            -nan     0.1000    0.0201\n     5        1.0995            -nan     0.1000    0.0162\n     6        1.0641            -nan     0.1000    0.0175\n     7        1.0315            -nan     0.1000    0.0151\n     8        1.0030            -nan     0.1000    0.0134\n     9        0.9767            -nan     0.1000    0.0122\n    10        0.9535            -nan     0.1000    0.0104\n    20        0.7759            -nan     0.1000    0.0061\n    40        0.5977            -nan     0.1000    0.0026\n    60        0.5040            -nan     0.1000    0.0022\n    80        0.4512            -nan     0.1000    0.0004\n   100        0.4146            -nan     0.1000    0.0002\n   120        0.3874            -nan     0.1000    0.0003\n   140        0.3655            -nan     0.1000   -0.0001\n   160        0.3498            -nan     0.1000    0.0006\n   180        0.3364            -nan     0.1000    0.0001\n   200        0.3254            -nan     0.1000    0.0000\n   220        0.3153            -nan     0.1000   -0.0001\n   240        0.3084            -nan     0.1000    0.0001\n   250        0.3037            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2640            -nan     0.1000    0.0374\n     2        1.1837            -nan     0.1000    0.0396\n     3        1.1173            -nan     0.1000    0.0322\n     4        1.0651            -nan     0.1000    0.0261\n     5        1.0202            -nan     0.1000    0.0215\n     6        0.9759            -nan     0.1000    0.0211\n     7        0.9385            -nan     0.1000    0.0170\n     8        0.9014            -nan     0.1000    0.0184\n     9        0.8709            -nan     0.1000    0.0151\n    10        0.8397            -nan     0.1000    0.0148\n    20        0.6309            -nan     0.1000    0.0063\n    40        0.4512            -nan     0.1000    0.0018\n    60        0.3824            -nan     0.1000    0.0003\n    80        0.3426            -nan     0.1000    0.0003\n   100        0.3169            -nan     0.1000    0.0004\n   120        0.2964            -nan     0.1000    0.0000\n   140        0.2801            -nan     0.1000   -0.0005\n   160        0.2706            -nan     0.1000   -0.0001\n   180        0.2588            -nan     0.1000   -0.0001\n   200        0.2502            -nan     0.1000   -0.0000\n   220        0.2441            -nan     0.1000   -0.0001\n   240        0.2370            -nan     0.1000   -0.0002\n   250        0.2340            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2365            -nan     0.1000    0.0505\n     2        1.1621            -nan     0.1000    0.0365\n     3        1.0878            -nan     0.1000    0.0370\n     4        1.0272            -nan     0.1000    0.0288\n     5        0.9764            -nan     0.1000    0.0241\n     6        0.9267            -nan     0.1000    0.0246\n     7        0.8868            -nan     0.1000    0.0197\n     8        0.8455            -nan     0.1000    0.0196\n     9        0.8094            -nan     0.1000    0.0168\n    10        0.7806            -nan     0.1000    0.0135\n    20        0.5647            -nan     0.1000    0.0061\n    40        0.3954            -nan     0.1000    0.0022\n    60        0.3348            -nan     0.1000    0.0000\n    80        0.3003            -nan     0.1000    0.0000\n   100        0.2761            -nan     0.1000   -0.0001\n   120        0.2599            -nan     0.1000   -0.0003\n   140        0.2439            -nan     0.1000   -0.0004\n   160        0.2309            -nan     0.1000   -0.0001\n   180        0.2208            -nan     0.1000   -0.0001\n   200        0.2105            -nan     0.1000   -0.0000\n   220        0.2012            -nan     0.1000   -0.0002\n   240        0.1949            -nan     0.1000   -0.0002\n   250        0.1914            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2327            -nan     0.1000    0.0541\n     2        1.1402            -nan     0.1000    0.0469\n     3        1.0660            -nan     0.1000    0.0358\n     4        1.0016            -nan     0.1000    0.0304\n     5        0.9455            -nan     0.1000    0.0271\n     6        0.8908            -nan     0.1000    0.0261\n     7        0.8468            -nan     0.1000    0.0204\n     8        0.8073            -nan     0.1000    0.0183\n     9        0.7731            -nan     0.1000    0.0169\n    10        0.7402            -nan     0.1000    0.0147\n    20        0.5251            -nan     0.1000    0.0061\n    40        0.3630            -nan     0.1000    0.0021\n    60        0.3027            -nan     0.1000    0.0003\n    80        0.2688            -nan     0.1000   -0.0001\n   100        0.2484            -nan     0.1000   -0.0001\n   120        0.2310            -nan     0.1000    0.0001\n   140        0.2153            -nan     0.1000   -0.0003\n   160        0.2031            -nan     0.1000   -0.0002\n   180        0.1923            -nan     0.1000   -0.0002\n   200        0.1822            -nan     0.1000   -0.0003\n   220        0.1719            -nan     0.1000   -0.0003\n   240        0.1630            -nan     0.1000   -0.0001\n   250        0.1595            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2219            -nan     0.1000    0.0590\n     2        1.1333            -nan     0.1000    0.0436\n     3        1.0553            -nan     0.1000    0.0376\n     4        0.9928            -nan     0.1000    0.0306\n     5        0.9299            -nan     0.1000    0.0305\n     6        0.8791            -nan     0.1000    0.0232\n     7        0.8308            -nan     0.1000    0.0231\n     8        0.7891            -nan     0.1000    0.0193\n     9        0.7509            -nan     0.1000    0.0180\n    10        0.7183            -nan     0.1000    0.0155\n    20        0.4999            -nan     0.1000    0.0099\n    40        0.3382            -nan     0.1000    0.0014\n    60        0.2798            -nan     0.1000    0.0002\n    80        0.2470            -nan     0.1000    0.0001\n   100        0.2241            -nan     0.1000   -0.0002\n   120        0.2087            -nan     0.1000   -0.0001\n   140        0.1931            -nan     0.1000   -0.0003\n   160        0.1781            -nan     0.1000   -0.0004\n   180        0.1675            -nan     0.1000   -0.0005\n   200        0.1585            -nan     0.1000   -0.0003\n   220        0.1491            -nan     0.1000   -0.0003\n   240        0.1405            -nan     0.1000   -0.0004\n   250        0.1357            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2724            -nan     0.1000    0.0344\n     2        1.2136            -nan     0.1000    0.0276\n     3        1.1671            -nan     0.1000    0.0233\n     4        1.1266            -nan     0.1000    0.0194\n     5        1.0861            -nan     0.1000    0.0199\n     6        1.0521            -nan     0.1000    0.0170\n     7        1.0206            -nan     0.1000    0.0154\n     8        0.9915            -nan     0.1000    0.0143\n     9        0.9646            -nan     0.1000    0.0133\n    10        0.9399            -nan     0.1000    0.0108\n    20        0.7629            -nan     0.1000    0.0067\n    40        0.5878            -nan     0.1000    0.0023\n    60        0.4985            -nan     0.1000    0.0009\n    80        0.4421            -nan     0.1000    0.0008\n   100        0.4063            -nan     0.1000    0.0002\n   120        0.3809            -nan     0.1000   -0.0000\n   140        0.3617            -nan     0.1000    0.0001\n   160        0.3467            -nan     0.1000    0.0001\n   180        0.3348            -nan     0.1000   -0.0001\n   200        0.3247            -nan     0.1000   -0.0001\n   220        0.3156            -nan     0.1000    0.0001\n   240        0.3056            -nan     0.1000    0.0001\n   250        0.3033            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2565            -nan     0.1000    0.0390\n     2        1.1715            -nan     0.1000    0.0413\n     3        1.1032            -nan     0.1000    0.0329\n     4        1.0484            -nan     0.1000    0.0262\n     5        0.9997            -nan     0.1000    0.0235\n     6        0.9585            -nan     0.1000    0.0195\n     7        0.9175            -nan     0.1000    0.0197\n     8        0.8821            -nan     0.1000    0.0170\n     9        0.8484            -nan     0.1000    0.0164\n    10        0.8175            -nan     0.1000    0.0140\n    20        0.6167            -nan     0.1000    0.0064\n    40        0.4457            -nan     0.1000    0.0019\n    60        0.3755            -nan     0.1000    0.0008\n    80        0.3369            -nan     0.1000    0.0005\n   100        0.3141            -nan     0.1000    0.0000\n   120        0.2976            -nan     0.1000    0.0003\n   140        0.2825            -nan     0.1000    0.0001\n   160        0.2708            -nan     0.1000   -0.0001\n   180        0.2624            -nan     0.1000   -0.0001\n   200        0.2536            -nan     0.1000   -0.0001\n   220        0.2449            -nan     0.1000   -0.0002\n   240        0.2387            -nan     0.1000   -0.0003\n   250        0.2359            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2287            -nan     0.1000    0.0556\n     2        1.1472            -nan     0.1000    0.0410\n     3        1.0719            -nan     0.1000    0.0371\n     4        1.0104            -nan     0.1000    0.0316\n     5        0.9589            -nan     0.1000    0.0252\n     6        0.9111            -nan     0.1000    0.0230\n     7        0.8693            -nan     0.1000    0.0208\n     8        0.8348            -nan     0.1000    0.0169\n     9        0.8005            -nan     0.1000    0.0168\n    10        0.7682            -nan     0.1000    0.0148\n    20        0.5600            -nan     0.1000    0.0102\n    40        0.3880            -nan     0.1000    0.0012\n    60        0.3237            -nan     0.1000    0.0009\n    80        0.2911            -nan     0.1000   -0.0001\n   100        0.2675            -nan     0.1000    0.0002\n   120        0.2517            -nan     0.1000   -0.0009\n   140        0.2388            -nan     0.1000   -0.0002\n   160        0.2281            -nan     0.1000    0.0003\n   180        0.2187            -nan     0.1000   -0.0002\n   200        0.2101            -nan     0.1000   -0.0003\n   220        0.1992            -nan     0.1000   -0.0002\n   240        0.1920            -nan     0.1000   -0.0001\n   250        0.1881            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2345            -nan     0.1000    0.0514\n     2        1.1311            -nan     0.1000    0.0499\n     3        1.0566            -nan     0.1000    0.0356\n     4        0.9894            -nan     0.1000    0.0335\n     5        0.9333            -nan     0.1000    0.0275\n     6        0.8789            -nan     0.1000    0.0261\n     7        0.8358            -nan     0.1000    0.0206\n     8        0.7925            -nan     0.1000    0.0207\n     9        0.7578            -nan     0.1000    0.0159\n    10        0.7277            -nan     0.1000    0.0136\n    20        0.5102            -nan     0.1000    0.0096\n    40        0.3532            -nan     0.1000    0.0013\n    60        0.2899            -nan     0.1000   -0.0002\n    80        0.2584            -nan     0.1000    0.0006\n   100        0.2379            -nan     0.1000   -0.0006\n   120        0.2202            -nan     0.1000   -0.0000\n   140        0.2068            -nan     0.1000   -0.0003\n   160        0.1950            -nan     0.1000   -0.0002\n   180        0.1839            -nan     0.1000   -0.0002\n   200        0.1733            -nan     0.1000   -0.0003\n   220        0.1654            -nan     0.1000   -0.0001\n   240        0.1572            -nan     0.1000   -0.0001\n   250        0.1535            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2252            -nan     0.1000    0.0575\n     2        1.1299            -nan     0.1000    0.0462\n     3        1.0419            -nan     0.1000    0.0422\n     4        0.9741            -nan     0.1000    0.0320\n     5        0.9112            -nan     0.1000    0.0299\n     6        0.8619            -nan     0.1000    0.0233\n     7        0.8139            -nan     0.1000    0.0229\n     8        0.7713            -nan     0.1000    0.0193\n     9        0.7379            -nan     0.1000    0.0148\n    10        0.7062            -nan     0.1000    0.0146\n    20        0.4935            -nan     0.1000    0.0061\n    40        0.3363            -nan     0.1000    0.0015\n    60        0.2782            -nan     0.1000    0.0010\n    80        0.2439            -nan     0.1000   -0.0002\n   100        0.2212            -nan     0.1000   -0.0009\n   120        0.2043            -nan     0.1000   -0.0005\n   140        0.1860            -nan     0.1000   -0.0003\n   160        0.1746            -nan     0.1000   -0.0002\n   180        0.1636            -nan     0.1000   -0.0001\n   200        0.1517            -nan     0.1000   -0.0001\n   220        0.1405            -nan     0.1000   -0.0003\n   240        0.1317            -nan     0.1000   -0.0001\n   250        0.1267            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2794            -nan     0.1000    0.0303\n     2        1.2246            -nan     0.1000    0.0277\n     3        1.1786            -nan     0.1000    0.0222\n     4        1.1387            -nan     0.1000    0.0198\n     5        1.0986            -nan     0.1000    0.0198\n     6        1.0623            -nan     0.1000    0.0157\n     7        1.0311            -nan     0.1000    0.0148\n     8        1.0021            -nan     0.1000    0.0129\n     9        0.9761            -nan     0.1000    0.0128\n    10        0.9495            -nan     0.1000    0.0126\n    20        0.7779            -nan     0.1000    0.0048\n    40        0.5967            -nan     0.1000    0.0022\n    60        0.5053            -nan     0.1000    0.0016\n    80        0.4498            -nan     0.1000    0.0010\n   100        0.4117            -nan     0.1000    0.0006\n   120        0.3864            -nan     0.1000    0.0009\n   140        0.3642            -nan     0.1000    0.0002\n   160        0.3490            -nan     0.1000    0.0004\n   180        0.3341            -nan     0.1000   -0.0001\n   200        0.3228            -nan     0.1000    0.0000\n   220        0.3145            -nan     0.1000   -0.0000\n   240        0.3061            -nan     0.1000    0.0001\n   250        0.3027            -nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2483            -nan     0.1000    0.0444\n     2        1.1782            -nan     0.1000    0.0347\n     3        1.1140            -nan     0.1000    0.0309\n     4        1.0550            -nan     0.1000    0.0275\n     5        1.0051            -nan     0.1000    0.0235\n     6        0.9636            -nan     0.1000    0.0198\n     7        0.9258            -nan     0.1000    0.0182\n     8        0.8934            -nan     0.1000    0.0150\n     9        0.8612            -nan     0.1000    0.0150\n    10        0.8360            -nan     0.1000    0.0124\n    20        0.6347            -nan     0.1000    0.0065\n    40        0.4511            -nan     0.1000    0.0018\n    60        0.3748            -nan     0.1000    0.0007\n    80        0.3372            -nan     0.1000    0.0006\n   100        0.3109            -nan     0.1000    0.0001\n   120        0.2925            -nan     0.1000    0.0002\n   140        0.2785            -nan     0.1000   -0.0000\n   160        0.2641            -nan     0.1000   -0.0001\n   180        0.2536            -nan     0.1000   -0.0002\n   200        0.2429            -nan     0.1000   -0.0001\n   220        0.2360            -nan     0.1000    0.0003\n   240        0.2291            -nan     0.1000   -0.0003\n   250        0.2262            -nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2440            -nan     0.1000    0.0458\n     2        1.1517            -nan     0.1000    0.0443\n     3        1.0845            -nan     0.1000    0.0319\n     4        1.0185            -nan     0.1000    0.0309\n     5        0.9661            -nan     0.1000    0.0255\n     6        0.9196            -nan     0.1000    0.0219\n     7        0.8787            -nan     0.1000    0.0198\n     8        0.8408            -nan     0.1000    0.0182\n     9        0.8049            -nan     0.1000    0.0172\n    10        0.7749            -nan     0.1000    0.0138\n    20        0.5722            -nan     0.1000    0.0069\n    40        0.3905            -nan     0.1000    0.0019\n    60        0.3230            -nan     0.1000    0.0008\n    80        0.2885            -nan     0.1000    0.0004\n   100        0.2666            -nan     0.1000   -0.0001\n   120        0.2524            -nan     0.1000   -0.0003\n   140        0.2368            -nan     0.1000   -0.0003\n   160        0.2259            -nan     0.1000   -0.0003\n   180        0.2156            -nan     0.1000   -0.0003\n   200        0.2042            -nan     0.1000    0.0001\n   220        0.1955            -nan     0.1000   -0.0001\n   240        0.1890            -nan     0.1000    0.0001\n   250        0.1853            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2278            -nan     0.1000    0.0550\n     2        1.1409            -nan     0.1000    0.0411\n     3        1.0695            -nan     0.1000    0.0342\n     4        0.9953            -nan     0.1000    0.0348\n     5        0.9360            -nan     0.1000    0.0281\n     6        0.8845            -nan     0.1000    0.0249\n     7        0.8413            -nan     0.1000    0.0201\n     8        0.8019            -nan     0.1000    0.0190\n     9        0.7671            -nan     0.1000    0.0160\n    10        0.7350            -nan     0.1000    0.0151\n    20        0.5253            -nan     0.1000    0.0111\n    40        0.3603            -nan     0.1000    0.0010\n    60        0.2945            -nan     0.1000   -0.0003\n    80        0.2632            -nan     0.1000   -0.0002\n   100        0.2399            -nan     0.1000   -0.0001\n   120        0.2223            -nan     0.1000   -0.0002\n   140        0.2053            -nan     0.1000   -0.0001\n   160        0.1931            -nan     0.1000   -0.0004\n   180        0.1822            -nan     0.1000   -0.0004\n   200        0.1741            -nan     0.1000   -0.0005\n   220        0.1651            -nan     0.1000   -0.0002\n   240        0.1579            -nan     0.1000   -0.0001\n   250        0.1539            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2303            -nan     0.1000    0.0536\n     2        1.1335            -nan     0.1000    0.0472\n     3        1.0480            -nan     0.1000    0.0411\n     4        0.9790            -nan     0.1000    0.0338\n     5        0.9200            -nan     0.1000    0.0269\n     6        0.8683            -nan     0.1000    0.0246\n     7        0.8241            -nan     0.1000    0.0206\n     8        0.7813            -nan     0.1000    0.0202\n     9        0.7480            -nan     0.1000    0.0161\n    10        0.7178            -nan     0.1000    0.0142\n    20        0.5055            -nan     0.1000    0.0067\n    40        0.3404            -nan     0.1000    0.0015\n    60        0.2760            -nan     0.1000    0.0005\n    80        0.2444            -nan     0.1000   -0.0005\n   100        0.2194            -nan     0.1000   -0.0002\n   120        0.2005            -nan     0.1000   -0.0002\n   140        0.1843            -nan     0.1000   -0.0006\n   160        0.1722            -nan     0.1000   -0.0003\n   180        0.1592            -nan     0.1000   -0.0004\n   200        0.1477            -nan     0.1000   -0.0000\n   220        0.1377            -nan     0.1000   -0.0002\n   240        0.1297            -nan     0.1000   -0.0001\n   250        0.1265            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2285            -nan     0.1000    0.0570\n     2        1.1402            -nan     0.1000    0.0417\n     3        1.0619            -nan     0.1000    0.0390\n     4        0.9958            -nan     0.1000    0.0329\n     5        0.9347            -nan     0.1000    0.0293\n     6        0.8878            -nan     0.1000    0.0230\n     7        0.8489            -nan     0.1000    0.0192\n     8        0.8086            -nan     0.1000    0.0190\n     9        0.7718            -nan     0.1000    0.0171\n    10        0.7396            -nan     0.1000    0.0151\n    20        0.5292            -nan     0.1000    0.0090\n    40        0.3602            -nan     0.1000    0.0017\n    60        0.3024            -nan     0.1000    0.0003\n    80        0.2720            -nan     0.1000    0.0001\n   100        0.2531            -nan     0.1000   -0.0007\n   120        0.2375            -nan     0.1000   -0.0004\n   140        0.2229            -nan     0.1000    0.0004\n   160        0.2104            -nan     0.1000   -0.0002\n   180        0.1998            -nan     0.1000   -0.0001\n   200        0.1893            -nan     0.1000    0.0000\n   220        0.1819            -nan     0.1000   -0.0002\n   240        0.1742            -nan     0.1000   -0.0002\n   250        0.1693            -nan     0.1000   -0.0002\n\n\nSe entreno el modelo, con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods \nresults &lt;- resamples(models) \nsummary(results) \n\n\nCall:\nsummary.resamples(object = results)\n\nModels: rf, svmRadial, knn, gbm \nNumber of resamples: 5 \n\nAccuracy \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.9375000 0.9443691 0.9497965 0.9486701 0.9524457 0.9592391    0\nsvmRadial 0.9144022 0.9293478 0.9294437 0.9291133 0.9335142 0.9388587    0\nknn       0.8968792 0.8994565 0.9009498 0.9044028 0.9103261 0.9144022    0\ngbm       0.9402174 0.9442935 0.9511533 0.9489385 0.9538043 0.9552239    0\n\nKappa \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.8680057 0.8828194 0.8945004 0.8919135 0.9001163 0.9141257    0\nsvmRadial 0.8199916 0.8503670 0.8509704 0.8503147 0.8594480 0.8707966    0\nknn       0.7844764 0.7884335 0.7893354 0.7983825 0.8094581 0.8202094    0\ngbm       0.8734367 0.8827096 0.8975409 0.8926848 0.9032627 0.9064739    0\n\n\nResumen estadístico del Accuracy y Kappa de cada modelo.\n\ndotplot(results) \n\n\n\n\n\n\n\n\nComo se observa en la gráfica, tanto el rf como el gbm tienes excelentes resultados.\n\nbest_model &lt;- models[[\"rf\"]] \npredictions &lt;- predict(best_model, newdata = testTransformed) \ncm = confusionMatrix(predictions, testTransformed$type) \ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction nonspam spam\n   nonspam     541   29\n   spam         16  333\n                                         \n               Accuracy : 0.951          \n                 95% CI : (0.935, 0.9641)\n    No Information Rate : 0.6061         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8968         \n                                         \n Mcnemar's Test P-Value : 0.07364        \n                                         \n            Sensitivity : 0.9713         \n            Specificity : 0.9199         \n         Pos Pred Value : 0.9491         \n         Neg Pred Value : 0.9542         \n             Prevalence : 0.6061         \n         Detection Rate : 0.5887         \n   Detection Prevalence : 0.6202         \n      Balanced Accuracy : 0.9456         \n                                         \n       'Positive' Class : nonspam        \n                                         \n\n\nEn los datos de prueba, se obtuvo excelentes resultados, a la hora de clasificar entre spam y no spam. Con un 95% de Accuracy\n\nggplot( as.data.frame(cm$table), aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), vjust = 1.5, color = \"black\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"lightyellow\") +\n  labs(title = \"Matriz de Confusión en test\", x = \"Clase Real\", y = \"Clase Predicha\")\n\n\n\n\n\n\n\n\nGráfico de la matriz de confusión en el test y se observa que el modelo tiene buen rendimiento al clasificar el correo spam\n\nimportance &lt;- varImp(best_model, scale = TRUE) \nplot(importance, top = 5)  \n\n\n\n\n\n\n\n\nEl gráfico muestra el top 5 de las variables mas importantes dentro de las 57 que existen."
  },
  {
    "objectID": "r/r_iris.html",
    "href": "r/r_iris.html",
    "title": "Dataset Iris",
    "section": "",
    "text": "El dataset de iris (de Fisher o Anderson) proporciona las medidas en centímetros de la longitud y el ancho del sépalo y del pétalo, respectivamente, para 50 flores de cada una de las 3 especies de iris. Las especies son Iris setosa, versicolor y virginica.\n\n\n\nlibrary(caret) \nlibrary(GGally) \n\nCargar el dataset dentro de la variable datos\n\ndata(iris) \ndatos  = iris \n\nInformación del dataset\n\ndim(datos) \n\n[1] 150   5\n\n\nTiene 150 observaciones con 5 variables\n\nstr(datos) \n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nSe observa que 4 variables son numericas, excepto Species que es categoria, es la variable a analizar\n Resumen estadístico, la variable Species tiene 3 categorias distribuidas\n\nsummary(datos) \n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\nggplot(data=datos,aes(x=Sepal.Width, y=Sepal.Length,color=Species)) + \n  geom_point() + theme_minimal() + ggtitle(\"Gráfico entre Sepal Lenght y Sepal Width distribuido por Specie\") \n\n\n\n\n\n\n\n\n\nggplot(data=datos, aes(x=Sepal.Width)) +  \n  geom_histogram(binwidth=0.2, color=\"black\", aes(fill=Species)) +  \n  xlab(\"Sepal Width\") + ylab(\"Frequency\") + ggtitle(\"Histograma de Sepal Width\") \n\n\n\n\n\n\n\n\n\nggplot(datos) + \n  geom_density(aes(x = Petal.Width, fill = Species), alpha=0.25) +  \n  ggtitle(\"Densidad distribuida de Petal Width\") \n\n\n\n\n\n\n\n\n\nggplot(data=datos,aes(x=Species, y=Sepal.Length,color=Species)) + \n  geom_boxplot() +theme_minimal()+  \n  theme() + ggtitle(\"Boxplot de Sepal.Length por Specie\")\n\n\n\n\n\n\n\n\nCorrelación y distribucíon de las variables numericas\n\nggpairs(datos[,1:4]) \n\n\n\n\n\n\n\n\nExiste una correlación alta entre Petal Length y Petal Width\n\n\n\n\nset.seed(123) \ntrain &lt;- createDataPartition(datos$Species, p = 0.8, list = FALSE) \ntrainData &lt;- datos[train, ] \ntestData  &lt;- datos[-train, ] \n\nSe divide el dataset en 80% train y 20% test\n\npreProcValues &lt;- preProcess(trainData[,1:4], method = c(\"center\", \"scale\")) \ntrainTransformed &lt;- predict(preProcValues, trainData[, 1:4]) \ntestTransformed  &lt;- predict(preProcValues, testData[, 1:4]) \n\nSe realiza un escalado y centrado de variables y disminuir la varianza de las variables\n\ntrainTransformed$Species &lt;- trainData$Species \ntestTransformed$Species  &lt;- testData$Species \n\nReconstruimos los datasets con la variable objetivo\n\nset.seed(123) \nctrl &lt;- trainControl(method = \"cv\", number = 5) \n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)\nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\")\n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbour y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) { \n  train(Species ~ ., \n        data = trainTransformed, \n        method = m, \n        trControl = ctrl, \n        tuneLength = 3)}) \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2819\n     2        0.9045            -nan     0.1000    0.1944\n     3        0.7669            -nan     0.1000    0.1632\n     4        0.6454            -nan     0.1000    0.1294\n     5        0.5552            -nan     0.1000    0.1057\n     6        0.4845            -nan     0.1000    0.0901\n     7        0.4219            -nan     0.1000    0.0661\n     8        0.3726            -nan     0.1000    0.0632\n     9        0.3267            -nan     0.1000    0.0397\n    10        0.2921            -nan     0.1000    0.0385\n    20        0.1212            -nan     0.1000    0.0049\n    40        0.0558            -nan     0.1000   -0.0062\n    60        0.0373            -nan     0.1000   -0.0114\n    80        0.0266            -nan     0.1000   -0.0015\n   100        0.0186            -nan     0.1000   -0.0014\n   120        0.0160            -nan     0.1000   -0.0022\n   140        0.0085            -nan     0.1000    0.0005\n   150        0.0064            -nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3863\n     2        0.8479            -nan     0.1000    0.2362\n     3        0.6854            -nan     0.1000    0.1921\n     4        0.5556            -nan     0.1000    0.1408\n     5        0.4567            -nan     0.1000    0.1102\n     6        0.3867            -nan     0.1000    0.0913\n     7        0.3235            -nan     0.1000    0.0673\n     8        0.2771            -nan     0.1000    0.0543\n     9        0.2393            -nan     0.1000    0.0418\n    10        0.2094            -nan     0.1000    0.0311\n    20        0.0811            -nan     0.1000   -0.0043\n    40        0.0329            -nan     0.1000   -0.0074\n    60        0.0191            -nan     0.1000   -0.0014\n    80        0.0084            -nan     0.1000   -0.0016\n   100        0.0066            -nan     0.1000   -0.0005\n   120        0.0048            -nan     0.1000   -0.0007\n   140        0.0031            -nan     0.1000   -0.0008\n   150        0.0036            -nan     0.1000    0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3371\n     2        0.8577            -nan     0.1000    0.1736\n     3        0.7203            -nan     0.1000    0.2120\n     4        0.5793            -nan     0.1000    0.1490\n     5        0.4778            -nan     0.1000    0.1021\n     6        0.3977            -nan     0.1000    0.0621\n     7        0.3488            -nan     0.1000    0.0844\n     8        0.2931            -nan     0.1000    0.0555\n     9        0.2512            -nan     0.1000    0.0439\n    10        0.2225            -nan     0.1000    0.0396\n    20        0.0773            -nan     0.1000   -0.0027\n    40        0.0321            -nan     0.1000   -0.0053\n    60        0.0189            -nan     0.1000   -0.0056\n    80        0.0102            -nan     0.1000   -0.0010\n   100        0.0079            -nan     0.1000    0.0000\n   120        0.0043            -nan     0.1000   -0.0012\n   140        0.0035            -nan     0.1000   -0.0002\n   150        0.0025            -nan     0.1000   -0.0006\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2701\n     2        0.9007            -nan     0.1000    0.2149\n     3        0.7505            -nan     0.1000    0.1610\n     4        0.6336            -nan     0.1000    0.1372\n     5        0.5398            -nan     0.1000    0.1046\n     6        0.4675            -nan     0.1000    0.0900\n     7        0.4039            -nan     0.1000    0.0710\n     8        0.3524            -nan     0.1000    0.0595\n     9        0.3107            -nan     0.1000    0.0553\n    10        0.2742            -nan     0.1000    0.0337\n    20        0.0976            -nan     0.1000    0.0092\n    40        0.0328            -nan     0.1000   -0.0002\n    60        0.0180            -nan     0.1000   -0.0013\n    80        0.0153            -nan     0.1000    0.0031\n   100        0.0086            -nan     0.1000   -0.0000\n   120        0.0070            -nan     0.1000   -0.0010\n   140        0.0045            -nan     0.1000    0.0005\n   150        0.0046            -nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3049\n     2        0.8981            -nan     0.1000    0.2934\n     3        0.7084            -nan     0.1000    0.2216\n     4        0.5594            -nan     0.1000    0.1525\n     5        0.4532            -nan     0.1000    0.1278\n     6        0.3724            -nan     0.1000    0.0720\n     7        0.3218            -nan     0.1000    0.0679\n     8        0.2701            -nan     0.1000    0.0478\n     9        0.2289            -nan     0.1000    0.0530\n    10        0.1943            -nan     0.1000    0.0361\n    20        0.0550            -nan     0.1000    0.0024\n    40        0.0233            -nan     0.1000   -0.0075\n    60        0.0075            -nan     0.1000   -0.0014\n    80        0.0048            -nan     0.1000   -0.0020\n   100        0.0043            -nan     0.1000   -0.0009\n   120        0.0023            -nan     0.1000   -0.0005\n   140        0.0044            -nan     0.1000   -0.0024\n   150        0.0017            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.4043\n     2        0.8441            -nan     0.1000    0.2732\n     3        0.6682            -nan     0.1000    0.1758\n     4        0.5397            -nan     0.1000    0.1437\n     5        0.4396            -nan     0.1000    0.1234\n     6        0.3603            -nan     0.1000    0.0891\n     7        0.3000            -nan     0.1000    0.0704\n     8        0.2547            -nan     0.1000    0.0438\n     9        0.2206            -nan     0.1000    0.0451\n    10        0.1872            -nan     0.1000    0.0315\n    20        0.0557            -nan     0.1000   -0.0028\n    40        0.0196            -nan     0.1000   -0.0027\n    60        0.0102            -nan     0.1000    0.0015\n    80        0.0034            -nan     0.1000   -0.0008\n   100        0.0026            -nan     0.1000   -0.0012\n   120        0.0055            -nan     0.1000   -0.0002\n   140        0.0024            -nan     0.1000   -0.0010\n   150        0.0030            -nan     0.1000   -0.0008\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2917\n     2        0.9100            -nan     0.1000    0.1953\n     3        0.7653            -nan     0.1000    0.1660\n     4        0.6527            -nan     0.1000    0.1216\n     5        0.5571            -nan     0.1000    0.1084\n     6        0.4836            -nan     0.1000    0.0834\n     7        0.4234            -nan     0.1000    0.0769\n     8        0.3652            -nan     0.1000    0.0650\n     9        0.3202            -nan     0.1000    0.0535\n    10        0.2816            -nan     0.1000    0.0418\n    20        0.1045            -nan     0.1000    0.0056\n    40        0.0307            -nan     0.1000   -0.0009\n    60        0.0132            -nan     0.1000   -0.0018\n    80        0.0078            -nan     0.1000   -0.0016\n   100        0.0044            -nan     0.1000   -0.0009\n   120        0.0028            -nan     0.1000   -0.0009\n   140        0.0019            -nan     0.1000   -0.0004\n   150        0.0016            -nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3907\n     2        0.8429            -nan     0.1000    0.2709\n     3        0.6693            -nan     0.1000    0.1874\n     4        0.5460            -nan     0.1000    0.1490\n     5        0.4461            -nan     0.1000    0.1171\n     6        0.3694            -nan     0.1000    0.0817\n     7        0.3084            -nan     0.1000    0.0653\n     8        0.2621            -nan     0.1000    0.0368\n     9        0.2313            -nan     0.1000    0.0388\n    10        0.1973            -nan     0.1000    0.0255\n    20        0.0658            -nan     0.1000    0.0018\n    40        0.0211            -nan     0.1000   -0.0051\n    60        0.0100            -nan     0.1000   -0.0006\n    80        0.0054            -nan     0.1000   -0.0018\n   100        0.0036            -nan     0.1000   -0.0008\n   120        0.0018            -nan     0.1000   -0.0005\n   140        0.0009            -nan     0.1000   -0.0003\n   150        0.0010            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3792\n     2        0.8543            -nan     0.1000    0.2861\n     3        0.6755            -nan     0.1000    0.1860\n     4        0.5480            -nan     0.1000    0.1077\n     5        0.4683            -nan     0.1000    0.1293\n     6        0.3789            -nan     0.1000    0.0698\n     7        0.3247            -nan     0.1000    0.0659\n     8        0.2790            -nan     0.1000    0.0639\n     9        0.2383            -nan     0.1000    0.0533\n    10        0.2045            -nan     0.1000    0.0416\n    20        0.0669            -nan     0.1000   -0.0062\n    40        0.0216            -nan     0.1000   -0.0048\n    60        0.0086            -nan     0.1000   -0.0019\n    80        0.0038            -nan     0.1000   -0.0003\n   100        0.0017            -nan     0.1000   -0.0001\n   120        0.0009            -nan     0.1000   -0.0002\n   140        0.0005            -nan     0.1000   -0.0001\n   150        0.0004            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2919\n     2        0.8982            -nan     0.1000    0.2070\n     3        0.7486            -nan     0.1000    0.1778\n     4        0.6277            -nan     0.1000    0.1337\n     5        0.5346            -nan     0.1000    0.1108\n     6        0.4559            -nan     0.1000    0.0926\n     7        0.3932            -nan     0.1000    0.0744\n     8        0.3404            -nan     0.1000    0.0649\n     9        0.2955            -nan     0.1000    0.0502\n    10        0.2597            -nan     0.1000    0.0472\n    20        0.0808            -nan     0.1000    0.0123\n    40        0.0156            -nan     0.1000   -0.0007\n    60        0.0049            -nan     0.1000    0.0006\n    80        0.0024            -nan     0.1000   -0.0003\n   100        0.0011            -nan     0.1000    0.0001\n   120        0.0005            -nan     0.1000   -0.0000\n   140        0.0003            -nan     0.1000    0.0000\n   150        0.0002            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3109\n     2        0.8801            -nan     0.1000    0.2917\n     3        0.6836            -nan     0.1000    0.2120\n     4        0.5433            -nan     0.1000    0.1511\n     5        0.4410            -nan     0.1000    0.1141\n     6        0.3607            -nan     0.1000    0.0905\n     7        0.2962            -nan     0.1000    0.0702\n     8        0.2474            -nan     0.1000    0.0549\n     9        0.2052            -nan     0.1000    0.0470\n    10        0.1716            -nan     0.1000    0.0363\n    20        0.0415            -nan     0.1000    0.0017\n    40        0.0066            -nan     0.1000   -0.0009\n    60        0.0021            -nan     0.1000    0.0001\n    80        0.0007            -nan     0.1000   -0.0001\n   100        0.0003            -nan     0.1000   -0.0001\n   120        0.0001            -nan     0.1000   -0.0000\n   140        0.0001            -nan     0.1000   -0.0000\n   150        0.0000            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2641\n     2        0.8942            -nan     0.1000    0.3135\n     3        0.6976            -nan     0.1000    0.2142\n     4        0.5509            -nan     0.1000    0.1557\n     5        0.4433            -nan     0.1000    0.0825\n     6        0.3806            -nan     0.1000    0.0998\n     7        0.3146            -nan     0.1000    0.0726\n     8        0.2580            -nan     0.1000    0.0509\n     9        0.2166            -nan     0.1000    0.0535\n    10        0.1823            -nan     0.1000    0.0367\n    20        0.0489            -nan     0.1000    0.0098\n    40        0.0116            -nan     0.1000   -0.0042\n    60        0.0042            -nan     0.1000   -0.0010\n    80        0.0032            -nan     0.1000   -0.0008\n   100        0.0028            -nan     0.1000   -0.0008\n   120        0.0033            -nan     0.1000   -0.0004\n   140        0.0017            -nan     0.1000   -0.0003\n   150        0.0012            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2878\n     2        0.8979            -nan     0.1000    0.1986\n     3        0.7505            -nan     0.1000    0.1575\n     4        0.6372            -nan     0.1000    0.1219\n     5        0.5550            -nan     0.1000    0.1002\n     6        0.4768            -nan     0.1000    0.0664\n     7        0.4121            -nan     0.1000    0.0741\n     8        0.3580            -nan     0.1000    0.0597\n     9        0.3184            -nan     0.1000    0.0558\n    10        0.2828            -nan     0.1000    0.0432\n    20        0.1116            -nan     0.1000    0.0054\n    40        0.0461            -nan     0.1000    0.0008\n    60        0.0251            -nan     0.1000   -0.0032\n    80        0.0149            -nan     0.1000   -0.0024\n   100        0.0128            -nan     0.1000   -0.0026\n   120        0.0083            -nan     0.1000   -0.0013\n   140        0.0042            -nan     0.1000   -0.0008\n   150        0.0039            -nan     0.1000   -0.0010\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3846\n     2        0.8554            -nan     0.1000    0.1914\n     3        0.7160            -nan     0.1000    0.1477\n     4        0.6091            -nan     0.1000    0.1698\n     5        0.4930            -nan     0.1000    0.1181\n     6        0.4099            -nan     0.1000    0.0950\n     7        0.3465            -nan     0.1000    0.0677\n     8        0.2943            -nan     0.1000    0.0635\n     9        0.2529            -nan     0.1000    0.0274\n    10        0.2230            -nan     0.1000    0.0386\n    20        0.0896            -nan     0.1000    0.0082\n    40        0.0243            -nan     0.1000    0.0006\n    60        0.0136            -nan     0.1000   -0.0024\n    80        0.0087            -nan     0.1000   -0.0011\n   100        0.0026            -nan     0.1000   -0.0003\n   120        0.0010            -nan     0.1000   -0.0000\n   140        0.0006            -nan     0.1000   -0.0002\n   150        0.0004            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3655\n     2        0.8520            -nan     0.1000    0.2548\n     3        0.6852            -nan     0.1000    0.1707\n     4        0.5612            -nan     0.1000    0.1614\n     5        0.4562            -nan     0.1000    0.1134\n     6        0.3832            -nan     0.1000    0.0902\n     7        0.3180            -nan     0.1000    0.0710\n     8        0.2674            -nan     0.1000    0.0469\n     9        0.2270            -nan     0.1000    0.0413\n    10        0.1963            -nan     0.1000    0.0367\n    20        0.0756            -nan     0.1000   -0.0011\n    40        0.0280            -nan     0.1000   -0.0029\n    60        0.0095            -nan     0.1000   -0.0006\n    80        0.0054            -nan     0.1000   -0.0019\n   100        0.0031            -nan     0.1000    0.0000\n   120        0.0013            -nan     0.1000   -0.0001\n   140        0.0009            -nan     0.1000   -0.0003\n   150        0.0008            -nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2649\n     2        0.9022            -nan     0.1000    0.2098\n     3        0.7622            -nan     0.1000    0.1729\n     4        0.6382            -nan     0.1000    0.1301\n     5        0.5465            -nan     0.1000    0.1152\n     6        0.4721            -nan     0.1000    0.0764\n     7        0.4132            -nan     0.1000    0.0736\n     8        0.3606            -nan     0.1000    0.0652\n     9        0.3179            -nan     0.1000    0.0495\n    10        0.2815            -nan     0.1000    0.0376\n    20        0.1116            -nan     0.1000    0.0081\n    40        0.0484            -nan     0.1000   -0.0014\n    50        0.0319            -nan     0.1000   -0.0016\n\n\nSe entreno el modelo, con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods\nresults &lt;- resamples(models)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: rf, svmRadial, knn, gbm \nNumber of resamples: 5 \n\nAccuracy \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.9583333 0.9583333 0.9583333 0.9750000 1.0000000 1.0000000    0\nsvmRadial 0.9166667 0.9583333 0.9583333 0.9500000 0.9583333 0.9583333    0\nknn       0.8750000 0.9583333 0.9583333 0.9583333 1.0000000 1.0000000    0\ngbm       0.9166667 0.9583333 0.9583333 0.9583333 0.9583333 1.0000000    0\n\nKappa \n            Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\nrf        0.9375  0.9375 0.9375 0.9625  1.0000 1.0000    0\nsvmRadial 0.8750  0.9375 0.9375 0.9250  0.9375 0.9375    0\nknn       0.8125  0.9375 0.9375 0.9375  1.0000 1.0000    0\ngbm       0.8750  0.9375 0.9375 0.9375  0.9375 1.0000    0\n\n\nResumen estadístico del Accuracy y Kappa de cada modelo.\n\ndotplot(results)\n\n\n\n\n\n\n\n\nComo se observa en la gráfica, rf tiene mayor Accuracy y Kappa.\n\nbest_model &lt;- models[[\"rf\"]]\npredictions &lt;- predict(best_model, newdata = testTransformed)\nconfusionMatrix(predictions, testTransformed$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0         10         2\n  virginica       0          0         8\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8000\nSpecificity                 1.0000            0.9000           1.0000\nPos Pred Value              1.0000            0.8333           1.0000\nNeg Pred Value              1.0000            1.0000           0.9091\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.2667\nDetection Prevalence        0.3333            0.4000           0.2667\nBalanced Accuracy           1.0000            0.9500           0.9000\n\n\nEn los datos de prueba, se obtuvo excelentes resultados a la hora de clasificar las diferentes Species. Con un 93.3% de Accuracy y 90% de Kappa\n\nimportance &lt;- varImp(best_model, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\nLas variables mas importantes son Petal Width y Petal Length"
  },
  {
    "objectID": "r/r_iris.html#cargar-las-librerias-y-el-dataset",
    "href": "r/r_iris.html#cargar-las-librerias-y-el-dataset",
    "title": "Dataset Iris",
    "section": "",
    "text": "library(caret) \nlibrary(GGally) \n\nCargar el dataset dentro de la variable datos\n\ndata(iris) \ndatos  = iris \n\nInformación del dataset\n\ndim(datos) \n\n[1] 150   5\n\n\nTiene 150 observaciones con 5 variables\n\nstr(datos) \n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nSe observa que 4 variables son numericas, excepto Species que es categoria, es la variable a analizar\n Resumen estadístico, la variable Species tiene 3 categorias distribuidas\n\nsummary(datos) \n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\nggplot(data=datos,aes(x=Sepal.Width, y=Sepal.Length,color=Species)) + \n  geom_point() + theme_minimal() + ggtitle(\"Gráfico entre Sepal Lenght y Sepal Width distribuido por Specie\") \n\n\n\n\n\n\n\n\n\nggplot(data=datos, aes(x=Sepal.Width)) +  \n  geom_histogram(binwidth=0.2, color=\"black\", aes(fill=Species)) +  \n  xlab(\"Sepal Width\") + ylab(\"Frequency\") + ggtitle(\"Histograma de Sepal Width\") \n\n\n\n\n\n\n\n\n\nggplot(datos) + \n  geom_density(aes(x = Petal.Width, fill = Species), alpha=0.25) +  \n  ggtitle(\"Densidad distribuida de Petal Width\") \n\n\n\n\n\n\n\n\n\nggplot(data=datos,aes(x=Species, y=Sepal.Length,color=Species)) + \n  geom_boxplot() +theme_minimal()+  \n  theme() + ggtitle(\"Boxplot de Sepal.Length por Specie\")\n\n\n\n\n\n\n\n\nCorrelación y distribucíon de las variables numericas\n\nggpairs(datos[,1:4]) \n\n\n\n\n\n\n\n\nExiste una correlación alta entre Petal Length y Petal Width"
  },
  {
    "objectID": "r/r_iris.html#modelado",
    "href": "r/r_iris.html#modelado",
    "title": "Dataset Iris",
    "section": "",
    "text": "set.seed(123) \ntrain &lt;- createDataPartition(datos$Species, p = 0.8, list = FALSE) \ntrainData &lt;- datos[train, ] \ntestData  &lt;- datos[-train, ] \n\nSe divide el dataset en 80% train y 20% test\n\npreProcValues &lt;- preProcess(trainData[,1:4], method = c(\"center\", \"scale\")) \ntrainTransformed &lt;- predict(preProcValues, trainData[, 1:4]) \ntestTransformed  &lt;- predict(preProcValues, testData[, 1:4]) \n\nSe realiza un escalado y centrado de variables y disminuir la varianza de las variables\n\ntrainTransformed$Species &lt;- trainData$Species \ntestTransformed$Species  &lt;- testData$Species \n\nReconstruimos los datasets con la variable objetivo\n\nset.seed(123) \nctrl &lt;- trainControl(method = \"cv\", number = 5) \n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)\nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\")\n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbour y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) { \n  train(Species ~ ., \n        data = trainTransformed, \n        method = m, \n        trControl = ctrl, \n        tuneLength = 3)}) \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2819\n     2        0.9045            -nan     0.1000    0.1944\n     3        0.7669            -nan     0.1000    0.1632\n     4        0.6454            -nan     0.1000    0.1294\n     5        0.5552            -nan     0.1000    0.1057\n     6        0.4845            -nan     0.1000    0.0901\n     7        0.4219            -nan     0.1000    0.0661\n     8        0.3726            -nan     0.1000    0.0632\n     9        0.3267            -nan     0.1000    0.0397\n    10        0.2921            -nan     0.1000    0.0385\n    20        0.1212            -nan     0.1000    0.0049\n    40        0.0558            -nan     0.1000   -0.0062\n    60        0.0373            -nan     0.1000   -0.0114\n    80        0.0266            -nan     0.1000   -0.0015\n   100        0.0186            -nan     0.1000   -0.0014\n   120        0.0160            -nan     0.1000   -0.0022\n   140        0.0085            -nan     0.1000    0.0005\n   150        0.0064            -nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3863\n     2        0.8479            -nan     0.1000    0.2362\n     3        0.6854            -nan     0.1000    0.1921\n     4        0.5556            -nan     0.1000    0.1408\n     5        0.4567            -nan     0.1000    0.1102\n     6        0.3867            -nan     0.1000    0.0913\n     7        0.3235            -nan     0.1000    0.0673\n     8        0.2771            -nan     0.1000    0.0543\n     9        0.2393            -nan     0.1000    0.0418\n    10        0.2094            -nan     0.1000    0.0311\n    20        0.0811            -nan     0.1000   -0.0043\n    40        0.0329            -nan     0.1000   -0.0074\n    60        0.0191            -nan     0.1000   -0.0014\n    80        0.0084            -nan     0.1000   -0.0016\n   100        0.0066            -nan     0.1000   -0.0005\n   120        0.0048            -nan     0.1000   -0.0007\n   140        0.0031            -nan     0.1000   -0.0008\n   150        0.0036            -nan     0.1000    0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3371\n     2        0.8577            -nan     0.1000    0.1736\n     3        0.7203            -nan     0.1000    0.2120\n     4        0.5793            -nan     0.1000    0.1490\n     5        0.4778            -nan     0.1000    0.1021\n     6        0.3977            -nan     0.1000    0.0621\n     7        0.3488            -nan     0.1000    0.0844\n     8        0.2931            -nan     0.1000    0.0555\n     9        0.2512            -nan     0.1000    0.0439\n    10        0.2225            -nan     0.1000    0.0396\n    20        0.0773            -nan     0.1000   -0.0027\n    40        0.0321            -nan     0.1000   -0.0053\n    60        0.0189            -nan     0.1000   -0.0056\n    80        0.0102            -nan     0.1000   -0.0010\n   100        0.0079            -nan     0.1000    0.0000\n   120        0.0043            -nan     0.1000   -0.0012\n   140        0.0035            -nan     0.1000   -0.0002\n   150        0.0025            -nan     0.1000   -0.0006\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2701\n     2        0.9007            -nan     0.1000    0.2149\n     3        0.7505            -nan     0.1000    0.1610\n     4        0.6336            -nan     0.1000    0.1372\n     5        0.5398            -nan     0.1000    0.1046\n     6        0.4675            -nan     0.1000    0.0900\n     7        0.4039            -nan     0.1000    0.0710\n     8        0.3524            -nan     0.1000    0.0595\n     9        0.3107            -nan     0.1000    0.0553\n    10        0.2742            -nan     0.1000    0.0337\n    20        0.0976            -nan     0.1000    0.0092\n    40        0.0328            -nan     0.1000   -0.0002\n    60        0.0180            -nan     0.1000   -0.0013\n    80        0.0153            -nan     0.1000    0.0031\n   100        0.0086            -nan     0.1000   -0.0000\n   120        0.0070            -nan     0.1000   -0.0010\n   140        0.0045            -nan     0.1000    0.0005\n   150        0.0046            -nan     0.1000   -0.0020\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3049\n     2        0.8981            -nan     0.1000    0.2934\n     3        0.7084            -nan     0.1000    0.2216\n     4        0.5594            -nan     0.1000    0.1525\n     5        0.4532            -nan     0.1000    0.1278\n     6        0.3724            -nan     0.1000    0.0720\n     7        0.3218            -nan     0.1000    0.0679\n     8        0.2701            -nan     0.1000    0.0478\n     9        0.2289            -nan     0.1000    0.0530\n    10        0.1943            -nan     0.1000    0.0361\n    20        0.0550            -nan     0.1000    0.0024\n    40        0.0233            -nan     0.1000   -0.0075\n    60        0.0075            -nan     0.1000   -0.0014\n    80        0.0048            -nan     0.1000   -0.0020\n   100        0.0043            -nan     0.1000   -0.0009\n   120        0.0023            -nan     0.1000   -0.0005\n   140        0.0044            -nan     0.1000   -0.0024\n   150        0.0017            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.4043\n     2        0.8441            -nan     0.1000    0.2732\n     3        0.6682            -nan     0.1000    0.1758\n     4        0.5397            -nan     0.1000    0.1437\n     5        0.4396            -nan     0.1000    0.1234\n     6        0.3603            -nan     0.1000    0.0891\n     7        0.3000            -nan     0.1000    0.0704\n     8        0.2547            -nan     0.1000    0.0438\n     9        0.2206            -nan     0.1000    0.0451\n    10        0.1872            -nan     0.1000    0.0315\n    20        0.0557            -nan     0.1000   -0.0028\n    40        0.0196            -nan     0.1000   -0.0027\n    60        0.0102            -nan     0.1000    0.0015\n    80        0.0034            -nan     0.1000   -0.0008\n   100        0.0026            -nan     0.1000   -0.0012\n   120        0.0055            -nan     0.1000   -0.0002\n   140        0.0024            -nan     0.1000   -0.0010\n   150        0.0030            -nan     0.1000   -0.0008\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2917\n     2        0.9100            -nan     0.1000    0.1953\n     3        0.7653            -nan     0.1000    0.1660\n     4        0.6527            -nan     0.1000    0.1216\n     5        0.5571            -nan     0.1000    0.1084\n     6        0.4836            -nan     0.1000    0.0834\n     7        0.4234            -nan     0.1000    0.0769\n     8        0.3652            -nan     0.1000    0.0650\n     9        0.3202            -nan     0.1000    0.0535\n    10        0.2816            -nan     0.1000    0.0418\n    20        0.1045            -nan     0.1000    0.0056\n    40        0.0307            -nan     0.1000   -0.0009\n    60        0.0132            -nan     0.1000   -0.0018\n    80        0.0078            -nan     0.1000   -0.0016\n   100        0.0044            -nan     0.1000   -0.0009\n   120        0.0028            -nan     0.1000   -0.0009\n   140        0.0019            -nan     0.1000   -0.0004\n   150        0.0016            -nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3907\n     2        0.8429            -nan     0.1000    0.2709\n     3        0.6693            -nan     0.1000    0.1874\n     4        0.5460            -nan     0.1000    0.1490\n     5        0.4461            -nan     0.1000    0.1171\n     6        0.3694            -nan     0.1000    0.0817\n     7        0.3084            -nan     0.1000    0.0653\n     8        0.2621            -nan     0.1000    0.0368\n     9        0.2313            -nan     0.1000    0.0388\n    10        0.1973            -nan     0.1000    0.0255\n    20        0.0658            -nan     0.1000    0.0018\n    40        0.0211            -nan     0.1000   -0.0051\n    60        0.0100            -nan     0.1000   -0.0006\n    80        0.0054            -nan     0.1000   -0.0018\n   100        0.0036            -nan     0.1000   -0.0008\n   120        0.0018            -nan     0.1000   -0.0005\n   140        0.0009            -nan     0.1000   -0.0003\n   150        0.0010            -nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3792\n     2        0.8543            -nan     0.1000    0.2861\n     3        0.6755            -nan     0.1000    0.1860\n     4        0.5480            -nan     0.1000    0.1077\n     5        0.4683            -nan     0.1000    0.1293\n     6        0.3789            -nan     0.1000    0.0698\n     7        0.3247            -nan     0.1000    0.0659\n     8        0.2790            -nan     0.1000    0.0639\n     9        0.2383            -nan     0.1000    0.0533\n    10        0.2045            -nan     0.1000    0.0416\n    20        0.0669            -nan     0.1000   -0.0062\n    40        0.0216            -nan     0.1000   -0.0048\n    60        0.0086            -nan     0.1000   -0.0019\n    80        0.0038            -nan     0.1000   -0.0003\n   100        0.0017            -nan     0.1000   -0.0001\n   120        0.0009            -nan     0.1000   -0.0002\n   140        0.0005            -nan     0.1000   -0.0001\n   150        0.0004            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2919\n     2        0.8982            -nan     0.1000    0.2070\n     3        0.7486            -nan     0.1000    0.1778\n     4        0.6277            -nan     0.1000    0.1337\n     5        0.5346            -nan     0.1000    0.1108\n     6        0.4559            -nan     0.1000    0.0926\n     7        0.3932            -nan     0.1000    0.0744\n     8        0.3404            -nan     0.1000    0.0649\n     9        0.2955            -nan     0.1000    0.0502\n    10        0.2597            -nan     0.1000    0.0472\n    20        0.0808            -nan     0.1000    0.0123\n    40        0.0156            -nan     0.1000   -0.0007\n    60        0.0049            -nan     0.1000    0.0006\n    80        0.0024            -nan     0.1000   -0.0003\n   100        0.0011            -nan     0.1000    0.0001\n   120        0.0005            -nan     0.1000   -0.0000\n   140        0.0003            -nan     0.1000    0.0000\n   150        0.0002            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3109\n     2        0.8801            -nan     0.1000    0.2917\n     3        0.6836            -nan     0.1000    0.2120\n     4        0.5433            -nan     0.1000    0.1511\n     5        0.4410            -nan     0.1000    0.1141\n     6        0.3607            -nan     0.1000    0.0905\n     7        0.2962            -nan     0.1000    0.0702\n     8        0.2474            -nan     0.1000    0.0549\n     9        0.2052            -nan     0.1000    0.0470\n    10        0.1716            -nan     0.1000    0.0363\n    20        0.0415            -nan     0.1000    0.0017\n    40        0.0066            -nan     0.1000   -0.0009\n    60        0.0021            -nan     0.1000    0.0001\n    80        0.0007            -nan     0.1000   -0.0001\n   100        0.0003            -nan     0.1000   -0.0001\n   120        0.0001            -nan     0.1000   -0.0000\n   140        0.0001            -nan     0.1000   -0.0000\n   150        0.0000            -nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2641\n     2        0.8942            -nan     0.1000    0.3135\n     3        0.6976            -nan     0.1000    0.2142\n     4        0.5509            -nan     0.1000    0.1557\n     5        0.4433            -nan     0.1000    0.0825\n     6        0.3806            -nan     0.1000    0.0998\n     7        0.3146            -nan     0.1000    0.0726\n     8        0.2580            -nan     0.1000    0.0509\n     9        0.2166            -nan     0.1000    0.0535\n    10        0.1823            -nan     0.1000    0.0367\n    20        0.0489            -nan     0.1000    0.0098\n    40        0.0116            -nan     0.1000   -0.0042\n    60        0.0042            -nan     0.1000   -0.0010\n    80        0.0032            -nan     0.1000   -0.0008\n   100        0.0028            -nan     0.1000   -0.0008\n   120        0.0033            -nan     0.1000   -0.0004\n   140        0.0017            -nan     0.1000   -0.0003\n   150        0.0012            -nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2878\n     2        0.8979            -nan     0.1000    0.1986\n     3        0.7505            -nan     0.1000    0.1575\n     4        0.6372            -nan     0.1000    0.1219\n     5        0.5550            -nan     0.1000    0.1002\n     6        0.4768            -nan     0.1000    0.0664\n     7        0.4121            -nan     0.1000    0.0741\n     8        0.3580            -nan     0.1000    0.0597\n     9        0.3184            -nan     0.1000    0.0558\n    10        0.2828            -nan     0.1000    0.0432\n    20        0.1116            -nan     0.1000    0.0054\n    40        0.0461            -nan     0.1000    0.0008\n    60        0.0251            -nan     0.1000   -0.0032\n    80        0.0149            -nan     0.1000   -0.0024\n   100        0.0128            -nan     0.1000   -0.0026\n   120        0.0083            -nan     0.1000   -0.0013\n   140        0.0042            -nan     0.1000   -0.0008\n   150        0.0039            -nan     0.1000   -0.0010\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3846\n     2        0.8554            -nan     0.1000    0.1914\n     3        0.7160            -nan     0.1000    0.1477\n     4        0.6091            -nan     0.1000    0.1698\n     5        0.4930            -nan     0.1000    0.1181\n     6        0.4099            -nan     0.1000    0.0950\n     7        0.3465            -nan     0.1000    0.0677\n     8        0.2943            -nan     0.1000    0.0635\n     9        0.2529            -nan     0.1000    0.0274\n    10        0.2230            -nan     0.1000    0.0386\n    20        0.0896            -nan     0.1000    0.0082\n    40        0.0243            -nan     0.1000    0.0006\n    60        0.0136            -nan     0.1000   -0.0024\n    80        0.0087            -nan     0.1000   -0.0011\n   100        0.0026            -nan     0.1000   -0.0003\n   120        0.0010            -nan     0.1000   -0.0000\n   140        0.0006            -nan     0.1000   -0.0002\n   150        0.0004            -nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.3655\n     2        0.8520            -nan     0.1000    0.2548\n     3        0.6852            -nan     0.1000    0.1707\n     4        0.5612            -nan     0.1000    0.1614\n     5        0.4562            -nan     0.1000    0.1134\n     6        0.3832            -nan     0.1000    0.0902\n     7        0.3180            -nan     0.1000    0.0710\n     8        0.2674            -nan     0.1000    0.0469\n     9        0.2270            -nan     0.1000    0.0413\n    10        0.1963            -nan     0.1000    0.0367\n    20        0.0756            -nan     0.1000   -0.0011\n    40        0.0280            -nan     0.1000   -0.0029\n    60        0.0095            -nan     0.1000   -0.0006\n    80        0.0054            -nan     0.1000   -0.0019\n   100        0.0031            -nan     0.1000    0.0000\n   120        0.0013            -nan     0.1000   -0.0001\n   140        0.0009            -nan     0.1000   -0.0003\n   150        0.0008            -nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0986            -nan     0.1000    0.2649\n     2        0.9022            -nan     0.1000    0.2098\n     3        0.7622            -nan     0.1000    0.1729\n     4        0.6382            -nan     0.1000    0.1301\n     5        0.5465            -nan     0.1000    0.1152\n     6        0.4721            -nan     0.1000    0.0764\n     7        0.4132            -nan     0.1000    0.0736\n     8        0.3606            -nan     0.1000    0.0652\n     9        0.3179            -nan     0.1000    0.0495\n    10        0.2815            -nan     0.1000    0.0376\n    20        0.1116            -nan     0.1000    0.0081\n    40        0.0484            -nan     0.1000   -0.0014\n    50        0.0319            -nan     0.1000   -0.0016\n\n\nSe entreno el modelo, con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods\nresults &lt;- resamples(models)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: rf, svmRadial, knn, gbm \nNumber of resamples: 5 \n\nAccuracy \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.9583333 0.9583333 0.9583333 0.9750000 1.0000000 1.0000000    0\nsvmRadial 0.9166667 0.9583333 0.9583333 0.9500000 0.9583333 0.9583333    0\nknn       0.8750000 0.9583333 0.9583333 0.9583333 1.0000000 1.0000000    0\ngbm       0.9166667 0.9583333 0.9583333 0.9583333 0.9583333 1.0000000    0\n\nKappa \n            Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\nrf        0.9375  0.9375 0.9375 0.9625  1.0000 1.0000    0\nsvmRadial 0.8750  0.9375 0.9375 0.9250  0.9375 0.9375    0\nknn       0.8125  0.9375 0.9375 0.9375  1.0000 1.0000    0\ngbm       0.8750  0.9375 0.9375 0.9375  0.9375 1.0000    0\n\n\nResumen estadístico del Accuracy y Kappa de cada modelo.\n\ndotplot(results)\n\n\n\n\n\n\n\n\nComo se observa en la gráfica, rf tiene mayor Accuracy y Kappa.\n\nbest_model &lt;- models[[\"rf\"]]\npredictions &lt;- predict(best_model, newdata = testTransformed)\nconfusionMatrix(predictions, testTransformed$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0         10         2\n  virginica       0          0         8\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8000\nSpecificity                 1.0000            0.9000           1.0000\nPos Pred Value              1.0000            0.8333           1.0000\nNeg Pred Value              1.0000            1.0000           0.9091\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.2667\nDetection Prevalence        0.3333            0.4000           0.2667\nBalanced Accuracy           1.0000            0.9500           0.9000\n\n\nEn los datos de prueba, se obtuvo excelentes resultados a la hora de clasificar las diferentes Species. Con un 93.3% de Accuracy y 90% de Kappa\n\nimportance &lt;- varImp(best_model, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\nLas variables mas importantes son Petal Width y Petal Length"
  },
  {
    "objectID": "p/p_regre.html",
    "href": "p/p_regre.html",
    "title": "Analisis del dataset de insurance",
    "section": "",
    "text": "Analisis del dataset de insurance\nEl dataset de insurance es un conjunto de datos clásico utilizado para modelar costos médicos en función de características demográficas y de estilo de vida. Contiene información sobre asegurados y el monto de sus gastos médicos, lo que lo hace ideal para practicar modelos de regresión.\n\nCargar el dataset\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom pycaret.datasets import get_data\nfrom pycaret.regression import *\nimport os, contextlib\ndata = get_data('insurance')\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nchildren\n\n\nsmoker\n\n\nregion\n\n\ncharges\n\n\n\n\n\n\n0\n\n\n19\n\n\nfemale\n\n\n27.900\n\n\n0\n\n\nyes\n\n\nsouthwest\n\n\n16884.92400\n\n\n\n\n1\n\n\n18\n\n\nmale\n\n\n33.770\n\n\n1\n\n\nno\n\n\nsoutheast\n\n\n1725.55230\n\n\n\n\n2\n\n\n28\n\n\nmale\n\n\n33.000\n\n\n3\n\n\nno\n\n\nsoutheast\n\n\n4449.46200\n\n\n\n\n3\n\n\n33\n\n\nmale\n\n\n22.705\n\n\n0\n\n\nno\n\n\nnorthwest\n\n\n21984.47061\n\n\n\n\n4\n\n\n32\n\n\nmale\n\n\n28.880\n\n\n0\n\n\nno\n\n\nnorthwest\n\n\n3866.85520\n\n\n\n\n\nage: Edad del asegurado (numérica).\nsex: Género del asegurado (male, female).\nbmi: Índice de masa corporal (numérica).\nchildren: Número de hijos/dependientes cubiertos por el seguro.\nsmoker: Si el asegurado fuma (yes, no).\nregion: Región geográfica en EE. UU. (northeast, northwest, southeast, southwest).\ncharges: Costos médicos individuales facturados por el seguro (variable objetivo).\n\n\nAnálisis Exploratorio de Datos\ndata.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\nSon 1337 observaciones y 7 variables\ndata.isna().sum()\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\nNo hay valores Faltantes\nnumeric = ['age', 'bmi', 'children', 'charges']\n\nplt.figure(figsize=(5,4))\nfor i, col in enumerate(numeric, 1):\n    plt.subplot(2, 2, i)\n    sns.histplot(data[col], bins=20, kde=True, color='skyblue')\n    plt.title(f'Distribución de {col}')\nplt.tight_layout();\n\n\n\npng\n\n\nHistograma de las 4 variable numericas\ncategorical = ['smoker', 'sex', 'region']\n\nfig, axes = plt.subplots(1, 3, figsize=(12,4))\n\nfor ax, col in zip(axes, categorical):\n    sns.countplot(x=col, data=data, ax=ax, palette='Set2')\n    ax.set_title(f'Distribución de {col}')\n    ax.set_xlabel(col)\n    ax.set_ylabel('Frecuencia')\n\nplt.tight_layout();\n\n\n\npng\n\n\nDistribución de las 3 variables categoricas\nfig, axes = plt.subplots(1, 3, figsize=(16,4))\n\nfor ax, col in zip(axes, categorical):\n    sns.histplot(data=data, x='charges', hue=col, multiple='stack', ax=ax, bins=20)\n    ax.set_title(f'Distribución de charges por {col}')\n    ax.set_xlabel('charges')\n    ax.set_ylabel('Frecuencia')\n\nplt.tight_layout();\n\n\n\npng\n\n\nDistribución de charges para cada variable categorica\ncols = ['age', 'bmi', 'charges', 'smoker']\nsns.pairplot(data[cols], hue='smoker');\n\n\n\npng\n\n\n\n\nModelado\nSe usara el 80% del dataset para entrenar al modelo, tambien se normalizara y escalado de los datos.\nwith contextlib.redirect_stdout(open(os.devnull, 'w')):\n    reg = setup(\n        data=data, target='charges', train_size=0.8, session_id=7402,\n        numeric_features=numeric[:-1], categorical_features=categorical,\n        feature_selection=True, transformation=True, normalize=True, use_gpu=True )\n\n\n\n\n\n \n\n\nDescription\n\n\nValue\n\n\n\n\n\n\n0\n\n\nSession id\n\n\n7402\n\n\n\n\n1\n\n\nTarget\n\n\ncharges\n\n\n\n\n2\n\n\nTarget type\n\n\nRegression\n\n\n\n\n3\n\n\nOriginal data shape\n\n\n(1338, 7)\n\n\n\n\n4\n\n\nTransformed data shape\n\n\n(1338, 10)\n\n\n\n\n5\n\n\nTransformed train set shape\n\n\n(1070, 10)\n\n\n\n\n6\n\n\nTransformed test set shape\n\n\n(268, 10)\n\n\n\n\n7\n\n\nNumeric features\n\n\n3\n\n\n\n\n8\n\n\nCategorical features\n\n\n3\n\n\n\n\n9\n\n\nPreprocess\n\n\nTrue\n\n\n\n\n10\n\n\nImputation type\n\n\nsimple\n\n\n\n\n11\n\n\nNumeric imputation\n\n\nmean\n\n\n\n\n12\n\n\nCategorical imputation\n\n\nmode\n\n\n\n\n13\n\n\nMaximum one-hot encoding\n\n\n25\n\n\n\n\n14\n\n\nEncoding method\n\n\nNone\n\n\n\n\n15\n\n\nTransformation\n\n\nTrue\n\n\n\n\n16\n\n\nTransformation method\n\n\nyeo-johnson\n\n\n\n\n17\n\n\nNormalize\n\n\nTrue\n\n\n\n\n18\n\n\nNormalize method\n\n\nzscore\n\n\n\n\n19\n\n\nFold Generator\n\n\nKFold\n\n\n\n\n20\n\n\nFold Number\n\n\n10\n\n\n\n\n21\n\n\nCPU Jobs\n\n\n-1\n\n\n\n\n22\n\n\nUse GPU\n\n\nTrue\n\n\n\n\n23\n\n\nLog Experiment\n\n\nFalse\n\n\n\n\n24\n\n\nExperiment Name\n\n\nreg-default-name\n\n\n\n\n25\n\n\nUSI\n\n\nc41a\n\n\n\n\nget_config('X_transformed').head()\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nchildren\n\n\nsmoker\n\n\nregion_northeast\n\n\nregion_southwest\n\n\nregion_southeast\n\n\nregion_northwest\n\n\n\n\n\n\n989\n\n\n-1.101651\n\n\n-1.032297\n\n\n-1.773173\n\n\n-1.080649\n\n\n2.017694\n\n\n1.747292\n\n\n-0.553599\n\n\n-0.616964\n\n\n-0.566558\n\n\n\n\n734\n\n\n1.359932\n\n\n-1.032297\n\n\n0.282258\n\n\n1.358853\n\n\n-0.495615\n\n\n-0.572314\n\n\n1.806363\n\n\n-0.616964\n\n\n-0.566558\n\n\n\n\n790\n\n\n0.038240\n\n\n-1.032297\n\n\n1.692765\n\n\n-1.080649\n\n\n-0.495615\n\n\n-0.572314\n\n\n-0.553599\n\n\n1.620839\n\n\n-0.566558\n\n\n\n\n1025\n\n\n-1.353821\n\n\n-1.032297\n\n\n0.666971\n\n\n-1.080649\n\n\n-0.495615\n\n\n-0.572314\n\n\n1.806363\n\n\n-0.616964\n\n\n-0.566558\n\n\n\n\n1209\n\n\n1.359932\n\n\n0.968713\n\n\n1.036003\n\n\n0.290510\n\n\n-0.495615\n\n\n-0.572314\n\n\n1.806363\n\n\n-0.616964\n\n\n-0.566558\n\n\n\n\n\nEs la salida de lso datos transformados, escalarlo y centrado\nComparación de los Modelos, de forma descendente para elegir el RMSE mas inferior\nbest = compare_models(sort='RMSE')\n\n\n\n\n\n \n\n\nModel\n\n\nMAE\n\n\nMSE\n\n\nRMSE\n\n\nR2\n\n\nRMSLE\n\n\nMAPE\n\n\nTT (Sec)\n\n\n\n\n\n\ncatboost\n\n\nCatBoost Regressor\n\n\n2544.1294\n\n\n21513021.5018\n\n\n4615.4353\n\n\n0.8447\n\n\n0.4245\n\n\n0.2967\n\n\n2.7520\n\n\n\n\ngbr\n\n\nGradient Boosting Regressor\n\n\n2653.5705\n\n\n22788369.3808\n\n\n4750.3269\n\n\n0.8355\n\n\n0.4412\n\n\n0.3166\n\n\n0.1200\n\n\n\n\nrf\n\n\nRandom Forest Regressor\n\n\n2843.6511\n\n\n25287914.2960\n\n\n5004.8833\n\n\n0.8187\n\n\n0.4733\n\n\n0.3391\n\n\n0.1480\n\n\n\n\nlightgbm\n\n\nLight Gradient Boosting Machine\n\n\n2973.0959\n\n\n25561696.4936\n\n\n5026.2688\n\n\n0.8162\n\n\n0.5445\n\n\n0.3709\n\n\n0.6040\n\n\n\n\net\n\n\nExtra Trees Regressor\n\n\n2756.0313\n\n\n26963644.8414\n\n\n5167.8475\n\n\n0.8046\n\n\n0.4776\n\n\n0.3152\n\n\n0.1150\n\n\n\n\nada\n\n\nAdaBoost Regressor\n\n\n4171.4930\n\n\n27224245.1707\n\n\n5196.6607\n\n\n0.8045\n\n\n0.6046\n\n\n0.6883\n\n\n0.0610\n\n\n\n\nxgboost\n\n\nExtreme Gradient Boosting\n\n\n3223.8355\n\n\n30431896.0000\n\n\n5494.1843\n\n\n0.7774\n\n\n0.5689\n\n\n0.4211\n\n\n0.1910\n\n\n\n\nknn\n\n\nK Neighbors Regressor\n\n\n3533.1890\n\n\n31952929.8000\n\n\n5631.9931\n\n\n0.7702\n\n\n0.4994\n\n\n0.3839\n\n\n0.0650\n\n\n\n\nllar\n\n\nLasso Least Angle Regression\n\n\n4264.9301\n\n\n37679182.0798\n\n\n6127.1917\n\n\n0.7279\n\n\n0.6161\n\n\n0.4359\n\n\n0.0530\n\n\n\n\nlasso\n\n\nLasso Regression\n\n\n4264.9304\n\n\n37679195.3633\n\n\n6127.1927\n\n\n0.7279\n\n\n0.6161\n\n\n0.4359\n\n\n0.0520\n\n\n\n\nridge\n\n\nRidge Regression\n\n\n4266.4851\n\n\n37679214.7628\n\n\n6127.2099\n\n\n0.7279\n\n\n0.6258\n\n\n0.4362\n\n\n0.0510\n\n\n\n\nbr\n\n\nBayesian Ridge\n\n\n4268.8960\n\n\n37679375.4023\n\n\n6127.2543\n\n\n0.7279\n\n\n0.6183\n\n\n0.4367\n\n\n0.0510\n\n\n\n\nlar\n\n\nLeast Angle Regression\n\n\n4266.5641\n\n\n37701506.2132\n\n\n6129.1207\n\n\n0.7277\n\n\n0.6162\n\n\n0.4358\n\n\n0.0520\n\n\n\n\nlr\n\n\nLinear Regression\n\n\n4290.9364\n\n\n37885922.5276\n\n\n6144.3979\n\n\n0.7266\n\n\n0.5960\n\n\n0.4394\n\n\n0.0520\n\n\n\n\ndt\n\n\nDecision Tree Regressor\n\n\n3086.5743\n\n\n43127202.5907\n\n\n6538.9012\n\n\n0.6896\n\n\n0.5244\n\n\n0.3362\n\n\n0.0530\n\n\n\n\nhuber\n\n\nHuber Regressor\n\n\n3513.6603\n\n\n47230838.3332\n\n\n6830.9834\n\n\n0.6504\n\n\n0.5446\n\n\n0.2449\n\n\n0.0540\n\n\n\n\npar\n\n\nPassive Aggressive Regressor\n\n\n3510.5454\n\n\n47708732.9582\n\n\n6866.4346\n\n\n0.6471\n\n\n0.5371\n\n\n0.2431\n\n\n0.0570\n\n\n\n\nen\n\n\nElastic Net\n\n\n5159.0111\n\n\n49494358.5064\n\n\n7009.0181\n\n\n0.6511\n\n\n0.6176\n\n\n0.6926\n\n\n0.0530\n\n\n\n\nomp\n\n\nOrthogonal Matching Pursuit\n\n\n5622.2865\n\n\n55666429.7377\n\n\n7453.8248\n\n\n0.5968\n\n\n0.7183\n\n\n0.8585\n\n\n0.0520\n\n\n\n\ndummy\n\n\nDummy Regressor\n\n\n9010.9561\n\n\n144536378.4000\n\n\n11975.2231\n\n\n-0.0147\n\n\n0.9891\n\n\n1.4901\n\n\n0.0510\n\n\n\n\nEl modelo con RMSE mas bajo es el CatBoost Regressor\nSelección y creación del mejor model\nmodel = create_model('catboost', fold = 10)\n\n\n\n\n\n \n\n\nMAE\n\n\nMSE\n\n\nRMSE\n\n\nR2\n\n\nRMSLE\n\n\nMAPE\n\n\n\n\nFold\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n0\n\n\n2569.1670\n\n\n21829690.2076\n\n\n4672.2254\n\n\n0.8575\n\n\n0.3876\n\n\n0.2685\n\n\n\n\n1\n\n\n2799.2041\n\n\n27564596.0582\n\n\n5250.1996\n\n\n0.7551\n\n\n0.5254\n\n\n0.4283\n\n\n\n\n2\n\n\n2650.2712\n\n\n19709606.2226\n\n\n4439.5502\n\n\n0.8703\n\n\n0.4503\n\n\n0.3044\n\n\n\n\n3\n\n\n2591.6070\n\n\n23262442.9423\n\n\n4823.1155\n\n\n0.8319\n\n\n0.4345\n\n\n0.2703\n\n\n\n\n4\n\n\n2417.2660\n\n\n17408995.1888\n\n\n4172.4088\n\n\n0.8462\n\n\n0.4306\n\n\n0.2953\n\n\n\n\n5\n\n\n2265.6284\n\n\n16846224.1121\n\n\n4104.4152\n\n\n0.9088\n\n\n0.3353\n\n\n0.2464\n\n\n\n\n6\n\n\n2761.8084\n\n\n28961778.5441\n\n\n5381.6149\n\n\n0.8382\n\n\n0.4450\n\n\n0.2782\n\n\n\n\n7\n\n\n2491.7776\n\n\n17703325.3222\n\n\n4207.5320\n\n\n0.8846\n\n\n0.3861\n\n\n0.3132\n\n\n\n\n8\n\n\n2219.1897\n\n\n16807135.0494\n\n\n4099.6506\n\n\n0.8333\n\n\n0.3766\n\n\n0.2846\n\n\n\n\n9\n\n\n2675.3743\n\n\n25036421.3707\n\n\n5003.6408\n\n\n0.8213\n\n\n0.4740\n\n\n0.2781\n\n\n\n\nMean\n\n\n2544.1294\n\n\n21513021.5018\n\n\n4615.4353\n\n\n0.8447\n\n\n0.4245\n\n\n0.2967\n\n\n\n\nStd\n\n\n185.9578\n\n\n4315253.4801\n\n\n459.1062\n\n\n0.0393\n\n\n0.0519\n\n\n0.0474\n\n\n\n\nSe ajusta los hiperparametros del modelo para obtener un RMSE mas bajo\nparams = {\n    'learning_rate': [0.05, 0.08, 0.1],\n    'depth': [3, 4, 5, 6, 7], \n    'n_estimators': [100, 200, 300, 400]\n    }\n\ntuned_model = tune_model(model, optimize = 'RMSE', fold = 5,\n                       custom_grid = params, n_iter = 30)\n\n\n\n\n\n \n\n\nMAE\n\n\nMSE\n\n\nRMSE\n\n\nR2\n\n\nRMSLE\n\n\nMAPE\n\n\n\n\nFold\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n0\n\n\n2591.4650\n\n\n21418270.6392\n\n\n4627.9878\n\n\n0.8601\n\n\n0.3847\n\n\n0.2770\n\n\n\n\n1\n\n\n2756.2860\n\n\n27397616.2146\n\n\n5234.2732\n\n\n0.7566\n\n\n0.5147\n\n\n0.4086\n\n\n\n\n2\n\n\n2562.3756\n\n\n18580836.8445\n\n\n4310.5495\n\n\n0.8777\n\n\n0.4274\n\n\n0.2874\n\n\n\n\n3\n\n\n2538.7444\n\n\n22371086.3670\n\n\n4729.8083\n\n\n0.8383\n\n\n0.4215\n\n\n0.2641\n\n\n\n\n4\n\n\n2395.5188\n\n\n17056626.4051\n\n\n4129.9669\n\n\n0.8493\n\n\n0.4251\n\n\n0.3042\n\n\n\n\n5\n\n\n2233.5858\n\n\n16654619.6520\n\n\n4081.0072\n\n\n0.9098\n\n\n0.3386\n\n\n0.2565\n\n\n\n\n6\n\n\n2690.8020\n\n\n27596123.7208\n\n\n5253.2013\n\n\n0.8458\n\n\n0.4358\n\n\n0.2835\n\n\n\n\n7\n\n\n2438.6125\n\n\n17392787.1402\n\n\n4170.4661\n\n\n0.8866\n\n\n0.3875\n\n\n0.3151\n\n\n\n\n8\n\n\n2221.5711\n\n\n16475135.2377\n\n\n4058.9574\n\n\n0.8366\n\n\n0.3904\n\n\n0.2986\n\n\n\n\n9\n\n\n2724.6473\n\n\n25059814.6331\n\n\n5005.9779\n\n\n0.8212\n\n\n0.4701\n\n\n0.2879\n\n\n\n\nMean\n\n\n2515.3608\n\n\n21000291.6854\n\n\n4560.2195\n\n\n0.8482\n\n\n0.4196\n\n\n0.2983\n\n\n\n\nStd\n\n\n180.8691\n\n\n4204123.2773\n\n\n452.4261\n\n\n0.0396\n\n\n0.0465\n\n\n0.0404\n\n\n\n\nFitting 10 folds for each of 90 candidates, totalling 900 fits\npredictions = predict_model(model)\npredictions.head()\n\n\n\n\n\n \n\n\nModel\n\n\nMAE\n\n\nMSE\n\n\nRMSE\n\n\nR2\n\n\nRMSLE\n\n\nMAPE\n\n\n\n\n\n\n0\n\n\nCatBoost Regressor\n\n\n2399.9307\n\n\n17707915.1682\n\n\n4208.0774\n\n\n0.8864\n\n\n0.4312\n\n\n0.3332\n\n\n\n\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nchildren\n\n\nsmoker\n\n\nregion\n\n\ncharges\n\n\nprediction_label\n\n\n\n\n\n\n1004\n\n\n47\n\n\nmale\n\n\n19.190001\n\n\n1\n\n\nno\n\n\nnortheast\n\n\n8627.541016\n\n\n8749.020960\n\n\n\n\n763\n\n\n27\n\n\nmale\n\n\n26.030001\n\n\n0\n\n\nno\n\n\nnortheast\n\n\n3070.808594\n\n\n4355.091508\n\n\n\n\n544\n\n\n54\n\n\nmale\n\n\n30.209999\n\n\n0\n\n\nno\n\n\nnorthwest\n\n\n10231.500000\n\n\n12464.764204\n\n\n\n\n256\n\n\n56\n\n\nmale\n\n\n33.630001\n\n\n0\n\n\nyes\n\n\nnorthwest\n\n\n43921.183594\n\n\n44215.140564\n\n\n\n\n1171\n\n\n43\n\n\nfemale\n\n\n26.700001\n\n\n2\n\n\nyes\n\n\nsouthwest\n\n\n22478.599609\n\n\n24826.324915\n\n\n\n\n\nSe realizo utilizo el modelo entrenado en los datos de test, que fue el 20% restante y se obtuvo en RMSE = 4208\nplot_model(tuned_model, 'feature')\n\n\n\npng\n\n\nEl gráfico nos muestra las variables más influyentes en promedio, el fumar sobresale del resto\ninterpret_model(tuned_model)\n\n\n\npng\n\n\nEl gráfico muestra la relación de todas las variables con charges\nRojo a la derecha: Ser fumador aumenta mucho el costo.\nAzul a la izquierda: No fumar reduce el costo.\nplot_model(model, 'error',scale=0.3)\n\n\n\npng\n\n\nEl gráfico muestra que el modelo explica un 88.6%."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heredia T. Cristian M.",
    "section": "",
    "text": "Hola, soy especialista en la extracción, tranformación, modelado y visualización de datos. Entusiasta por las nuevas tecnologías emergentes de Machine Learning, Deep Learning y el Cloud Computing.\nApasionado por el entorno R, Python y Power BI.\n\n\n\n\nContacto\n\n\n\n\n\n\n\n\n Experiencia en proyectos de análisis de datos utilizando R, Python y Power BI, cubriendo todo el ciclo: obtención, limpieza, modelado y visualización. \n He diseñado KPI’s y dashboards interactivos como tambien modelos de machine learning y deep learning para apoyar la toma de decisiones estratégicas. Combino habilidades técnicas con capacidad de comunicar hallazgos de forma clara y visual. \n\n\n\n\n\n\n PORTAFOLIO \n\n\nR\n\n\n\nClasificación con el dataset iris\n Análisis de clasificación en R utilizando el conjunto de datos Iris, donde se entrenaron distintos modelos de aprendizaje automático y se eligió el más preciso para reconocer las especies de flores. \n\n\nClasificación del dataset spam\nAplicación de modelos de clasificación en R sobre el conjunto de datos Spam, evaluando diversos algoritmos de aprendizaje automático y seleccionando el más eficiente para distinguir correos electrónicos legítimos de aquellos considerados no deseados.\n\n\nPredicción de precios en viviendas de Sacramento\nAplicación de técnicas de machine learning en R con el dataset de viviendas de Sacramento, comparando varios algoritmos y seleccionando el más eficaz para predecir los precios de las propiedades.\n\n\n\n\n\nPython\n\n\n\nPredicción de gastos sanitarios\nAplicación de modelos predictivos de gastos sanitarios en Python sobre el dataset Insurance, evaluando diversos algoritmos con PyCaret y eligiendo el más eficiente para predecir gastos sanitarios.\n\n\nClasificación de depósito al banco\nAplicación de técnicas de machine learning en Python con la libreria de scikit-learn en el dataset Bank Marketing, comparando varios algoritmos y seleccionando el más eficaz para clasificar la suscripción de depósitos.\n\n\nAnálisis temporal del CO₂ de 1958-2021\nAnálisis de series temporales sobre el dataset de concentraciones de CO₂ desde 1958 a 2021, utilizando Python para modelar y pronosticar tendencias futuras.\n\n\n\n\n\nPower BI\n\n\n\nDashboard del COVID-19\nDashboard interactivo en Power BI para el análisis de datos de COVID-19, mostrando el total de casos confirmados, fallecidos y recuperados, así como el incremento en las últimas 24 horas. El panel incluye funcionalidades de filtrado por país, permitiendo una visualización dinámica y comparativa de la evolución de la pandemia a nivel global y regional.\n\n\nDashboard de proyección de financiera\nDashboard financiero en Power BI para el análisis diario de proyecciones económicas, comparando valores reales frente a proyecciones estimadas. El panel incluye intervalos de tiempo configurables y muestra la variación entre ambos escenarios, permitiendo identificar desviaciones y tendencias.\n\n\nDashboard de fitness\nDashboard financiero en Power BI para el análisis diario de proyecciones económicas, comparando valores reales frente a proyecciones estimadas. El panel incluye intervalos de tiempo configurables y muestra la variación entre ambos escenarios, permitiendo identificar desviaciones y tendencias.\n\n\nDashboard de Recursos Humanos\nDashboard de Recursos Humanos en Power BI para la evaluación de desempeño y análisis de sueldos. El reporte integra información clave sobre la fuerza laboral, incluyendo el total de empleados por departamento y métricas comparativas de rendimiento.\n\n\n\n\nPower BI – SAP BO\nConexión de Power BI y SAP Business One: Dashboard de un Database de SAP BO, utilizando Power BI para generar visualizaciones interactivas.\n\n\n\n\n\n\n\n\n\nAplicaciones Web\n\n\n\nTranscripción de Videos/audios\nAplicación Web desarrollado con Pytorch/Whisper, que transcribe videos/audios en inglés.\nNota: Al ingresar te saldra una página de advertencia, debes hacer clíck en el boton “Visite site”.\n\n\nAplicación web sobre la tabla de crecimiento infantil\nAplicación Web dinámica desarrollada con R(shiny) para determinar el crecimiento infantil en menores de 5 años de la OMS.\n\n\nAplicación web sobre regresión lineal\nAplicación Web dinámica desarrollada con R(shiny) para modelar el dataset de Swiss, sobre la fertilidad y indicadores socioeconomicos.\n\n\nAplicación web de Texto predictivo\nAplicación Web dinámica desarrollada con R(shiny) que predice la palabra que vendra después, basándose en el contexto de tu frase. En inglés.\n\n\n\n\n\nAplicaciones Móviles\n\n\n\nAplicación móvil para la evaluación nutricional\nAplicación móvil desarrollada en Flutter para evaluar el estado nutricional de diferentes poblaciones y su crecimiento en menores de 18 años. Se Tomo de referencias la información de la OMS.\n\n\nAplicación móvil predicción de precios\nAplicación móvil creado con Flutter para predecir precios de los inmuebles, se utilizo modelos de machine learning.\n\n\n\n\n\nTableau\n\n\n\nDashboard de aerolíneas\nDashboard diseñado con tableau, el objetivo es analizar el comportamiento de los retrasos en vuelos comerciales.\n\n\n\n\n\n\n\n\nContacto\nCel: (+591)-79391176\nCorreo: cristianvoice@gmail.com"
  },
  {
    "objectID": "p/p_clasi.html",
    "href": "p/p_clasi.html",
    "title": "Analisis del dataset de bank",
    "section": "",
    "text": "Analisis del dataset de bank\nEl dataset bank proviene de una campaña de marketing de un banco portugués. Contiene información sobre clientes y llamadas telefónicas realizadas para promocionar depósitos a plazo fijo. El objetivo es predecir si un cliente aceptará abrir un depósito\n\nCargar las librerias y el dataset\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom pycaret.datasets import get_data\nfrom pycaret.classification import *\nimport os, contextlib\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ndata = get_data(\"bank\")\n\n\n\n\n\n\n\n\nage\n\n\njob\n\n\nmarital\n\n\neducation\n\n\ndefault\n\n\nbalance\n\n\nhousing\n\n\nloan\n\n\ncontact\n\n\nday\n\n\nmonth\n\n\nduration\n\n\ncampaign\n\n\npdays\n\n\nprevious\n\n\npoutcome\n\n\ndeposit\n\n\n\n\n\n\n0\n\n\n58\n\n\nmanagement\n\n\nmarried\n\n\ntertiary\n\n\nno\n\n\n2143\n\n\nyes\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n261\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\n\n\n1\n\n\n44\n\n\ntechnician\n\n\nsingle\n\n\nsecondary\n\n\nno\n\n\n29\n\n\nyes\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n151\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\n\n\n2\n\n\n33\n\n\nentrepreneur\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n2\n\n\nyes\n\n\nyes\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n76\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\n\n\n3\n\n\n47\n\n\nblue-collar\n\n\nmarried\n\n\nunknown\n\n\nno\n\n\n1506\n\n\nyes\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n92\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\n\n\n4\n\n\n33\n\n\nunknown\n\n\nsingle\n\n\nunknown\n\n\nno\n\n\n1\n\n\nno\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n198\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\n\n\n\nage: edad.\njob: tipo de trabajo (admin, blue-collar, technician, etc.).\nmarital: estado civil (married, single, divorced).\neducation: nivel educativo.\ndefault: si tiene créditos en default.\nhousing: si tiene préstamo hipotecario.\nloan: si tiene préstamo personal.\nmonth: mes de la última llamada.\nday_of_week: día de la semana de la última llamada.\nduration: duración de la llamada (segundos).\ncampaign: número de contactos realizados durante la campaña.\npdays: días desde el último contacto en una campaña anterior.\nprevious: número de contactos anteriores.\npoutcome: resultado de la campaña anterior.\n\n\nAnálisis Exploratorio de Datos\ndata.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 45211 entries, 0 to 45210\nData columns (total 17 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   age        45211 non-null  int64 \n 1   job        45211 non-null  object\n 2   marital    45211 non-null  object\n 3   education  45211 non-null  object\n 4   default    45211 non-null  object\n 5   balance    45211 non-null  int64 \n 6   housing    45211 non-null  object\n 7   loan       45211 non-null  object\n 8   contact    45211 non-null  object\n 9   day        45211 non-null  int64 \n 10  month      45211 non-null  object\n 11  duration   45211 non-null  int64 \n 12  campaign   45211 non-null  int64 \n 13  pdays      45211 non-null  int64 \n 14  previous   45211 non-null  int64 \n 15  poutcome   45211 non-null  object\n 16  deposit    45211 non-null  object\ndtypes: int64(7), object(10)\nmemory usage: 5.9+ MB\nHay 10 variables categoricas, incluyendo a la variable objetivo, y 7 variables numerica\ndata.isna().sum()\nage          0\njob          0\nmarital      0\neducation    0\ndefault      0\nbalance      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\ndeposit      0\ndtype: int64\nNo existen valores faltantes\ndata['deposit'].value_counts()\ndeposit\nno     39922\nyes     5289\nName: count, dtype: int64\nExiste un desbalance en la variable objetivo\ncategoricas = data.select_dtypes(include=[\"object\"])\ncategoricas.nunique()\njob          12\nmarital       3\neducation     4\ndefault       2\nhousing       2\nloan          2\ncontact       3\nmonth        12\npoutcome      4\ndeposit       2\ndtype: int64\nSe observa que job tiene 12 distintos de trabajo en el dataset\nnumeric = data.select_dtypes(include=[\"number\"])\nnumeric.describe().round(2)\n\n\n\n\n\n\n\n\nage\n\n\nbalance\n\n\nday\n\n\nduration\n\n\ncampaign\n\n\npdays\n\n\nprevious\n\n\n\n\n\n\ncount\n\n\n45211.00\n\n\n45211.00\n\n\n45211.00\n\n\n45211.00\n\n\n45211.00\n\n\n45211.00\n\n\n45211.00\n\n\n\n\nmean\n\n\n40.94\n\n\n1362.27\n\n\n15.81\n\n\n258.16\n\n\n2.76\n\n\n40.20\n\n\n0.58\n\n\n\n\nstd\n\n\n10.62\n\n\n3044.77\n\n\n8.32\n\n\n257.53\n\n\n3.10\n\n\n100.13\n\n\n2.30\n\n\n\n\nmin\n\n\n18.00\n\n\n-8019.00\n\n\n1.00\n\n\n0.00\n\n\n1.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n25%\n\n\n33.00\n\n\n72.00\n\n\n8.00\n\n\n103.00\n\n\n1.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n50%\n\n\n39.00\n\n\n448.00\n\n\n16.00\n\n\n180.00\n\n\n2.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n75%\n\n\n48.00\n\n\n1428.00\n\n\n21.00\n\n\n319.00\n\n\n3.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\nmax\n\n\n95.00\n\n\n102127.00\n\n\n31.00\n\n\n4918.00\n\n\n63.00\n\n\n871.00\n\n\n275.00\n\n\n\n\n\nResumen estadístico de las variables numericas, la variable balance, tiene mayor dispersión de los datos con respecto a su media\nsns.countplot(x='deposit', data=data)\nplt.title('Distribución de la variable objetivo (deposit)');\n\n\n\npng\n\n\nLa proporción de la variable deposit no es similar\nsns.histplot(data['age'], bins=30, kde=True)\nplt.title('Distribución de la edad');\n\n\n\npng\n\n\nLa mayoria de individuos tiene entre 30 a 40 años\nsns.boxplot(x='deposit', y='age', data=data)\nplt.title('Edad vs deposit');\n\n\n\npng\n\n\nEl gráfico muestra las diferencias de edad entre quienes aceptan y quienes no.\nsns.countplot(x='marital', hue='deposit', data=data)\nplt.title('Estado civil vs Deposit');\n\n\n\npng\n\n\nEl gráfico permite ver si el estado civil influye en la decisiones\nplt.figure(figsize=(10,5))\nsns.countplot(y='job', hue='deposit', data=data, order=data['job'].value_counts().index)\nplt.title('Tipo de trabajo vs Deposit');\n\n\n\npng\n\n\nEl gráfico muestra qué profesiones tienen mayor tasa de aceptación.\nsns.histplot(data=data, x='duration', hue='deposit', bins=30, kde=True)\nplt.title('Duración de llamada vs deposit');\n\n\n\npng\n\n\ndata.groupby(\"deposit\")[\"duration\"].describe().round(2)\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\ndeposit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nno\n\n\n39922.0\n\n\n221.18\n\n\n207.38\n\n\n0.0\n\n\n95.0\n\n\n164.0\n\n\n279.0\n\n\n4918.0\n\n\n\n\nyes\n\n\n5289.0\n\n\n537.29\n\n\n392.53\n\n\n8.0\n\n\n244.0\n\n\n426.0\n\n\n725.0\n\n\n3881.0\n\n\n\n\n\nLa duración de las llamadas parece ser muy influyente: llamadas más largas tienden a correlacionarse con aceptación.\n\n\nModelado\nPara modelar en pycaret es necesario separar segun el tipos de variable y convertir en listas\ncategoricas = data.select_dtypes(include=[\"object\"]).columns.tolist()\nnumericas = data.select_dtypes(include=[\"number\"]).columns.tolist()\nprint(categoricas)\n['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']\nSe realiza un SMOTE en el dataset para datos sinteticos, por el tema de desbalance, se utiliza el 80% del dataset para el entrenamiento.\nwith contextlib.redirect_stdout(open(os.devnull, 'w')):\n    clf = setup( \n        data=data, \n        target='deposit',\n        fix_imbalance=True,\n        fix_imbalance_method='SMOTE',\n        train_size=0.8, \n        session_id=7402,\n        numeric_features=numericas, \n        categorical_features=categoricas[:-1],\n        normalize=True,   \n        transformation=True,       \n        feature_selection=True,    \n        remove_multicollinearity=True,\n        multicollinearity_threshold=0.9,\n        use_gpu=True, verbose=True )\n\n\n\n\n\n \n\n\nDescription\n\n\nValue\n\n\n\n\n\n\n0\n\n\nSession id\n\n\n7402\n\n\n\n\n1\n\n\nTarget\n\n\ndeposit\n\n\n\n\n2\n\n\nTarget type\n\n\nBinary\n\n\n\n\n3\n\n\nTarget mapping\n\n\nno: 0, yes: 1\n\n\n\n\n4\n\n\nOriginal data shape\n\n\n(45211, 17)\n\n\n\n\n5\n\n\nTransformed data shape\n\n\n(72917, 4)\n\n\n\n\n6\n\n\nTransformed train set shape\n\n\n(63874, 4)\n\n\n\n\n7\n\n\nTransformed test set shape\n\n\n(9043, 4)\n\n\n\n\n8\n\n\nNumeric features\n\n\n7\n\n\n\n\n9\n\n\nCategorical features\n\n\n9\n\n\n\n\n10\n\n\nPreprocess\n\n\nTrue\n\n\n\n\n11\n\n\nImputation type\n\n\nsimple\n\n\n\n\n12\n\n\nNumeric imputation\n\n\nmean\n\n\n\n\n13\n\n\nCategorical imputation\n\n\nmode\n\n\n\n\n14\n\n\nMaximum one-hot encoding\n\n\n25\n\n\n\n\n15\n\n\nEncoding method\n\n\nNone\n\n\n\n\n16\n\n\nRemove multicollinearity\n\n\nTrue\n\n\n\n\n17\n\n\nMulticollinearity threshold\n\n\n0.900000\n\n\n\n\n18\n\n\nFix imbalance\n\n\nTrue\n\n\n\n\n19\n\n\nFix imbalance method\n\n\nSMOTE\n\n\n\n\n20\n\n\nTransformation\n\n\nTrue\n\n\n\n\n21\n\n\nTransformation method\n\n\nyeo-johnson\n\n\n\n\n22\n\n\nNormalize\n\n\nTrue\n\n\n\n\n23\n\n\nNormalize method\n\n\nzscore\n\n\n\n\n24\n\n\nFeature selection\n\n\nTrue\n\n\n\n\n25\n\n\nFeature selection method\n\n\nclassic\n\n\n\n\n26\n\n\nFeature selection estimator\n\n\nlightgbm\n\n\n\n\n27\n\n\nNumber of features selected\n\n\n0.200000\n\n\n\n\n28\n\n\nFold Generator\n\n\nStratifiedKFold\n\n\n\n\n29\n\n\nFold Number\n\n\n10\n\n\n\n\n30\n\n\nCPU Jobs\n\n\n-1\n\n\n\n\n31\n\n\nUse GPU\n\n\nTrue\n\n\n\n\n32\n\n\nLog Experiment\n\n\nFalse\n\n\n\n\n33\n\n\nExperiment Name\n\n\nclf-default-name\n\n\n\n\n34\n\n\nUSI\n\n\nbeed\n\n\n\n\nSe selecciona como metrica el F1 porque nos da un balance entre Recall y Precision\nbest = compare_models(sort='F1')\nbest\n\n\n\n\n\n \n\n\nModel\n\n\nAccuracy\n\n\nAUC\n\n\nRecall\n\n\nPrec.\n\n\nF1\n\n\nKappa\n\n\nMCC\n\n\nTT (Sec)\n\n\n\n\n\n\nlightgbm\n\n\nLight Gradient Boosting Machine\n\n\n0.8821\n\n\n0.8303\n\n\n0.8821\n\n\n0.8657\n\n\n0.8714\n\n\n0.3317\n\n\n0.3406\n\n\n2.4650\n\n\n\n\nxgboost\n\n\nExtreme Gradient Boosting\n\n\n0.8712\n\n\n0.8191\n\n\n0.8712\n\n\n0.8630\n\n\n0.8663\n\n\n0.3311\n\n\n0.3340\n\n\n2.0040\n\n\n\n\net\n\n\nExtra Trees Classifier\n\n\n0.8466\n\n\n0.7836\n\n\n0.8466\n\n\n0.8534\n\n\n0.8498\n\n\n0.2893\n\n\n0.2897\n\n\n2.1900\n\n\n\n\ngbc\n\n\nGradient Boosting Classifier\n\n\n0.8294\n\n\n0.8212\n\n\n0.8294\n\n\n0.8683\n\n\n0.8450\n\n\n0.3382\n\n\n0.3502\n\n\n5.7350\n\n\n\n\nrf\n\n\nRandom Forest Classifier\n\n\n0.8327\n\n\n0.7907\n\n\n0.8327\n\n\n0.8562\n\n\n0.8428\n\n\n0.2944\n\n\n0.2992\n\n\n2.6410\n\n\n\n\ndummy\n\n\nDummy Classifier\n\n\n0.8830\n\n\n0.5000\n\n\n0.8830\n\n\n0.7797\n\n\n0.8282\n\n\n0.0000\n\n\n0.0000\n\n\n1.8110\n\n\n\n\ndt\n\n\nDecision Tree Classifier\n\n\n0.8193\n\n\n0.6025\n\n\n0.8193\n\n\n0.8330\n\n\n0.8257\n\n\n0.1897\n\n\n0.1907\n\n\n2.0340\n\n\n\n\ncatboost\n\n\nCatBoost Classifier\n\n\n0.7862\n\n\n0.8208\n\n\n0.7862\n\n\n0.8714\n\n\n0.8162\n\n\n0.3072\n\n\n0.3404\n\n\n4.2160\n\n\n\n\nsvm\n\n\nSVM - Linear Kernel\n\n\n0.7503\n\n\n0.8115\n\n\n0.7503\n\n\n0.8696\n\n\n0.7901\n\n\n0.2704\n\n\n0.3159\n\n\n1.9090\n\n\n\n\nknn\n\n\nK Neighbors Classifier\n\n\n0.7442\n\n\n0.7603\n\n\n0.7442\n\n\n0.8591\n\n\n0.7841\n\n\n0.2384\n\n\n0.2753\n\n\n1.9720\n\n\n\n\nada\n\n\nAda Boost Classifier\n\n\n0.7418\n\n\n0.8107\n\n\n0.7418\n\n\n0.8700\n\n\n0.7839\n\n\n0.2639\n\n\n0.3127\n\n\n2.7380\n\n\n\n\nridge\n\n\nRidge Classifier\n\n\n0.7386\n\n\n0.8121\n\n\n0.7386\n\n\n0.8704\n\n\n0.7816\n\n\n0.2624\n\n\n0.3128\n\n\n1.8630\n\n\n\n\nlda\n\n\nLinear Discriminant Analysis\n\n\n0.7386\n\n\n0.8121\n\n\n0.7386\n\n\n0.8704\n\n\n0.7816\n\n\n0.2624\n\n\n0.3128\n\n\n1.8940\n\n\n\n\nlr\n\n\nLogistic Regression\n\n\n0.7376\n\n\n0.8122\n\n\n0.7376\n\n\n0.8711\n\n\n0.7810\n\n\n0.2631\n\n\n0.3145\n\n\n1.9060\n\n\n\n\nnb\n\n\nNaive Bayes\n\n\n0.7308\n\n\n0.8056\n\n\n0.7308\n\n\n0.8689\n\n\n0.7756\n\n\n0.2522\n\n\n0.3035\n\n\n1.8410\n\n\n\n\nqda\n\n\nQuadratic Discriminant Analysis\n\n\n0.7305\n\n\n0.8066\n\n\n0.7305\n\n\n0.8692\n\n\n0.7755\n\n\n0.2529\n\n\n0.3047\n\n\n1.8390\n\n\n\n\n\n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               device='gpu', importance_type='split', learning_rate=0.1,\n               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n               objective=None, random_state=7402, reg_alpha=0.0, reg_lambda=0.0,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n LGBMClassifieriFitted\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               device='gpu', importance_type='split', learning_rate=0.1,\n               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n               objective=None, random_state=7402, reg_alpha=0.0, reg_lambda=0.0,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\n\nSe elegio el modelo Light Gradient Boosting Machine ya que proporciona un F1 alto y un AUC\nlightgbm= create_model('lightgbm')\n\n\n\n\n\n \n\n\nAccuracy\n\n\nAUC\n\n\nRecall\n\n\nPrec.\n\n\nF1\n\n\nKappa\n\n\nMCC\n\n\n\n\nFold\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n0\n\n\n0.8886\n\n\n0.8350\n\n\n0.8886\n\n\n0.8708\n\n\n0.8758\n\n\n0.3459\n\n\n0.3604\n\n\n\n\n1\n\n\n0.8806\n\n\n0.8165\n\n\n0.8806\n\n\n0.8611\n\n\n0.8677\n\n\n0.3048\n\n\n0.3160\n\n\n\n\n2\n\n\n0.8864\n\n\n0.8510\n\n\n0.8864\n\n\n0.8730\n\n\n0.8779\n\n\n0.3719\n\n\n0.3784\n\n\n\n\n3\n\n\n0.8819\n\n\n0.8315\n\n\n0.8819\n\n\n0.8682\n\n\n0.8735\n\n\n0.3505\n\n\n0.3562\n\n\n\n\n4\n\n\n0.8728\n\n\n0.8176\n\n\n0.8728\n\n\n0.8552\n\n\n0.8622\n\n\n0.2862\n\n\n0.2926\n\n\n\n\n5\n\n\n0.8781\n\n\n0.8274\n\n\n0.8781\n\n\n0.8607\n\n\n0.8672\n\n\n0.3099\n\n\n0.3177\n\n\n\n\n6\n\n\n0.8883\n\n\n0.8259\n\n\n0.8883\n\n\n0.8717\n\n\n0.8768\n\n\n0.3547\n\n\n0.3665\n\n\n\n\n7\n\n\n0.8761\n\n\n0.8296\n\n\n0.8761\n\n\n0.8613\n\n\n0.8672\n\n\n0.3178\n\n\n0.3230\n\n\n\n\n8\n\n\n0.8800\n\n\n0.8302\n\n\n0.8800\n\n\n0.8625\n\n\n0.8688\n\n\n0.3167\n\n\n0.3255\n\n\n\n\n9\n\n\n0.8880\n\n\n0.8385\n\n\n0.8880\n\n\n0.8720\n\n\n0.8771\n\n\n0.3585\n\n\n0.3692\n\n\n\n\nMean\n\n\n0.8821\n\n\n0.8303\n\n\n0.8821\n\n\n0.8657\n\n\n0.8714\n\n\n0.3317\n\n\n0.3406\n\n\n\n\nStd\n\n\n0.0053\n\n\n0.0095\n\n\n0.0053\n\n\n0.0059\n\n\n0.0052\n\n\n0.0267\n\n\n0.0274\n\n\n\n\nSe optimiza hiperparámetros del modelo seleccionado\nlightgbm_tuned = tune_model(lightgbm, optimize='F1')  \nlightgbm_tuned\n\n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               device='gpu', importance_type='split', learning_rate=0.1,\n               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n               objective=None, random_state=7402, reg_alpha=0.0, reg_lambda=0.0,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n LGBMClassifieriFitted\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               device='gpu', importance_type='split', learning_rate=0.1,\n               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n               objective=None, random_state=7402, reg_alpha=0.0, reg_lambda=0.0,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\n\nMatriz de confusion de los datos de entrenamiento\nplot_model(lightgbm_tuned, plot='confusion_matrix')\n\n\n\npng\n\n\nCurva ROC\nplot_model(lightgbm_tuned, plot='auc')\n\n\n\npng\n\n\nLas variables mas importantes fueron: dia de la semana de la ultima llamada, la duracion de la llamada y balance\nplot_model(lightgbm_tuned, plot='feature')\n\n\n\npng\n\n\nVamos a utilizar el modelo entrenado sobre el dataset de test, que fueron el 20%, en el cual tenemos un F1 del 0.8746\nholdout_preds = predict_model(lightgbm_tuned)\nholdout_preds.head()\n\n\n\n\n\n \n\n\nModel\n\n\nAccuracy\n\n\nAUC\n\n\nRecall\n\n\nPrec.\n\n\nF1\n\n\nKappa\n\n\nMCC\n\n\n\n\n\n\n0\n\n\nLight Gradient Boosting Machine\n\n\n0.8842\n\n\n0.8382\n\n\n0.8842\n\n\n0.8691\n\n\n0.8746\n\n\n0.3510\n\n\n0.3588\n\n\n\n\n\n\n\n\n\n\n\n\nage\n\n\njob\n\n\nmarital\n\n\neducation\n\n\ndefault\n\n\nbalance\n\n\nhousing\n\n\nloan\n\n\ncontact\n\n\nday\n\n\nmonth\n\n\nduration\n\n\ncampaign\n\n\npdays\n\n\nprevious\n\n\npoutcome\n\n\ndeposit\n\n\nprediction_label\n\n\nprediction_score\n\n\n\n\n\n\n23776\n\n\n55\n\n\ntechnician\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n55\n\n\nno\n\n\nyes\n\n\ncellular\n\n\n28\n\n\naug\n\n\n19\n\n\n17\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\nno\n\n\n0.9991\n\n\n\n\n29237\n\n\n60\n\n\nblue-collar\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n272\n\n\nno\n\n\nno\n\n\ncellular\n\n\n2\n\n\nfeb\n\n\n421\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\nno\n\n\n0.6191\n\n\n\n\n41064\n\n\n38\n\n\ntechnician\n\n\ndivorced\n\n\nsecondary\n\n\nno\n\n\n902\n\n\nyes\n\n\nno\n\n\ncellular\n\n\n14\n\n\naug\n\n\n108\n\n\n1\n\n\n102\n\n\n4\n\n\nsuccess\n\n\nno\n\n\nno\n\n\n0.9177\n\n\n\n\n21071\n\n\n57\n\n\nretired\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n209\n\n\nno\n\n\nno\n\n\ncellular\n\n\n14\n\n\naug\n\n\n56\n\n\n4\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nno\n\n\nno\n\n\n0.9892\n\n\n\n\n43775\n\n\n78\n\n\nretired\n\n\nmarried\n\n\nprimary\n\n\nno\n\n\n680\n\n\nno\n\n\nno\n\n\ntelephone\n\n\n24\n\n\nmay\n\n\n838\n\n\n1\n\n\n89\n\n\n9\n\n\nfailure\n\n\nno\n\n\nno\n\n\n0.5004\n\n\n\n\n\nMatriz de confusion sobre el dataset de test\ny_true = holdout_preds['deposit']  \ny_pred = holdout_preds['prediction_label']     \n\ncm = confusion_matrix(y_true, y_pred, labels=['yes','no'])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['yes','no'])\ndisp.plot(cmap='Blues')\nplt.title(\"Matriz de confusión en datos de test\");\n\n\n\npng"
  },
  {
    "objectID": "p/p_ts.html",
    "href": "p/p_ts.html",
    "title": "Analisis del dataset de CO2",
    "section": "",
    "text": "Analisis del dataset de CO2\nEl dataset de CO₂ que comienza en 1958 y llega hasta 2021 corresponde a las mediciones en el Observatorio de Mauna Loa (Hawái). Es la serie temporal más famosa de dióxido de carbono atmosférico, conocida como la Curva de Keeling, y se considera el registro continuo más largo de CO₂ en la atmósfera\n\nCargar las librerias y el dataset\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom pycaret.time_series import *\nfrom statsmodels.graphics.tsaplots import month_plot, quarter_plot, plot_pacf\nimport os, contextlib\ndata = pd.read_csv(\"./mauna_loa_co2.csv\", index_col = 'datetime', parse_dates = True)\ndata = data.asfreq(\"M\")\ndata.head()\n\n\n\n\n\n\n\n\nCO2\n\n\n\n\ndatetime\n\n\n\n\n\n\n\n\n1958-03-31\n\n\n315.70\n\n\n\n\n1958-04-30\n\n\n317.45\n\n\n\n\n1958-05-31\n\n\n317.51\n\n\n\n\n1958-06-30\n\n\n317.25\n\n\n\n\n1958-07-31\n\n\n315.86\n\n\n\n\n\nEl índice son las fechas del ultimo día de cada mes\n\n\nAnálisis Exploratorio de Datos\ndata.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 766 entries, 1958-03-31 to 2021-12-31\nFreq: M\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   CO2     766 non-null    float64\ndtypes: float64(1)\nmemory usage: 12.0 KB\nplt.figure(figsize=(12,6))\nsns.lineplot(x='datetime', y='CO2', data=data)\nplt.title(\"Evolución mensual de CO₂ (1958–2021)\")\nplt.grid(False)\nplt.ylabel(\"CO₂ (ppm)\")\nplt.xlabel(\"Años\");\n\n\n\npng\n\n\nCon el pasar de los años el CO2 fue aumentando\nsns.histplot(data['CO2'], bins=30, kde=True)\nplt.title(\"Distribución de concentraciones de CO₂\")\nplt.xlabel(\"CO₂ (ppm)\");\n\n\n\npng\n\n\nA nivel mensual no se observa una estacionalidad\nmonth_plot(data);\n\n\n\npng\n\n\nTampoco se obseva estacionalidad a nivel trimestral\nquarter_plot(data[\"CO2\"].resample(\"Q\").mean());\n\n\n\npng\n\n\nSe observa una tendencia creciente, los residuos esta no estan dispersos\nseasonal_decompose(data[\"CO2\"], model=\"add\", period=12).plot().set_size_inches(18,6); \n\n\n\npng\n\n\nEn el gráfico de la autocorrelación parcial, se observa que el anterior mes y el mes del año anterior son importantes para la predicción\nplot_pacf(data[\"CO2\"], lags=24).set_size_inches(18,6);\n\n\n\npng\n\n\n\n\nModelado\nAntes de iniciar el modelo, no usaremos todo el dataset, porque hay valores muy antiguos, tomaremos encuenta desde 2014.\nnew_data = data[\"2014\":]\nnew_data.head()\n\n\n\n\n\n\n\n\nCO2\n\n\n\n\ndatetime\n\n\n\n\n\n\n\n\n2014-01-31\n\n\n398.01\n\n\n\n\n2014-02-28\n\n\n398.18\n\n\n\n\n2014-03-31\n\n\n399.56\n\n\n\n\n2014-04-30\n\n\n401.44\n\n\n\n\n2014-05-31\n\n\n401.99\n\n\n\n\n\nSe entrenara con casi todo el dataset, menos los ultimos 12 meses, que serviran de comparacion\nwith contextlib.redirect_stdout(open(os.devnull, 'w')):\n    exp = setup(\n        data=new_data,\n        target='CO2',\n        fh=12,                   \n        fold_strategy='expanding', \n        fold=3,  \n        session_id=123,\n        use_gpu=True, verbose=True\n        )\n\n\n\n\n\n \n\n\nDescription\n\n\nValue\n\n\n\n\n\n\n0\n\n\nsession_id\n\n\n123\n\n\n\n\n1\n\n\nTarget\n\n\nCO2\n\n\n\n\n2\n\n\nApproach\n\n\nUnivariate\n\n\n\n\n3\n\n\nExogenous Variables\n\n\nNot Present\n\n\n\n\n4\n\n\nOriginal data shape\n\n\n(96, 1)\n\n\n\n\n5\n\n\nTransformed data shape\n\n\n(96, 1)\n\n\n\n\n6\n\n\nTransformed train set shape\n\n\n(84, 1)\n\n\n\n\n7\n\n\nTransformed test set shape\n\n\n(12, 1)\n\n\n\n\n8\n\n\nRows with missing values\n\n\n0.0%\n\n\n\n\n9\n\n\nFold Generator\n\n\nExpandingWindowSplitter\n\n\n\n\n10\n\n\nFold Number\n\n\n3\n\n\n\n\n11\n\n\nEnforce Prediction Interval\n\n\nFalse\n\n\n\n\n12\n\n\nSplits used for hyperparameters\n\n\nall\n\n\n\n\n13\n\n\nUser Defined Seasonal Period(s)\n\n\nNone\n\n\n\n\n14\n\n\nIgnore Seasonality Test\n\n\nFalse\n\n\n\n\n15\n\n\nSeasonality Detection Algo\n\n\nauto\n\n\n\n\n16\n\n\nMax Period to Consider\n\n\n60\n\n\n\n\n17\n\n\nSeasonal Period(s) Tested\n\n\n[12, 11, 24, 13]\n\n\n\n\n18\n\n\nSignificant Seasonal Period(s)\n\n\n[12, 11, 24, 13]\n\n\n\n\n19\n\n\nSignificant Seasonal Period(s) without Harmonics\n\n\n[24, 11, 13]\n\n\n\n\n20\n\n\nRemove Harmonics\n\n\nFalse\n\n\n\n\n21\n\n\nHarmonics Order Method\n\n\nharmonic_max\n\n\n\n\n22\n\n\nNum Seasonalities to Use\n\n\n1\n\n\n\n\n23\n\n\nAll Seasonalities to Use\n\n\n[12]\n\n\n\n\n24\n\n\nPrimary Seasonality\n\n\n12\n\n\n\n\n25\n\n\nSeasonality Present\n\n\nTrue\n\n\n\n\n26\n\n\nSeasonality Type\n\n\nmul\n\n\n\n\n27\n\n\nTarget Strictly Positive\n\n\nTrue\n\n\n\n\n28\n\n\nTarget White Noise\n\n\nNo\n\n\n\n\n29\n\n\nRecommended d\n\n\n1\n\n\n\n\n30\n\n\nRecommended Seasonal D\n\n\n1\n\n\n\n\n31\n\n\nPreprocess\n\n\nFalse\n\n\n\n\n32\n\n\nCPU Jobs\n\n\n-1\n\n\n\n\n33\n\n\nUse GPU\n\n\nTrue\n\n\n\n\n34\n\n\nLog Experiment\n\n\nFalse\n\n\n\n\n35\n\n\nExperiment Name\n\n\nts-default-name\n\n\n\n\n36\n\n\nUSI\n\n\n336c\n\n\n\n\nUsaremos la metrica de MAPE para elegir el mejor modelo\nbest = compare_models(sort='MAPE')\nbest\n\n\n\n\n\n \n\n\nModel\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\nR2\n\n\nTT (Sec)\n\n\n\n\n\n\nauto_arima\n\n\nAuto ARIMA\n\n\n0.1159\n\n\n0.1485\n\n\n0.2941\n\n\n0.3914\n\n\n0.0007\n\n\n0.0007\n\n\n0.9439\n\n\n12.2633\n\n\n\n\nexp_smooth\n\n\nExponential Smoothing\n\n\n0.1093\n\n\n0.1421\n\n\n0.2779\n\n\n0.3750\n\n\n0.0007\n\n\n0.0007\n\n\n0.9502\n\n\n0.0467\n\n\n\n\nets\n\n\nETS\n\n\n0.1093\n\n\n0.1421\n\n\n0.2779\n\n\n0.3750\n\n\n0.0007\n\n\n0.0007\n\n\n0.9502\n\n\n0.0933\n\n\n\n\nada_cds_dt\n\n\nAdaBoost w/ Cond. Deseasonalize & Detrending\n\n\n0.1552\n\n\n0.1801\n\n\n0.3912\n\n\n0.4714\n\n\n0.0009\n\n\n0.0010\n\n\n0.9313\n\n\n0.2700\n\n\n\n\nstlf\n\n\nSTLF\n\n\n0.1459\n\n\n0.1734\n\n\n0.3678\n\n\n0.4549\n\n\n0.0009\n\n\n0.0009\n\n\n0.9256\n\n\n0.0367\n\n\n\n\nomp_cds_dt\n\n\nOrthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending\n\n\n0.1680\n\n\n0.1903\n\n\n0.4252\n\n\n0.4996\n\n\n0.0010\n\n\n0.0010\n\n\n0.9058\n\n\n0.2067\n\n\n\n\ncatboost_cds_dt\n\n\nCatBoost Regressor w/ Cond. Deseasonalize & Detrending\n\n\n0.1705\n\n\n0.1995\n\n\n0.4307\n\n\n0.5229\n\n\n0.0010\n\n\n0.0010\n\n\n0.9172\n\n\n3.4733\n\n\n\n\narima\n\n\nARIMA\n\n\n0.1590\n\n\n0.1792\n\n\n0.4026\n\n\n0.4709\n\n\n0.0010\n\n\n0.0010\n\n\n0.9176\n\n\n0.0500\n\n\n\n\nrf_cds_dt\n\n\nRandom Forest w/ Cond. Deseasonalize & Detrending\n\n\n0.1637\n\n\n0.1882\n\n\n0.4129\n\n\n0.4923\n\n\n0.0010\n\n\n0.0010\n\n\n0.9258\n\n\n0.4533\n\n\n\n\nlr_cds_dt\n\n\nLinear w/ Cond. Deseasonalize & Detrending\n\n\n0.1842\n\n\n0.2047\n\n\n0.4655\n\n\n0.5366\n\n\n0.0011\n\n\n0.0011\n\n\n0.9036\n\n\n0.2000\n\n\n\n\ngbr_cds_dt\n\n\nGradient Boosting w/ Cond. Deseasonalize & Detrending\n\n\n0.1721\n\n\n0.1961\n\n\n0.4350\n\n\n0.5146\n\n\n0.0011\n\n\n0.0011\n\n\n0.9192\n\n\n0.2367\n\n\n\n\net_cds_dt\n\n\nExtra Trees w/ Cond. Deseasonalize & Detrending\n\n\n0.1778\n\n\n0.1981\n\n\n0.4492\n\n\n0.5197\n\n\n0.0011\n\n\n0.0011\n\n\n0.9161\n\n\n0.4233\n\n\n\n\nbr_cds_dt\n\n\nBayesian Ridge w/ Cond. Deseasonalize & Detrending\n\n\n0.1786\n\n\n0.1986\n\n\n0.4514\n\n\n0.5207\n\n\n0.0011\n\n\n0.0011\n\n\n0.9084\n\n\n0.2067\n\n\n\n\nridge_cds_dt\n\n\nRidge w/ Cond. Deseasonalize & Detrending\n\n\n0.1816\n\n\n0.2024\n\n\n0.4590\n\n\n0.5305\n\n\n0.0011\n\n\n0.0011\n\n\n0.9053\n\n\n0.2033\n\n\n\n\nhuber_cds_dt\n\n\nHuber w/ Cond. Deseasonalize & Detrending\n\n\n0.1899\n\n\n0.2190\n\n\n0.4784\n\n\n0.5720\n\n\n0.0012\n\n\n0.0012\n\n\n0.8948\n\n\n0.2133\n\n\n\n\nxgboost_cds_dt\n\n\nExtreme Gradient Boosting w/ Cond. Deseasonalize & Detrending\n\n\n0.2172\n\n\n0.2385\n\n\n0.5500\n\n\n0.6270\n\n\n0.0013\n\n\n0.0013\n\n\n0.8783\n\n\n0.3267\n\n\n\n\nen_cds_dt\n\n\nElastic Net w/ Cond. Deseasonalize & Detrending\n\n\n0.2197\n\n\n0.2508\n\n\n0.5539\n\n\n0.6562\n\n\n0.0013\n\n\n0.0013\n\n\n0.8620\n\n\n0.2000\n\n\n\n\nlasso_cds_dt\n\n\nLasso w/ Cond. Deseasonalize & Detrending\n\n\n0.2243\n\n\n0.2593\n\n\n0.5648\n\n\n0.6779\n\n\n0.0014\n\n\n0.0014\n\n\n0.8550\n\n\n0.2067\n\n\n\n\ndt_cds_dt\n\n\nDecision Tree w/ Cond. Deseasonalize & Detrending\n\n\n0.2221\n\n\n0.2778\n\n\n0.5612\n\n\n0.7276\n\n\n0.0014\n\n\n0.0014\n\n\n0.8313\n\n\n0.2133\n\n\n\n\nllar_cds_dt\n\n\nLasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending\n\n\n0.2243\n\n\n0.2593\n\n\n0.5649\n\n\n0.6779\n\n\n0.0014\n\n\n0.0014\n\n\n0.8550\n\n\n0.2100\n\n\n\n\nknn_cds_dt\n\n\nK Neighbors w/ Cond. Deseasonalize & Detrending\n\n\n0.2196\n\n\n0.2444\n\n\n0.5555\n\n\n0.6410\n\n\n0.0014\n\n\n0.0014\n\n\n0.8720\n\n\n0.3600\n\n\n\n\ntheta\n\n\nTheta Forecaster\n\n\n0.2636\n\n\n0.3004\n\n\n0.6657\n\n\n0.7873\n\n\n0.0016\n\n\n0.0016\n\n\n0.8118\n\n\n0.0233\n\n\n\n\nlightgbm_cds_dt\n\n\nLight Gradient Boosting w/ Cond. Deseasonalize & Detrending\n\n\n0.3385\n\n\n0.3940\n\n\n0.8681\n\n\n1.0470\n\n\n0.0021\n\n\n0.0021\n\n\n0.4626\n\n\n0.4833\n\n\n\n\npolytrend\n\n\nPolynomial Trend Forecaster\n\n\n0.7683\n\n\n0.8534\n\n\n1.9426\n\n\n2.2377\n\n\n0.0047\n\n\n0.0047\n\n\n-0.5182\n\n\n0.0133\n\n\n\n\nnaive\n\n\nNaive Forecaster\n\n\n0.9256\n\n\n1.0532\n\n\n2.3406\n\n\n2.7611\n\n\n0.0057\n\n\n0.0057\n\n\n-1.3108\n\n\n0.0267\n\n\n\n\nsnaive\n\n\nSeasonal Naive Forecaster\n\n\n0.9817\n\n\n0.9564\n\n\n2.4772\n\n\n2.5031\n\n\n0.0060\n\n\n0.0060\n\n\n-0.9030\n\n\n0.0400\n\n\n\n\ncroston\n\n\nCroston\n\n\n1.4725\n\n\n1.5799\n\n\n3.7221\n\n\n4.1416\n\n\n0.0090\n\n\n0.0091\n\n\n-4.1988\n\n\n0.0133\n\n\n\n\ngrand_means\n\n\nGrand Means Forecaster\n\n\n2.9694\n\n\n2.9479\n\n\n7.5052\n\n\n7.7253\n\n\n0.0182\n\n\n0.0184\n\n\n-17.3184\n\n\n0.0200\n\n\n\n\n\n\n\nAutoARIMA(random_state=123, sp=12, suppress_warnings=True)\nPlease rerun this cell to show the HTML repr or trust the notebook.\n\n\n\n\nAutoARIMA\n\nAutoARIMA(random_state=123, sp=12, suppress_warnings=True)\n\n\n\n\n\nDebido a que 3 modelos tienen el mismo MAPE, elegimos el modelo Exponential Smoothing, ya que sus otras metricas tambien son buenas\nmodel = create_model('exp_smooth')\n\n\n\n\n\n \n\n\ncutoff\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\nR2\n\n\n\n\n\n\n0\n\n\n2017-12\n\n\n0.1404\n\n\n0.2011\n\n\n0.3655\n\n\n0.5422\n\n\n0.0009\n\n\n0.0009\n\n\n0.8993\n\n\n\n\n1\n\n\n2018-12\n\n\n0.1016\n\n\n0.1209\n\n\n0.2495\n\n\n0.3085\n\n\n0.0006\n\n\n0.0006\n\n\n0.9738\n\n\n\n\n2\n\n\n2019-12\n\n\n0.0860\n\n\n0.1043\n\n\n0.2186\n\n\n0.2742\n\n\n0.0005\n\n\n0.0005\n\n\n0.9775\n\n\n\n\nMean\n\n\nNaT\n\n\n0.1093\n\n\n0.1421\n\n\n0.2779\n\n\n0.3750\n\n\n0.0007\n\n\n0.0007\n\n\n0.9502\n\n\n\n\nSD\n\n\nNaT\n\n\n0.0229\n\n\n0.0423\n\n\n0.0632\n\n\n0.1191\n\n\n0.0002\n\n\n0.0002\n\n\n0.0360\n\n\n\n\nSe optimiza hiperparámetros del modelo seleccionado\ntuned_model = tune_model(model)\ntuned_model\n\n\n\n\n\n \n\n\ncutoff\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\nR2\n\n\n\n\n\n\n0\n\n\n2017-12\n\n\n0.1352\n\n\n0.1918\n\n\n0.3518\n\n\n0.5172\n\n\n0.0009\n\n\n0.0009\n\n\n0.9084\n\n\n\n\n1\n\n\n2018-12\n\n\n0.1024\n\n\n0.1184\n\n\n0.2515\n\n\n0.3023\n\n\n0.0006\n\n\n0.0006\n\n\n0.9749\n\n\n\n\n2\n\n\n2019-12\n\n\n0.0783\n\n\n0.0972\n\n\n0.1991\n\n\n0.2557\n\n\n0.0005\n\n\n0.0005\n\n\n0.9805\n\n\n\n\nMean\n\n\nNaT\n\n\n0.1053\n\n\n0.1358\n\n\n0.2675\n\n\n0.3584\n\n\n0.0007\n\n\n0.0007\n\n\n0.9546\n\n\n\n\nSD\n\n\nNaT\n\n\n0.0233\n\n\n0.0405\n\n\n0.0634\n\n\n0.1139\n\n\n0.0002\n\n\n0.0002\n\n\n0.0327\n\n\n\n\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.3s finished\n\n\n\nExponentialSmoothing(seasonal='add', sp=12, trend='mul', use_boxcox=False)\nPlease rerun this cell to show the HTML repr or trust the notebook.\n\n\n\n\nExponentialSmoothing\n\nExponentialSmoothing(seasonal='add', sp=12, trend='mul', use_boxcox=False)\n\n\n\n\n\nSe muestran los valores que predijo el modelo con los ultimo 12 meses del dataset.\nfuture_preds = predict_model(tuned_model, fh=12)\nprint(future_preds)\n\n\n\n\n\n \n\n\nModel\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\nR2\n\n\n\n\n\n\n0\n\n\nExponential Smoothing\n\n\n0.1079\n\n\n0.1194\n\n\n0.2742\n\n\n0.3120\n\n\n0.0007\n\n\n0.0007\n\n\n0.9732\n\n\n\n\n           y_pred\n2021-01  415.4306\n2021-02  416.0870\n2021-03  416.8464\n2021-04  418.6585\n2021-05  419.4407\n2021-06  418.6742\n2021-07  416.6934\n2021-08  414.6729\n2021-09  413.1250\n2021-10  413.4386\n2021-11  415.2763\n2021-12  416.6386\nEl gráfico muestra la predición del modelo comparado con el valor real\nplot_model(tuned_model, plot='forecast')  \nEn el Q-Q plot muestra que los datos tienen una distribución normal\nplot_model(tuned_model, plot='diagnostics') \n\n\nModelado con todo el dataset con exp_smooth\nUtilizaremos todo el dataset desde 2014, eligiendo el modelo de Exponential Smoothing y pronosticando los próximos 12 meses\nwith contextlib.redirect_stdout(open(os.devnull, 'w')):\n    exp = setup(\n        data=new_data,\n        target='CO2',                \n        fold_strategy='expanding', \n        fold=3,  \n        session_id=123,\n        use_gpu=True, verbose=True\n        )\n\nmodel = create_model('exp_smooth')\ntuned_model = tune_model(model)\nfuture_preds = predict_model(tuned_model, fh=12)\n\n\n\n\n\n \n\n\nDescription\n\n\nValue\n\n\n\n\n\n\n0\n\n\nsession_id\n\n\n123\n\n\n\n\n1\n\n\nTarget\n\n\nCO2\n\n\n\n\n2\n\n\nApproach\n\n\nUnivariate\n\n\n\n\n3\n\n\nExogenous Variables\n\n\nNot Present\n\n\n\n\n4\n\n\nOriginal data shape\n\n\n(96, 1)\n\n\n\n\n5\n\n\nTransformed data shape\n\n\n(96, 1)\n\n\n\n\n6\n\n\nTransformed train set shape\n\n\n(95, 1)\n\n\n\n\n7\n\n\nTransformed test set shape\n\n\n(1, 1)\n\n\n\n\n8\n\n\nRows with missing values\n\n\n0.0%\n\n\n\n\n9\n\n\nFold Generator\n\n\nExpandingWindowSplitter\n\n\n\n\n10\n\n\nFold Number\n\n\n3\n\n\n\n\n11\n\n\nEnforce Prediction Interval\n\n\nFalse\n\n\n\n\n12\n\n\nSplits used for hyperparameters\n\n\nall\n\n\n\n\n13\n\n\nUser Defined Seasonal Period(s)\n\n\nNone\n\n\n\n\n14\n\n\nIgnore Seasonality Test\n\n\nFalse\n\n\n\n\n15\n\n\nSeasonality Detection Algo\n\n\nauto\n\n\n\n\n16\n\n\nMax Period to Consider\n\n\n60\n\n\n\n\n17\n\n\nSeasonal Period(s) Tested\n\n\n[12, 11, 24, 13]\n\n\n\n\n18\n\n\nSignificant Seasonal Period(s)\n\n\n[12, 11, 24, 13]\n\n\n\n\n19\n\n\nSignificant Seasonal Period(s) without Harmonics\n\n\n[24, 11, 13]\n\n\n\n\n20\n\n\nRemove Harmonics\n\n\nFalse\n\n\n\n\n21\n\n\nHarmonics Order Method\n\n\nharmonic_max\n\n\n\n\n22\n\n\nNum Seasonalities to Use\n\n\n1\n\n\n\n\n23\n\n\nAll Seasonalities to Use\n\n\n[12]\n\n\n\n\n24\n\n\nPrimary Seasonality\n\n\n12\n\n\n\n\n25\n\n\nSeasonality Present\n\n\nTrue\n\n\n\n\n26\n\n\nSeasonality Type\n\n\nmul\n\n\n\n\n27\n\n\nTarget Strictly Positive\n\n\nTrue\n\n\n\n\n28\n\n\nTarget White Noise\n\n\nNo\n\n\n\n\n29\n\n\nRecommended d\n\n\n1\n\n\n\n\n30\n\n\nRecommended Seasonal D\n\n\n1\n\n\n\n\n31\n\n\nPreprocess\n\n\nFalse\n\n\n\n\n32\n\n\nCPU Jobs\n\n\n-1\n\n\n\n\n33\n\n\nUse GPU\n\n\nTrue\n\n\n\n\n34\n\n\nLog Experiment\n\n\nFalse\n\n\n\n\n35\n\n\nExperiment Name\n\n\nts-default-name\n\n\n\n\n36\n\n\nUSI\n\n\nabd6\n\n\n\n\n\n\n\n\n\n \n\n\ncutoff\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\n\n\n\n\n0\n\n\n2021-08\n\n\n0.0321\n\n\n0.0312\n\n\n0.0802\n\n\n0.0802\n\n\n0.0002\n\n\n0.0002\n\n\n\n\n1\n\n\n2021-09\n\n\n0.1512\n\n\n0.1469\n\n\n0.3770\n\n\n0.3770\n\n\n0.0009\n\n\n0.0009\n\n\n\n\n2\n\n\n2021-10\n\n\n0.1748\n\n\n0.1699\n\n\n0.4356\n\n\n0.4356\n\n\n0.0011\n\n\n0.0010\n\n\n\n\nMean\n\n\nNaT\n\n\n0.1193\n\n\n0.1160\n\n\n0.2976\n\n\n0.2976\n\n\n0.0007\n\n\n0.0007\n\n\n\n\nSD\n\n\nNaT\n\n\n0.0625\n\n\n0.0607\n\n\n0.1556\n\n\n0.1556\n\n\n0.0004\n\n\n0.0004\n\n\n\n\n\n\n\n\n\n \n\n\ncutoff\n\n\nMASE\n\n\nRMSSE\n\n\nMAE\n\n\nRMSE\n\n\nMAPE\n\n\nSMAPE\n\n\n\n\n\n\n0\n\n\n2021-08\n\n\n0.0032\n\n\n0.0031\n\n\n0.0079\n\n\n0.0079\n\n\n0.0000\n\n\n0.0000\n\n\n\n\n1\n\n\n2021-09\n\n\n0.1335\n\n\n0.1298\n\n\n0.3330\n\n\n0.3330\n\n\n0.0008\n\n\n0.0008\n\n\n\n\n2\n\n\n2021-10\n\n\n0.1646\n\n\n0.1600\n\n\n0.4102\n\n\n0.4102\n\n\n0.0010\n\n\n0.0010\n\n\n\n\nMean\n\n\nNaT\n\n\n0.1004\n\n\n0.0976\n\n\n0.2504\n\n\n0.2504\n\n\n0.0006\n\n\n0.0006\n\n\n\n\nSD\n\n\nNaT\n\n\n0.0699\n\n\n0.0680\n\n\n0.1743\n\n\n0.1743\n\n\n0.0004\n\n\n0.0004\n\n\n\n\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    2.3s finished\nLos valores de los próximos 12 meses son los siguientes\nprint(future_preds)\n           y_pred\n2021-12  416.3793\n2022-01  417.6960\n2022-02  418.4364\n2022-03  419.1880\n2022-04  420.9097\n2022-05  421.6838\n2022-06  420.9830\n2022-07  418.9946\n2022-08  416.9388\n2022-09  415.4054\n2022-10  415.7621\n2022-11  417.5300\nhist = new_data.rename(columns={\"CO2\":\"valor\"}).assign(tipo=\"Histórico\")\npred = future_preds.rename(columns={\"y_pred\":\"valor\"}).assign(tipo=\"Predicción\")\n\ncombined = pd.concat([hist, pred])\n\nplt.figure(figsize=(12,6))\nsns.lineplot(data=combined, x=combined.index, y=\"valor\", hue=\"tipo\", palette={\"Histórico\":\"blue\", \"Predicción\":\"orange\"})\n\nplt.title(\"Pronóstico de CO2 de los próximos 12 meses\")\nplt.xlabel(\"Fecha\")\nplt.ylabel(\"CO2\")\nplt.legend();\n\n\n\npng"
  },
  {
    "objectID": "r/r_price.html",
    "href": "r/r_price.html",
    "title": "Dataset Sacramento",
    "section": "",
    "text": "El dataset de Sacramento contiene información sobre viviendas y precios de venta de 932 casas en Sacramento, California. Los datos originales se obtuvieron del sitio web del software SpatialKey. Según su sitio web: «El archivo de transacciones inmobiliarias de Sacramento es una lista de 985 transacciones inmobiliarias en el área de Sacramento, registradas durante un período de cinco días, según lo publicado por el Sacramento Bee». Se utilizó Google para completar los datos faltantes o incorrectos.\nMas información\n\n\n\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nCargar el dataset dentro de la variable datos.\n\ndata(Sacramento)\ndatos  = Sacramento\n\nInformación del dataset\n\n# city: Ciudad donde se encuentra la propiedad (factor).\n# zip: Código postal (factor).\n# beds: Número de habitaciones (numérico).\n# baths: Número de baños (numérico).\n# sqft: Superficie en pies cuadrados (numérico).\n# type: Tipo de propiedad (factor: Residential, Condo, Multi-Family, etc.).\n# price: Precio de venta en dólares (numérico).\n# latitude: Latitud geográfica (numérico).\n# longitude: Longitud geográfica (numérico).\n\n\ndim(datos)\n\n[1] 932   9\n\n\nEl dataset tiene 932 observaciones y 9 variables.\n\nstr(datos)\n\n'data.frame':   932 obs. of  9 variables:\n $ city     : Factor w/ 37 levels \"ANTELOPE\",\"AUBURN\",..: 34 34 34 34 34 34 34 34 29 31 ...\n $ zip      : Factor w/ 68 levels \"z95603\",\"z95608\",..: 64 52 44 44 53 65 66 49 24 25 ...\n $ beds     : int  2 3 2 2 2 3 3 3 2 3 ...\n $ baths    : num  1 1 1 1 1 1 2 1 2 2 ...\n $ sqft     : int  836 1167 796 852 797 1122 1104 1177 941 1146 ...\n $ type     : Factor w/ 3 levels \"Condo\",\"Multi_Family\",..: 3 3 3 3 3 1 3 3 1 3 ...\n $ price    : int  59222 68212 68880 69307 81900 89921 90895 91002 94905 98937 ...\n $ latitude : num  38.6 38.5 38.6 38.6 38.5 ...\n $ longitude: num  -121 -121 -121 -121 -121 ...\n\n\nSe observa que existen variables numéricas y categóricas, la variable a analizar es price, la cual es numérica.\n\nResumen estadístico del dataset\n\nsummary(datos)\n\n             city          zip           beds           baths      \n SACRAMENTO    :438   z95823 : 61   Min.   :1.000   Min.   :1.000  \n ELK_GROVE     :114   z95828 : 45   1st Qu.:3.000   1st Qu.:2.000  \n ROSEVILLE     : 48   z95758 : 44   Median :3.000   Median :2.000  \n CITRUS_HEIGHTS: 35   z95835 : 37   Mean   :3.276   Mean   :2.053  \n ANTELOPE      : 33   z95838 : 37   3rd Qu.:4.000   3rd Qu.:2.000  \n RANCHO_CORDOVA: 28   z95757 : 36   Max.   :8.000   Max.   :5.000  \n (Other)       :236   (Other):672                                  \n      sqft                type         price           latitude    \n Min.   : 484   Condo       : 53   Min.   : 30000   Min.   :38.24  \n 1st Qu.:1167   Multi_Family: 13   1st Qu.:156000   1st Qu.:38.48  \n Median :1470   Residential :866   Median :220000   Median :38.62  \n Mean   :1680                      Mean   :246662   Mean   :38.59  \n 3rd Qu.:1954                      3rd Qu.:305000   3rd Qu.:38.69  \n Max.   :4878                      Max.   :884790   Max.   :39.02  \n                                                                   \n   longitude     \n Min.   :-121.6  \n 1st Qu.:-121.4  \n Median :-121.4  \n Mean   :-121.4  \n 3rd Qu.:-121.3  \n Max.   :-120.6  \n                 \n\n\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price), alpha = 0.7, size = 3) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  labs(title = \"Mapa de precios de viviendas en Sacramento\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico es un Scatterplot geográfico: latitud vs longitud\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price), alpha = 0.7, size = 3) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  facet_wrap(~ type) +\n  labs(title = \"Mapa de precios por tipo de propiedad\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico de latitud vs longitud se divido en 3 categorias: condominio, multifamiliar y residencial\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price, size = beds), alpha = 0.6) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  labs(title = \"Mapa de precios con tamaño según habitaciones\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\", size = \"Habitaciones\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico de latitud vs longitud, se añadio el tamaño de las habitaciones\n\nggplot(datos, aes(x=price)) +  geom_histogram(color=\"black\", fill=\"lightblue\",\n                                              linetype=\"dashed\")+ xlab(label = \"Precio (USD)\")+\n  labs(title =\"Distribución de log precios de casas\" )\n\n\n\n\n\n\n\n\nDistribución de precios de las viviendas, se ve una asimetría en los datos\n\nggplot(datos, aes(x=log10(price))) +  geom_histogram(color=\"black\", fill=\"lightgreen\",\n                                              linetype=\"dashed\")+ xlab(label = \"Precio (USD)\")+\n  labs(title =\"Distribución de log precios de casas\" )\n\n\n\n\n\n\n\n\nDistribución de log10(Precio) de las viviendas, debido a que existe una asimetría en los datos\n\nggplot(datos) +\n  geom_density(aes(x = log10(price), fill = type), alpha=0.25)+\n  ggtitle(\"Densidad sobre log10(price) de las viviendas\")\n\n\n\n\n\n\n\n\n\nggplot(data=datos,aes(x=log10(price), y=sqft,color=type)) +\n  geom_boxplot() +theme_minimal()+ theme() +\n  ggtitle(\"Boxplot de log10(price) de las viviendas\")\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\ntrain &lt;- createDataPartition(log10(datos$price), p = 0.8, list = FALSE)\ntrainData &lt;- datos[train, ]\ntestData  &lt;- datos[-train, ]\n\nSe divide el dataset en 80% train y 20% test, se utilizo el log para una mejor distribución de los datos\n\nx_train_Data &lt;- trainData %&gt;% select(-price)\nx_test_Data &lt;- testData %&gt;% select(-price)\n\npreProcValues &lt;- preProcess(x_train_Data, method = c(\"center\", \"scale\"))\ntrainTransformed &lt;- predict(preProcValues, x_train_Data)\ntestTransformed  &lt;- predict(preProcValues, x_test_Data)\n\nSe realiza un escalado y centrado de variables y disminuir la varianza de las variables\n\ntrainTransformed$price &lt;- trainData$price\ntestTransformed$price  &lt;- testData$price\n\nReconstruimos los datasets con la variable objetivo y ademas transformadolo en log10 para un mejor modelado de los datos\n\nset.seed(123)\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)\nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\")\n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbors y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) {\n  train(price ~ .,\n        data = trainTransformed,\n        method = m,\n        trControl = ctrl,\n        tuneLength = 5,\n        verbose = FALSE)})\n\nSe entreno el modelo con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods\nresults &lt;- resamples(models)\nsummary(results)$statistics\n\n$MAE\n              Min.  1st Qu.   Median     Mean  3rd Qu.      Max. NA's\nrf        49238.18 51598.00 53164.11 53445.53 56602.50  56624.86    0\nsvmRadial 91789.55 92955.09 95152.69 95895.11 98034.69 101543.53    0\nknn       49924.93 56814.36 57146.50 57134.09 59973.65  61811.00    0\ngbm       50253.21 50766.03 52178.14 53445.29 54415.45  59613.60    0\n\n$RMSE\n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf         70454.95  71479.84  80554.00  77476.81  80764.17  84131.10    0\nsvmRadial 125715.68 134115.58 134245.84 134809.59 138444.56 141526.26    0\nknn        71110.45  79521.12  84608.08  82789.01  88674.75  90030.63    0\ngbm        69287.99  73344.62  78326.73  76928.29  79544.99  84137.15    0\n\n$Rsquared\n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.5987966 0.6300578 0.6643078 0.6620008 0.6895211 0.7273205    0\nsvmRadial 0.3993386 0.4330001 0.4491988 0.4478477 0.4508483 0.5068530    0\nknn       0.5540326 0.6000681 0.6172839 0.6127602 0.6373535 0.6550627    0\ngbm       0.6399544 0.6458829 0.6762382 0.6654302 0.6812276 0.6838476    0\n\n\nResumen de las métricas de MAE, RMSE, Rsquared\n\ndotplot(results)\n\n\n\n\n\n\n\n\nSe observa en la gráfica, tanto el rf como gbm tiene los valores mas bajos de RMSE.\n\nbest_model &lt;- models[[\"rf\"]]\npred_price &lt;- predict(best_model, newdata = testTransformed)\nRMSE(pred_price, testData$price)\n\n[1] 59945.81\n\n\nEn los datos de test se observa que el RMSE es de casi 60000,\n\nimportance &lt;- varImp(best_model, scale = TRUE)\nplot(importance, top = 5)\n\n\n\n\n\n\n\n\nEl gráfico muestra las variables mas importantes, en las que se encuentra sqft, longitude y baths"
  },
  {
    "objectID": "r/r_price.html#cargar-las-librerias-y-el-dataset",
    "href": "r/r_price.html#cargar-las-librerias-y-el-dataset",
    "title": "Dataset Sacramento",
    "section": "",
    "text": "library(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nCargar el dataset dentro de la variable datos.\n\ndata(Sacramento)\ndatos  = Sacramento\n\nInformación del dataset\n\n# city: Ciudad donde se encuentra la propiedad (factor).\n# zip: Código postal (factor).\n# beds: Número de habitaciones (numérico).\n# baths: Número de baños (numérico).\n# sqft: Superficie en pies cuadrados (numérico).\n# type: Tipo de propiedad (factor: Residential, Condo, Multi-Family, etc.).\n# price: Precio de venta en dólares (numérico).\n# latitude: Latitud geográfica (numérico).\n# longitude: Longitud geográfica (numérico).\n\n\ndim(datos)\n\n[1] 932   9\n\n\nEl dataset tiene 932 observaciones y 9 variables.\n\nstr(datos)\n\n'data.frame':   932 obs. of  9 variables:\n $ city     : Factor w/ 37 levels \"ANTELOPE\",\"AUBURN\",..: 34 34 34 34 34 34 34 34 29 31 ...\n $ zip      : Factor w/ 68 levels \"z95603\",\"z95608\",..: 64 52 44 44 53 65 66 49 24 25 ...\n $ beds     : int  2 3 2 2 2 3 3 3 2 3 ...\n $ baths    : num  1 1 1 1 1 1 2 1 2 2 ...\n $ sqft     : int  836 1167 796 852 797 1122 1104 1177 941 1146 ...\n $ type     : Factor w/ 3 levels \"Condo\",\"Multi_Family\",..: 3 3 3 3 3 1 3 3 1 3 ...\n $ price    : int  59222 68212 68880 69307 81900 89921 90895 91002 94905 98937 ...\n $ latitude : num  38.6 38.5 38.6 38.6 38.5 ...\n $ longitude: num  -121 -121 -121 -121 -121 ...\n\n\nSe observa que existen variables numéricas y categóricas, la variable a analizar es price, la cual es numérica.\n\nResumen estadístico del dataset\n\nsummary(datos)\n\n             city          zip           beds           baths      \n SACRAMENTO    :438   z95823 : 61   Min.   :1.000   Min.   :1.000  \n ELK_GROVE     :114   z95828 : 45   1st Qu.:3.000   1st Qu.:2.000  \n ROSEVILLE     : 48   z95758 : 44   Median :3.000   Median :2.000  \n CITRUS_HEIGHTS: 35   z95835 : 37   Mean   :3.276   Mean   :2.053  \n ANTELOPE      : 33   z95838 : 37   3rd Qu.:4.000   3rd Qu.:2.000  \n RANCHO_CORDOVA: 28   z95757 : 36   Max.   :8.000   Max.   :5.000  \n (Other)       :236   (Other):672                                  \n      sqft                type         price           latitude    \n Min.   : 484   Condo       : 53   Min.   : 30000   Min.   :38.24  \n 1st Qu.:1167   Multi_Family: 13   1st Qu.:156000   1st Qu.:38.48  \n Median :1470   Residential :866   Median :220000   Median :38.62  \n Mean   :1680                      Mean   :246662   Mean   :38.59  \n 3rd Qu.:1954                      3rd Qu.:305000   3rd Qu.:38.69  \n Max.   :4878                      Max.   :884790   Max.   :39.02  \n                                                                   \n   longitude     \n Min.   :-121.6  \n 1st Qu.:-121.4  \n Median :-121.4  \n Mean   :-121.4  \n 3rd Qu.:-121.3  \n Max.   :-120.6  \n                 \n\n\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price), alpha = 0.7, size = 3) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  labs(title = \"Mapa de precios de viviendas en Sacramento\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico es un Scatterplot geográfico: latitud vs longitud\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price), alpha = 0.7, size = 3) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  facet_wrap(~ type) +\n  labs(title = \"Mapa de precios por tipo de propiedad\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico de latitud vs longitud se divido en 3 categorias: condominio, multifamiliar y residencial\n\nggplot(Sacramento, aes(x = longitude, y = latitude)) +\n  geom_point(aes(color = price, size = beds), alpha = 0.6) +\n  scale_color_gradient(low = \"yellow\", high = \"red\") +\n  labs(title = \"Mapa de precios con tamaño según habitaciones\",\n       x = \"Longitud\", y = \"Latitud\", color = \"Precio (USD)\", size = \"Habitaciones\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl color indica el precio: amarillo = más barato, rojo = más caro. El gráfico de latitud vs longitud, se añadio el tamaño de las habitaciones\n\nggplot(datos, aes(x=price)) +  geom_histogram(color=\"black\", fill=\"lightblue\",\n                                              linetype=\"dashed\")+ xlab(label = \"Precio (USD)\")+\n  labs(title =\"Distribución de log precios de casas\" )\n\n\n\n\n\n\n\n\nDistribución de precios de las viviendas, se ve una asimetría en los datos\n\nggplot(datos, aes(x=log10(price))) +  geom_histogram(color=\"black\", fill=\"lightgreen\",\n                                              linetype=\"dashed\")+ xlab(label = \"Precio (USD)\")+\n  labs(title =\"Distribución de log precios de casas\" )\n\n\n\n\n\n\n\n\nDistribución de log10(Precio) de las viviendas, debido a que existe una asimetría en los datos\n\nggplot(datos) +\n  geom_density(aes(x = log10(price), fill = type), alpha=0.25)+\n  ggtitle(\"Densidad sobre log10(price) de las viviendas\")\n\n\n\n\n\n\n\n\n\nggplot(data=datos,aes(x=log10(price), y=sqft,color=type)) +\n  geom_boxplot() +theme_minimal()+ theme() +\n  ggtitle(\"Boxplot de log10(price) de las viviendas\")"
  },
  {
    "objectID": "r/r_price.html#modelado",
    "href": "r/r_price.html#modelado",
    "title": "Dataset Sacramento",
    "section": "",
    "text": "set.seed(123)\ntrain &lt;- createDataPartition(log10(datos$price), p = 0.8, list = FALSE)\ntrainData &lt;- datos[train, ]\ntestData  &lt;- datos[-train, ]\n\nSe divide el dataset en 80% train y 20% test, se utilizo el log para una mejor distribución de los datos\n\nx_train_Data &lt;- trainData %&gt;% select(-price)\nx_test_Data &lt;- testData %&gt;% select(-price)\n\npreProcValues &lt;- preProcess(x_train_Data, method = c(\"center\", \"scale\"))\ntrainTransformed &lt;- predict(preProcValues, x_train_Data)\ntestTransformed  &lt;- predict(preProcValues, x_test_Data)\n\nSe realiza un escalado y centrado de variables y disminuir la varianza de las variables\n\ntrainTransformed$price &lt;- trainData$price\ntestTransformed$price  &lt;- testData$price\n\nReconstruimos los datasets con la variable objetivo y ademas transformadolo en log10 para un mejor modelado de los datos\n\nset.seed(123)\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nRealizamos un cross-validation para evitar el sobreajuste\n\nset.seed(123)\nmethods &lt;- c(\"rf\", \"svmRadial\", \"knn\", \"gbm\")\n\nLos modelos que se van a entrenar son: random forest, svmRadial, k-Nearest Neighbors y Generalized Boosted Models\n\nmodels &lt;- lapply(methods, function(m) {\n  train(price ~ .,\n        data = trainTransformed,\n        method = m,\n        trControl = ctrl,\n        tuneLength = 5,\n        verbose = FALSE)})\n\nSe entreno el modelo con los 4 modelos mas utilizados\n\nnames(models) &lt;- methods\nresults &lt;- resamples(models)\nsummary(results)$statistics\n\n$MAE\n              Min.  1st Qu.   Median     Mean  3rd Qu.      Max. NA's\nrf        49238.18 51598.00 53164.11 53445.53 56602.50  56624.86    0\nsvmRadial 91789.55 92955.09 95152.69 95895.11 98034.69 101543.53    0\nknn       49924.93 56814.36 57146.50 57134.09 59973.65  61811.00    0\ngbm       50253.21 50766.03 52178.14 53445.29 54415.45  59613.60    0\n\n$RMSE\n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf         70454.95  71479.84  80554.00  77476.81  80764.17  84131.10    0\nsvmRadial 125715.68 134115.58 134245.84 134809.59 138444.56 141526.26    0\nknn        71110.45  79521.12  84608.08  82789.01  88674.75  90030.63    0\ngbm        69287.99  73344.62  78326.73  76928.29  79544.99  84137.15    0\n\n$Rsquared\n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nrf        0.5987966 0.6300578 0.6643078 0.6620008 0.6895211 0.7273205    0\nsvmRadial 0.3993386 0.4330001 0.4491988 0.4478477 0.4508483 0.5068530    0\nknn       0.5540326 0.6000681 0.6172839 0.6127602 0.6373535 0.6550627    0\ngbm       0.6399544 0.6458829 0.6762382 0.6654302 0.6812276 0.6838476    0\n\n\nResumen de las métricas de MAE, RMSE, Rsquared\n\ndotplot(results)\n\n\n\n\n\n\n\n\nSe observa en la gráfica, tanto el rf como gbm tiene los valores mas bajos de RMSE.\n\nbest_model &lt;- models[[\"rf\"]]\npred_price &lt;- predict(best_model, newdata = testTransformed)\nRMSE(pred_price, testData$price)\n\n[1] 59945.81\n\n\nEn los datos de test se observa que el RMSE es de casi 60000,\n\nimportance &lt;- varImp(best_model, scale = TRUE)\nplot(importance, top = 5)\n\n\n\n\n\n\n\n\nEl gráfico muestra las variables mas importantes, en las que se encuentra sqft, longitude y baths"
  },
  {
    "objectID": "pb/pb_sap.html",
    "href": "pb/pb_sap.html",
    "title": "Visualización de Datos de SAP BO en Power BI”",
    "section": "",
    "text": "Para este ejemplo se utilizo el Database demo de México 2025, para descargar presione aquí.\nPara conocer sobre las tablas que contiene el demo, como tambien los nombres de cada tabla, descripción y respuesta de las filas se utilizó como referencia la siguiente página web aquí.\nPara el análisis del demo, se utilizo como los siguientes herramientas:\n\nMicrosoft SQL Server\nDocker\nSQLPad\nPower BI\n\n\nPaso 1\nSe descargo el Database demo de México, llamado SBODemoMX.\n\n\nPaso 2\nSe creo el archivo docker-compose.yml para instalar Microsoft SQL Server y SQLPad para la GUI.\n\n\n\nPaso 3\nSe testeo la conexión en SQLPad.\n\n\n\nPaso 4\nSe utilizó la siguiente sintaxis para extraer la información para el análisis en Power BI.\n\n\n\nPaso 5\nSe realizó la conexión entre Power BI y SQL Server.\n\n\n\nPaso 6\nSe realizó el Dashboard de Database demo de México en Power BI. El dashboard puedes descargarlo de aquí."
  },
  {
    "objectID": "p/p_clasi_sklearn.html",
    "href": "p/p_clasi_sklearn.html",
    "title": "Analisis del dataset de bank",
    "section": "",
    "text": "Analisis del dataset de bank\nEl dataset bank proviene de una campaña de marketing de un banco portugués. Contiene información sobre clientes y llamadas telefónicas realizadas para promocionar depósitos a plazo fijo. El objetivo es predecir si un cliente aceptará abrir un depósito\n\nCargar las librerias y el dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import (\n    classification_report, ConfusionMatrixDisplay,\n    roc_curve, auc, f1_score, accuracy_score,\n    average_precision_score)\n\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    GradientBoostingClassifier,\n    HistGradientBoostingClassifier)\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n\n\n1 Cargar Dataset\ndf = pd.read_csv(\"bank.csv\")\ndf.head(3)\n\n\n\n\n\n\n\n\nage\n\n\njob\n\n\nmarital\n\n\neducation\n\n\ndefault\n\n\nbalance\n\n\nhousing\n\n\nloan\n\n\ncontact\n\n\nday\n\n\nmonth\n\n\nduration\n\n\ncampaign\n\n\npdays\n\n\nprevious\n\n\npoutcome\n\n\ndeposit\n\n\n\n\n\n\n0\n\n\n59\n\n\nadmin.\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n2343\n\n\nyes\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n1042\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nyes\n\n\n\n\n1\n\n\n56\n\n\nadmin.\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n45\n\n\nno\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n1467\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nyes\n\n\n\n\n2\n\n\n41\n\n\ntechnician\n\n\nmarried\n\n\nsecondary\n\n\nno\n\n\n1270\n\n\nyes\n\n\nno\n\n\nunknown\n\n\n5\n\n\nmay\n\n\n1389\n\n\n1\n\n\n-1\n\n\n0\n\n\nunknown\n\n\nyes\n\n\n\n\n\nage: edad.\njob: tipo de trabajo (admin, blue-collar, technician, etc.).\nmarital: estado civil (married, single, divorced).\neducation: nivel educativo.\ndefault: si tiene créditos en default.\nhousing: si tiene préstamo hipotecario.\nloan: si tiene préstamo personal.\nmonth: mes de la última llamada.\nday_of_week: día de la semana de la última llamada.\nduration: duración de la llamada (segundos).\ncampaign: número de contactos realizados durante la campaña.\npdays: días desde el último contacto en una campaña anterior.\nprevious: número de contactos anteriores.\npoutcome: resultado de la campaña anterior.\n\n\n2 Análisis Exploratorio de Datos\ndf.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11162 entries, 0 to 11161\nData columns (total 17 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   age        11162 non-null  int64 \n 1   job        11162 non-null  object\n 2   marital    11162 non-null  object\n 3   education  11162 non-null  object\n 4   default    11162 non-null  object\n 5   balance    11162 non-null  int64 \n 6   housing    11162 non-null  object\n 7   loan       11162 non-null  object\n 8   contact    11162 non-null  object\n 9   day        11162 non-null  int64 \n 10  month      11162 non-null  object\n 11  duration   11162 non-null  int64 \n 12  campaign   11162 non-null  int64 \n 13  pdays      11162 non-null  int64 \n 14  previous   11162 non-null  int64 \n 15  poutcome   11162 non-null  object\n 16  deposit    11162 non-null  object\ndtypes: int64(7), object(10)\nmemory usage: 1.4+ MB\nHay 10 variables categoricas, incluyendo a la variable objetivo, y 7 variables numerica\ndf.isna().sum()\nage          0\njob          0\nmarital      0\neducation    0\ndefault      0\nbalance      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\ndeposit      0\ndtype: int64\nNo existen valores faltantes\ndf['deposit'].value_counts()\ndeposit\nno     5873\nyes    5289\nName: count, dtype: int64\nLa variable objetivo tiene un balance entre los 2 valores\ncategoricas = df.select_dtypes(include=[\"object\"])\ncategoricas.nunique()\njob          12\nmarital       3\neducation     4\ndefault       2\nhousing       2\nloan          2\ncontact       3\nmonth        12\npoutcome      4\ndeposit       2\ndtype: int64\nSe observa que la variable job tiene 12 distintos valores\nnumericas = df.select_dtypes(include=[\"number\"])\nnumericas.describe().round(2)\n\n\n\n\n\n\n\n\nage\n\n\nbalance\n\n\nday\n\n\nduration\n\n\ncampaign\n\n\npdays\n\n\nprevious\n\n\n\n\n\n\ncount\n\n\n11162.00\n\n\n11162.00\n\n\n11162.00\n\n\n11162.00\n\n\n11162.00\n\n\n11162.00\n\n\n11162.00\n\n\n\n\nmean\n\n\n41.23\n\n\n1528.54\n\n\n15.66\n\n\n371.99\n\n\n2.51\n\n\n51.33\n\n\n0.83\n\n\n\n\nstd\n\n\n11.91\n\n\n3225.41\n\n\n8.42\n\n\n347.13\n\n\n2.72\n\n\n108.76\n\n\n2.29\n\n\n\n\nmin\n\n\n18.00\n\n\n-6847.00\n\n\n1.00\n\n\n2.00\n\n\n1.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n25%\n\n\n32.00\n\n\n122.00\n\n\n8.00\n\n\n138.00\n\n\n1.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n50%\n\n\n39.00\n\n\n550.00\n\n\n15.00\n\n\n255.00\n\n\n2.00\n\n\n-1.00\n\n\n0.00\n\n\n\n\n75%\n\n\n49.00\n\n\n1708.00\n\n\n22.00\n\n\n496.00\n\n\n3.00\n\n\n20.75\n\n\n1.00\n\n\n\n\nmax\n\n\n95.00\n\n\n81204.00\n\n\n31.00\n\n\n3881.00\n\n\n63.00\n\n\n854.00\n\n\n58.00\n\n\n\n\n\nResumen estadístico de las variables numericas, la variable balance, tiene mayor dispersión de los datos con respecto a su media\nsns.countplot(x='deposit', data=df, hue=df[\"deposit\"], legend=True)\nplt.title('Distribución de la variable objetivo (deposit)');\n\n\n\npng\n\n\nLa proporción de la variable deposit es similar\nsns.histplot(df['age'], bins=30, kde=True)\nplt.title('Distribución de la edad');\n\n\n\npng\n\n\nLa mayoria de individuos tiene entre 30 a 40 años\nsns.boxplot(x='deposit', y='age', data=df, hue=\"deposit\", legend=True)\nplt.title('Edad vs deposit');\n\n\n\npng\n\n\nEl gráfico muestra las diferencias de edad entre quienes aceptan y quienes no.\nsns.countplot(x='marital', hue='deposit', data=df)\nplt.title('Estado civil vs Deposit');\n\n\n\npng\n\n\nEl gráfico permite ver si el estado civil influye en la decisiones\nplt.figure(figsize=(10,5))\nsns.countplot(y='job', hue='deposit', data=df, order=df['job'].value_counts().index)\nplt.title('Tipo de trabajo vs Deposit');\n\n\n\npng\n\n\nEl gráfico muestra qué profesiones tienen mayor tasa de aceptación.\nsns.histplot(data=df, x='duration', hue='deposit', bins=30, kde=True)\nplt.title('Duración de llamada vs deposit');\n\n\n\npng\n\n\ndf.groupby(\"deposit\")[\"duration\"].describe().round(2)\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\ndeposit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nno\n\n\n5873.0\n\n\n223.13\n\n\n208.58\n\n\n2.0\n\n\n94.0\n\n\n163.0\n\n\n282.0\n\n\n3284.0\n\n\n\n\nyes\n\n\n5289.0\n\n\n537.29\n\n\n392.53\n\n\n8.0\n\n\n244.0\n\n\n426.0\n\n\n725.0\n\n\n3881.0\n\n\n\n\n\nLa duración de las llamadas parece ser muy influyente: llamadas más largas tienden a correlacionarse con aceptación.\n\n\n3 Preprocesamiento\nse realizan un escalado en las variables numericas y convierte los datos categóricos en un formato numérico.\nX = df.drop(\"deposit\", axis=1)\ny = df[\"deposit\"].map({\"no\": 0, \"yes\": 1})\n\ncategoricas = categoricas.columns.tolist()[:-1]\nnumericas = numericas.columns.tolist()\n\npreproc = ColumnTransformer([\n    (\"num\", StandardScaler(), numericas),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categoricas)\n])\n\n\n4 Split Dataset\nSe divide el dataset en 80% entrenamiento y 20% test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42)\n\n\n5 Definir los modelos\nmodelos = {\n    \"rf\": RandomForestClassifier(random_state=42),\n    \"gb\": GradientBoostingClassifier(random_state=42),\n    \"hgb\": HistGradientBoostingClassifier(random_state=42),\n    \"xgb\": XGBClassifier(\n        objective=\"binary:logistic\",\n        eval_metric=\"logloss\",\n        random_state=42, \n    ),\n    \"lgbm\": LGBMClassifier(random_state=42, verbosity=-1, enable_categorical=True)}\n\n\n6 Entrenar todos los modelos\nresultados_default = {}\n\nfor name, modelo in modelos.items():\n\n    pipe = Pipeline([\n        (\"preproc\", preproc),\n        (\"clf\", modelo)\n    ])\n\n    pipe.fit(X_train, y_train)\n\n    y_pred = pipe.predict(X_test)\n    y_prob = pipe.predict_proba(X_test)[:, 1]\n\n    f1 = f1_score(y_test, y_pred)\n    acc = accuracy_score(y_test, y_pred)\n    auprc = average_precision_score(y_test, y_pred)\n    auc_score = auc(*roc_curve(y_test, y_prob)[:2])\n\n    resultados_default[name] = {\n        \"pipeline\": pipe,\n        \"f1\": f1,\n        \"accuracy\": acc,\n        \"auc\": auc_score,\n        \"AUPRC\": auprc\n    }\npd.DataFrame(resultados_default)[1:].transpose().sort_values(\"f1\", ascending=False)\n\n\n\n\n\n\n\n\nf1\n\n\naccuracy\n\n\nauc\n\n\nAUPRC\n\n\n\n\n\n\nhgb\n\n\n0.865604\n\n\n0.867891\n\n\n0.929861\n\n\n0.798607\n\n\n\n\nlgbm\n\n\n0.862116\n\n\n0.865204\n\n\n0.929475\n\n\n0.796341\n\n\n\n\nxgb\n\n\n0.861213\n\n\n0.864756\n\n\n0.927248\n\n\n0.79644\n\n\n\n\nrf\n\n\n0.859617\n\n\n0.862069\n\n\n0.919273\n\n\n0.791377\n\n\n\n\ngb\n\n\n0.844403\n\n\n0.849978\n\n\n0.922934\n\n\n0.779954\n\n\n\n\n\n\n\n7 Elegir los top 3 por F1 score\nranking = sorted(resultados_default.items(), key=lambda x: x[1][\"f1\"], reverse=True)\ntop3 = [name for name, _ in ranking[:3]]\n\nprint(\"======= TOP 3 MODELOS =======\")\nprint(top3)\n======= TOP 3 MODELOS =======\n['hgb', 'lgbm', 'xgb']\n\n\n8 Gridsearch solo a los Top 3\nparam_grid = {\n    \"rf\": {\n        \"clf__n_estimators\": [200, 400, 600],\n        \"clf__max_depth\": [5, 10, 15]\n    },\n    \"gb\": {\n        \"clf__learning_rate\": [0.01, 0.05, 0.1],\n        \"clf__n_estimators\": [100, 200, 400]\n    },\n    \"hgb\": {\n        \"clf__learning_rate\": [0.01, 0.05, 0.1],\n        \"clf__max_depth\": [5, 10, 15]\n    },\n    \"xgb\": {\n        \"clf__eta\": [0.01,0.05, 0.1],\n        \"clf__max_depth\": [4, 8],\n        \"clf__n_estimators\": [200, 400, 600]\n    },\n    \"lgbm\": {\n        \"clf__learning_rate\": [0.01, 0.05, 0.1],\n        \"clf__n_estimators\": [200, 400, 600],\n        \"clf__max_depth\": [-1, 10]\n    }\n}\n\nEntrenando los modelos con Gridsearch\nbest_models = {}\n\ncv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name in top3:\n\n    print(f\"---- GRIDSEARCH: {name} ----\")\n\n    pipe = Pipeline([\n        (\"preproc\", preproc),        \n        (\"clf\", modelos[name])\n    ])\n\n    grid = GridSearchCV(\n        pipe,\n        param_grid[name],\n        cv=cv_strategy,\n        scoring=\"f1\",\n        n_jobs=-1\n    )\n\n    grid.fit(X_train, y_train)\n    best_models[name] = grid.best_estimator_\n---- GRIDSEARCH: hgb ----\n---- GRIDSEARCH: lgbm ----\n\n\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/home/cris/miniconda3/envs/mi_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n\n\n---- GRIDSEARCH: xgb ----\n\n\n\n9 Seleccionar el mejor modelo\nscores = {}\nfor name, model in best_models.items():\n    scores[name] = model.score(X_test, y_test)\n\nbest_name = max(scores, key=scores.get)\nbest_model = best_models[best_name]\n\nprint(\"=== MEJOR MODELO ===\")\nprint(best_name, scores[best_name])\n=== MEJOR MODELO ===\nxgb 0.864755933721451\nMétricas sobre dataset en train, en el cual el f1-score es 0.97\ny_pred = best_model.predict(X_train)\n\nprint(\"=== Reporte en Train===\")\nprint(classification_report(y_train, y_pred))\n=== Reporte en Train===\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      4698\n           1       0.97      0.98      0.97      4231\n\n    accuracy                           0.97      8929\n   macro avg       0.97      0.97      0.97      8929\nweighted avg       0.97      0.97      0.97      8929\nLa matrix de confusión\nConfusionMatrixDisplay.from_predictions(y_train, y_pred);\n\n\n\npng\n\n\n\n\n10 Evaluación Final\nEl f1-score en el test es del 0.86\ny_pred = best_model.predict(X_test)\n\nprint(\"=== Reporte en Test ===\")\nprint(classification_report(y_test, y_pred))\n=== Reporte en Test ===\n              precision    recall  f1-score   support\n\n           0       0.89      0.85      0.87      1175\n           1       0.84      0.89      0.86      1058\n\n    accuracy                           0.86      2233\n   macro avg       0.86      0.87      0.86      2233\nweighted avg       0.87      0.86      0.86      2233\nConfusionMatrixDisplay.from_predictions(y_test, y_pred);\n\n\n\npng\n\n\n\n\n9 Gráfico ROC & Curva PR\nEl mejor umbral es a 43%\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,5))\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\nplt.plot([0,1], [0,1], linestyle=\"--\")\n\npunto = (tpr - fpr).argmax()\nfpr_best = fpr[punto]\ntpr_best = tpr[punto]\nthreshold_best = thresholds[punto]\n\nplt.scatter(fpr_best, tpr_best, color=\"red\", s=80,\n            label=f\"Mejor punto(thr={threshold_best:.2f})\")\n\nplt.xlabel(\"Tasa de Falso Positivo\")\nplt.ylabel(\"Tasa de Verdadero Positivo\")\nplt.title(\"Curva ROC\")\nplt.legend();\n\nprint(\"Limite:\", threshold_best)\nprint(\"FPR:\", fpr_best)\nprint(\"TPR:\", tpr_best)\nLimite: 0.4329248454634197\nFPR: 0.1753191489361702\nTPR: 0.9187145557655955\n\n\n\npng\n\n\nLas variables mas importantes es cuando la campaña anterior fue exitosa.\nfeat_imp = pd.DataFrame({\n    \"var\": preproc.get_feature_names_out(),\n    \"imp\": best_model.named_steps[\"clf\"].feature_importances_\n}).sort_values(by=\"imp\", ascending=False)[:5]\n\nplt.figure(figsize=(8,6))\nplt.barh(feat_imp[\"var\"], feat_imp[\"imp\"])\nplt.gca().invert_yaxis()\nplt.title(\"Variables Importantes\");\n\n\n\npng"
  }
]